Spark Command: /opt/jdk1.8.0_112/bin/java -cp ../thirdparty/spark-2.4.3-bin-hadoop2.7/conf/:/opt/fusiondb/sbin/../thirdparty/spark-2.4.3-bin-hadoop2.7/jars/* -Xmx1g org.apache.spark.deploy.SparkSubmit --conf spark.sql.server.port=54322 --conf spark.sql.server.psql.enabled=true --conf spark.sql.server.binaryTransferMode=false --properties-file /opt/fusiondb/sbin/../conf/spark-defaults.conf --class org.apache.spark.sql.fdb.SQLServer --name FusionDB SQL Server /opt/fusiondb/sbin/../assembly/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar
========================================
19/06/29 17:41:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
19/06/29 17:41:25 INFO SQLServer: Started daemon with process name: 935@35758d068699
19/06/29 17:41:25 INFO SignalUtils: Registered signal handler for TERM
19/06/29 17:41:25 INFO SignalUtils: Registered signal handler for HUP
19/06/29 17:41:25 INFO SignalUtils: Registered signal handler for INT
19/06/29 17:41:25 INFO SQLServer: Spark properties passed to the SQL server:
  key=spark.sql.crossJoin.enabled value=true
  key=spark.sql.server.port value=54322
  key=spark.app.name value=FusionDB SQL Server
  key=spark.sql.server.binaryTransferMode value=false
  key=spark.master value=local[*]
  key=spark.submit.deployMode value=client
  key=spark.sql.server.psql.enabled value=true
  key=spark.jars value=file:/opt/fusiondb/sbin/../assembly/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar
       
19/06/29 17:41:25 INFO RecurringTimer: Started timer for Idle Session Cleaner at time 1561830300000
19/06/29 17:41:26 INFO SparkContext: Running Spark version 2.4.3
19/06/29 17:41:26 INFO SparkContext: Submitted application: FusionDB SQL Server
19/06/29 17:41:26 INFO SecurityManager: Changing view acls to: root
19/06/29 17:41:26 INFO SecurityManager: Changing modify acls to: root
19/06/29 17:41:26 INFO SecurityManager: Changing view acls groups to: 
19/06/29 17:41:26 INFO SecurityManager: Changing modify acls groups to: 
19/06/29 17:41:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
19/06/29 17:41:26 INFO Utils: Successfully started service 'sparkDriver' on port 45203.
19/06/29 17:41:26 INFO SparkEnv: Registering MapOutputTracker
19/06/29 17:41:26 INFO SparkEnv: Registering BlockManagerMaster
19/06/29 17:41:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/06/29 17:41:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/06/29 17:41:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d98017cb-fb91-467c-b8d8-4a3d8304d5ed
19/06/29 17:41:26 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
19/06/29 17:41:26 INFO SparkEnv: Registering OutputCommitCoordinator
19/06/29 17:41:26 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/06/29 17:41:26 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://35758d068699:4040
19/06/29 17:41:26 INFO SparkContext: Added JAR file:/opt/fusiondb/sbin/../assembly/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar at spark://35758d068699:45203/jars/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar with timestamp 1561830086880
19/06/29 17:41:26 INFO Executor: Starting executor ID driver on host localhost
19/06/29 17:41:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33063.
19/06/29 17:41:27 INFO NettyBlockTransferService: Server created on 35758d068699:33063
19/06/29 17:41:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/06/29 17:41:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 35758d068699, 33063, None)
19/06/29 17:41:27 INFO BlockManagerMasterEndpoint: Registering block manager 35758d068699:33063 with 366.3 MB RAM, BlockManagerId(driver, 35758d068699, 33063, None)
19/06/29 17:41:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 35758d068699, 33063, None)
19/06/29 17:41:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 35758d068699, 33063, None)
19/06/29 17:41:27 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/fusiondb/sbin/spark-warehouse').
19/06/29 17:41:27 INFO SharedState: Warehouse path is 'file:/opt/fusiondb/sbin/spark-warehouse'.
19/06/29 17:41:28 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
19/06/29 17:41:28 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19/06/29 17:41:29 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19/06/29 17:41:29 INFO ObjectStore: ObjectStore, initialize called
19/06/29 17:41:29 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
19/06/29 17:41:29 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
19/06/29 17:41:31 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19/06/29 17:41:32 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:41:32 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:41:33 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:41:33 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:41:33 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/06/29 17:41:33 INFO ObjectStore: Initialized ObjectStore
19/06/29 17:41:34 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
19/06/29 17:41:34 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
19/06/29 17:41:34 INFO HiveMetaStore: Added admin role in metastore
19/06/29 17:41:34 INFO HiveMetaStore: Added public role in metastore
19/06/29 17:41:34 INFO HiveMetaStore: No user is added in admin role, since config is empty
19/06/29 17:41:34 INFO HiveMetaStore: 0: get_all_databases
19/06/29 17:41:34 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
19/06/29 17:41:34 INFO HiveMetaStore: 0: get_functions: db=default pat=*
19/06/29 17:41:34 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19/06/29 17:41:34 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:41:34 INFO SessionState: Created local directory: /tmp/e2b3751a-cd65-40f9-82a7-427751e3c02c_resources
19/06/29 17:41:34 INFO SessionState: Created HDFS directory: /tmp/hive/root/e2b3751a-cd65-40f9-82a7-427751e3c02c
19/06/29 17:41:34 INFO SessionState: Created local directory: /tmp/root/e2b3751a-cd65-40f9-82a7-427751e3c02c
19/06/29 17:41:34 INFO SessionState: Created HDFS directory: /tmp/hive/root/e2b3751a-cd65-40f9-82a7-427751e3c02c/_tmp_space.db
19/06/29 17:41:34 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/opt/fusiondb/sbin/spark-warehouse
19/06/29 17:41:34 INFO HiveMetaStore: 0: get_database: default
19/06/29 17:41:34 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/06/29 17:41:34 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:34 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:34 WARN ObjectStore: Failed to get database pg_catalog, returning NoSuchObjectException
19/06/29 17:41:36 INFO HiveMetaStore: 0: get_database: global_temp
19/06/29 17:41:36 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
19/06/29 17:41:36 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
19/06/29 17:41:36 INFO HiveMetaStore: 0: create_database: Database(name:pg_catalog, description:, locationUri:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db, parameters:{})
19/06/29 17:41:36 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_database: Database(name:pg_catalog, description:, locationUri:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db, parameters:{})	
19/06/29 17:41:36 WARN ObjectStore: Failed to get database pg_catalog, returning NoSuchObjectException
19/06/29 17:41:36 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db
19/06/29 17:41:36 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_namespace
19/06/29 17:41:36 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_namespace	
19/06/29 17:41:36 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_namespace
19/06/29 17:41:36 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_namespace	
19/06/29 17:41:36 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:36 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:36 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_namespace
19/06/29 17:41:36 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_namespace	
19/06/29 17:41:36 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:36 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:36 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:36 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:36 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:36 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:36 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_namespace
19/06/29 17:41:36 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_namespace	
19/06/29 17:41:36 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:36 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:37 INFO HiveMetaStore: 0: create_table: Table(tableName:pg_namespace, dbName:pg_catalog, owner:root, createTime:1561830096, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:nspname, type:string, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_namespace, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"nspname","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:41:37 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_namespace, dbName:pg_catalog, owner:root, createTime:1561830096, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:nspname, type:string, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_namespace, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"nspname","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:41:37 WARN HiveMetaStore: Location: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_namespace specified for non-external table:pg_namespace
19/06/29 17:41:37 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_namespace
19/06/29 17:41:37 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_namespace
19/06/29 17:41:37 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_namespace	
19/06/29 17:41:37 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_namespace/.hive-staging_hive_2019-06-29_17-41-37_903_4614084715709358213-1
19/06/29 17:41:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:38 INFO CodeGenerator: Code generated in 389.343476 ms
19/06/29 17:41:39 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:39 INFO DAGScheduler: Got job 0 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:39 INFO DAGScheduler: Final stage: ResultStage 0 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:39 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:39 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 214.1 KB, free 366.1 MB)
19/06/29 17:41:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 75.2 KB, free 366.0 MB)
19/06/29 17:41:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 35758d068699:33063 (size: 75.2 KB, free: 366.2 MB)
19/06/29 17:41:39 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
19/06/29 17:41:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8060 bytes)
19/06/29 17:41:39 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
19/06/29 17:41:39 INFO Executor: Fetching spark://35758d068699:45203/jars/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar with timestamp 1561830086880
19/06/29 17:41:39 INFO TransportClientFactory: Successfully created connection to 35758d068699/172.17.0.2:45203 after 57 ms (0 ms spent in bootstraps)
19/06/29 17:41:39 INFO Utils: Fetching spark://35758d068699:45203/jars/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar to /tmp/spark-64e949b5-3d90-4892-a29e-1de73ecd0be4/userFiles-bf26c48e-5931-4415-82a4-e306f04526fb/fetchFileTemp4425532147827787565.tmp
19/06/29 17:41:40 INFO Executor: Adding file:/tmp/spark-64e949b5-3d90-4892-a29e-1de73ecd0be4/userFiles-bf26c48e-5931-4415-82a4-e306f04526fb/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar to class loader
19/06/29 17:41:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:40 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174138_0000_m_000000_0' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_namespace/.hive-staging_hive_2019-06-29_17-41-37_903_4614084715709358213-1/-ext-10000/_temporary/0/task_20190629174138_0000_m_000000
19/06/29 17:41:40 INFO SparkHadoopMapRedUtil: attempt_20190629174138_0000_m_000000_0: Committed
19/06/29 17:41:40 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2022 bytes result sent to driver
19/06/29 17:41:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1282 ms on localhost (executor driver) (1/1)
19/06/29 17:41:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
19/06/29 17:41:40 INFO DAGScheduler: ResultStage 0 (main at NativeMethodAccessorImpl.java:0) finished in 1.570 s
19/06/29 17:41:40 INFO DAGScheduler: Job 0 finished: main at NativeMethodAccessorImpl.java:0, took 1.666961 s
19/06/29 17:41:40 INFO FileFormatWriter: Write Job dc14fc30-c88a-4e35-8a82-bbe9aabede3a committed.
19/06/29 17:41:40 INFO FileFormatWriter: Finished processing stats for write job dc14fc30-c88a-4e35-8a82-bbe9aabede3a.
19/06/29 17:41:40 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_namespace
19/06/29 17:41:40 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_namespace	
19/06/29 17:41:40 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_namespace
19/06/29 17:41:40 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_namespace	
19/06/29 17:41:40 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:40 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_namespace/.hive-staging_hive_2019-06-29_17-41-37_903_4614084715709358213-1/-ext-10000/part-00000-f5a923f9-fba6-47cf-beaf-480ccf41f0bb-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_namespace/part-00000-f5a923f9-fba6-47cf-beaf-480ccf41f0bb-c000, Status:true
19/06/29 17:41:40 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_namespace newtbl=pg_namespace
19/06/29 17:41:40 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_namespace newtbl=pg_namespace	
19/06/29 17:41:40 INFO log: Updating table stats fast for pg_namespace
19/06/29 17:41:40 INFO log: Updated size of table pg_namespace to 11
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_namespace
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_namespace	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_namespace
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_namespace	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_roles
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_roles	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_roles
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_roles	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_roles
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_roles	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_roles
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_roles	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:41 INFO HiveMetaStore: 0: create_table: Table(tableName:pg_roles, dbName:pg_catalog, owner:root, createTime:1561830101, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:rolname, type:string, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_roles, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"rolname","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_roles, dbName:pg_catalog, owner:root, createTime:1561830101, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:rolname, type:string, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_roles, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"rolname","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:41:41 WARN HiveMetaStore: Location: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_roles specified for non-external table:pg_roles
19/06/29 17:41:41 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_roles
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_roles
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_roles	
19/06/29 17:41:41 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_roles/.hive-staging_hive_2019-06-29_17-41-41_251_5835993585117287353-1
19/06/29 17:41:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:41 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:41 INFO DAGScheduler: Got job 1 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:41 INFO DAGScheduler: Final stage: ResultStage 1 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:41 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:41 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:41 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 214.1 KB, free 365.8 MB)
19/06/29 17:41:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 75.2 KB, free 365.7 MB)
19/06/29 17:41:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 35758d068699:33063 (size: 75.2 KB, free: 366.2 MB)
19/06/29 17:41:41 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:41 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
19/06/29 17:41:41 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 8068 bytes)
19/06/29 17:41:41 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
19/06/29 17:41:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:41 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174141_0001_m_000000_1' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_roles/.hive-staging_hive_2019-06-29_17-41-41_251_5835993585117287353-1/-ext-10000/_temporary/0/task_20190629174141_0001_m_000000
19/06/29 17:41:41 INFO SparkHadoopMapRedUtil: attempt_20190629174141_0001_m_000000_1: Committed
19/06/29 17:41:41 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2022 bytes result sent to driver
19/06/29 17:41:41 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 98 ms on localhost (executor driver) (1/1)
19/06/29 17:41:41 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
19/06/29 17:41:41 INFO DAGScheduler: ResultStage 1 (main at NativeMethodAccessorImpl.java:0) finished in 0.151 s
19/06/29 17:41:41 INFO DAGScheduler: Job 1 finished: main at NativeMethodAccessorImpl.java:0, took 0.156252 s
19/06/29 17:41:41 INFO FileFormatWriter: Write Job 37a71c15-77ce-43d3-9583-99d4e7a2502d committed.
19/06/29 17:41:41 INFO FileFormatWriter: Finished processing stats for write job 37a71c15-77ce-43d3-9583-99d4e7a2502d.
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_roles
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_roles	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_roles
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_roles	
19/06/29 17:41:41 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:41 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_roles/.hive-staging_hive_2019-06-29_17-41-41_251_5835993585117287353-1/-ext-10000/part-00000-cca8619b-a99f-4623-907d-508d98e03c28-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_roles/part-00000-cca8619b-a99f-4623-907d-508d98e03c28-c000, Status:true
19/06/29 17:41:41 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_roles newtbl=pg_roles
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_roles newtbl=pg_roles	
19/06/29 17:41:41 INFO log: Updating table stats fast for pg_roles
19/06/29 17:41:41 INFO log: Updated size of table pg_roles to 16
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_roles
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_roles	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_roles
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_roles	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_user
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_user	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_user
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_user	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_user
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_user	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_user
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_user	
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:41 INFO HiveMetaStore: 0: create_table: Table(tableName:pg_user, dbName:pg_catalog, owner:root, createTime:1561830101, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:usename, type:string, comment:null), FieldSchema(name:usesysid, type:int, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_user, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"usename","type":"string","nullable":true,"metadata":{}},{"name":"usesysid","type":"integer","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_user, dbName:pg_catalog, owner:root, createTime:1561830101, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:usename, type:string, comment:null), FieldSchema(name:usesysid, type:int, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_user, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"usename","type":"string","nullable":true,"metadata":{}},{"name":"usesysid","type":"integer","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:41:41 WARN HiveMetaStore: Location: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_user specified for non-external table:pg_user
19/06/29 17:41:41 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_user
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_user
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_user	
19/06/29 17:41:41 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_user/.hive-staging_hive_2019-06-29_17-41-41_749_5283667243106673546-1
19/06/29 17:41:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:41 INFO CodeGenerator: Code generated in 11.419576 ms
19/06/29 17:41:41 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:41 INFO DAGScheduler: Got job 2 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:41 INFO DAGScheduler: Final stage: ResultStage 2 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:41 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:41 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:41 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 214.1 KB, free 365.5 MB)
19/06/29 17:41:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 75.3 KB, free 365.5 MB)
19/06/29 17:41:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 35758d068699:33063 (size: 75.3 KB, free: 366.1 MB)
19/06/29 17:41:41 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:41 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
19/06/29 17:41:41 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 8068 bytes)
19/06/29 17:41:41 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
19/06/29 17:41:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:41 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174141_0002_m_000000_2' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_user/.hive-staging_hive_2019-06-29_17-41-41_749_5283667243106673546-1/-ext-10000/_temporary/0/task_20190629174141_0002_m_000000
19/06/29 17:41:41 INFO SparkHadoopMapRedUtil: attempt_20190629174141_0002_m_000000_2: Committed
19/06/29 17:41:41 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2022 bytes result sent to driver
19/06/29 17:41:41 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 110 ms on localhost (executor driver) (1/1)
19/06/29 17:41:41 INFO DAGScheduler: ResultStage 2 (main at NativeMethodAccessorImpl.java:0) finished in 0.146 s
19/06/29 17:41:41 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
19/06/29 17:41:41 INFO DAGScheduler: Job 2 finished: main at NativeMethodAccessorImpl.java:0, took 0.150088 s
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 16
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 49
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 24
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 37
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 59
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 56
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 10
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 34
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 33
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 35
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 36
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 52
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 31
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 43
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 8
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 5
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 20
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 55
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 48
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 26
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 21
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 13
19/06/29 17:41:41 INFO ContextCleaner: Cleaned accumulator 57
19/06/29 17:41:41 INFO FileFormatWriter: Write Job c9f8427f-8bea-418c-9f37-67d1c2eaa73f committed.
19/06/29 17:41:41 INFO FileFormatWriter: Finished processing stats for write job c9f8427f-8bea-418c-9f37-67d1c2eaa73f.
19/06/29 17:41:41 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_user
19/06/29 17:41:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_user	
19/06/29 17:41:42 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 35758d068699:33063 in memory (size: 75.2 KB, free: 366.2 MB)
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 11
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 27
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 39
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 9
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 25
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 41
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 30
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 58
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 51
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 12
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 18
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 6
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 22
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 32
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 44
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 40
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 45
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 15
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 28
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 53
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 17
19/06/29 17:41:42 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 35758d068699:33063 in memory (size: 75.2 KB, free: 366.2 MB)
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 46
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 42
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 54
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 23
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 29
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 14
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 50
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 7
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 38
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 19
19/06/29 17:41:42 INFO ContextCleaner: Cleaned accumulator 47
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_user
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_user	
19/06/29 17:41:42 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:42 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_user/.hive-staging_hive_2019-06-29_17-41-41_749_5283667243106673546-1/-ext-10000/part-00000-bcaf2ad0-00a8-44fa-954d-7b2b97fdb982-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_user/part-00000-bcaf2ad0-00a8-44fa-954d-7b2b97fdb982-c000, Status:true
19/06/29 17:41:42 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_user newtbl=pg_user
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_user newtbl=pg_user	
19/06/29 17:41:42 INFO log: Updating table stats fast for pg_user
19/06/29 17:41:42 INFO log: Updated size of table pg_user to 16
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_user
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_user	
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_user
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_user	
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:42 INFO HiveMetaStore: 0: create_table: Table(tableName:pg_type, dbName:pg_catalog, owner:root, createTime:1561830102, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:typname, type:string, comment:null), FieldSchema(name:typtype, type:string, comment:null), FieldSchema(name:typlen, type:int, comment:null), FieldSchema(name:typnotnull, type:boolean, comment:null), FieldSchema(name:typelem, type:int, comment:null), FieldSchema(name:typdelim, type:string, comment:null), FieldSchema(name:typinput, type:string, comment:null), FieldSchema(name:typrelid, type:int, comment:null), FieldSchema(name:typbasetype, type:int, comment:null), FieldSchema(name:typcollation, type:int, comment:null), FieldSchema(name:typnamespace, type:int, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"typname","type":"string","nullable":true,"metadata":{}},{"name":"typtype","type":"string","nullable":true,"metadata":{}},{"name":"typlen","type":"integer","nullable":true,"metadata":{}},{"name":"typnotnull","type":"boolean","nullable":true,"metadata":{}},{"name":"typelem","type":"integer","nullable":true,"metadata":{}},{"name":"typdelim","type":"string","nullable":true,"metadata":{}},{"name":"typinput","type":"string","nullable":true,"metadata":{}},{"name":"typrelid","type":"integer","nullable":true,"metadata":{}},{"name":"typbasetype","type":"integer","nullable":true,"metadata":{}},{"name":"typcollation","type":"integer","nullable":true,"metadata":{}},{"name":"typnamespace","type":"integer","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_type, dbName:pg_catalog, owner:root, createTime:1561830102, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:typname, type:string, comment:null), FieldSchema(name:typtype, type:string, comment:null), FieldSchema(name:typlen, type:int, comment:null), FieldSchema(name:typnotnull, type:boolean, comment:null), FieldSchema(name:typelem, type:int, comment:null), FieldSchema(name:typdelim, type:string, comment:null), FieldSchema(name:typinput, type:string, comment:null), FieldSchema(name:typrelid, type:int, comment:null), FieldSchema(name:typbasetype, type:int, comment:null), FieldSchema(name:typcollation, type:int, comment:null), FieldSchema(name:typnamespace, type:int, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"typname","type":"string","nullable":true,"metadata":{}},{"name":"typtype","type":"string","nullable":true,"metadata":{}},{"name":"typlen","type":"integer","nullable":true,"metadata":{}},{"name":"typnotnull","type":"boolean","nullable":true,"metadata":{}},{"name":"typelem","type":"integer","nullable":true,"metadata":{}},{"name":"typdelim","type":"string","nullable":true,"metadata":{}},{"name":"typinput","type":"string","nullable":true,"metadata":{}},{"name":"typrelid","type":"integer","nullable":true,"metadata":{}},{"name":"typbasetype","type":"integer","nullable":true,"metadata":{}},{"name":"typcollation","type":"integer","nullable":true,"metadata":{}},{"name":"typnamespace","type":"integer","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:41:42 WARN HiveMetaStore: Location: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type specified for non-external table:pg_type
19/06/29 17:41:42 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:42 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-42_356_4890160566674678444-1
19/06/29 17:41:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:42 INFO CodeGenerator: Code generated in 38.323433 ms
19/06/29 17:41:42 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:42 INFO DAGScheduler: Got job 3 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:42 INFO DAGScheduler: Final stage: ResultStage 3 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:42 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:42 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:42 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[7] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:42 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 215.8 KB, free 365.8 MB)
19/06/29 17:41:42 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 76.1 KB, free 365.7 MB)
19/06/29 17:41:42 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 366.2 MB)
19/06/29 17:41:42 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:42 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
19/06/29 17:41:42 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:42 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
19/06/29 17:41:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:42 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174142_0003_m_000000_3' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-42_356_4890160566674678444-1/-ext-10000/_temporary/0/task_20190629174142_0003_m_000000
19/06/29 17:41:42 INFO SparkHadoopMapRedUtil: attempt_20190629174142_0003_m_000000_3: Committed
19/06/29 17:41:42 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2022 bytes result sent to driver
19/06/29 17:41:42 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 71 ms on localhost (executor driver) (1/1)
19/06/29 17:41:42 INFO DAGScheduler: ResultStage 3 (main at NativeMethodAccessorImpl.java:0) finished in 0.127 s
19/06/29 17:41:42 INFO DAGScheduler: Job 3 finished: main at NativeMethodAccessorImpl.java:0, took 0.129614 s
19/06/29 17:41:42 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
19/06/29 17:41:42 INFO FileFormatWriter: Write Job 02d3c2b0-d92f-45fb-96e4-0551c5ae49cb committed.
19/06/29 17:41:42 INFO FileFormatWriter: Finished processing stats for write job 02d3c2b0-d92f-45fb-96e4-0551c5ae49cb.
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:42 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:42 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-42_356_4890160566674678444-1/-ext-10000/part-00000-29c367bf-2a8a-43ab-8c82-e6c62a50385a-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-29c367bf-2a8a-43ab-8c82-e6c62a50385a-c000, Status:true
19/06/29 17:41:42 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:42 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:42 INFO log: Updated size of table pg_type to 40
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:42 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-42_783_7576214065815146125-1
19/06/29 17:41:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:42 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:42 INFO DAGScheduler: Got job 4 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:42 INFO DAGScheduler: Final stage: ResultStage 4 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:42 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:42 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:42 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[9] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:42 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 215.8 KB, free 365.5 MB)
19/06/29 17:41:42 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 76.1 KB, free 365.4 MB)
19/06/29 17:41:42 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 366.1 MB)
19/06/29 17:41:42 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:42 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
19/06/29 17:41:42 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:42 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
19/06/29 17:41:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:42 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174142_0004_m_000000_4' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-42_783_7576214065815146125-1/-ext-10000/_temporary/0/task_20190629174142_0004_m_000000
19/06/29 17:41:42 INFO SparkHadoopMapRedUtil: attempt_20190629174142_0004_m_000000_4: Committed
19/06/29 17:41:42 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2022 bytes result sent to driver
19/06/29 17:41:42 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 34 ms on localhost (executor driver) (1/1)
19/06/29 17:41:42 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
19/06/29 17:41:42 INFO DAGScheduler: ResultStage 4 (main at NativeMethodAccessorImpl.java:0) finished in 0.081 s
19/06/29 17:41:42 INFO DAGScheduler: Job 4 finished: main at NativeMethodAccessorImpl.java:0, took 0.084704 s
19/06/29 17:41:42 INFO FileFormatWriter: Write Job 0cffc91f-8bc8-4cbf-b7ed-778800df16a7 committed.
19/06/29 17:41:42 INFO FileFormatWriter: Finished processing stats for write job 0cffc91f-8bc8-4cbf-b7ed-778800df16a7.
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:42 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:42 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:42 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-42_783_7576214065815146125-1/-ext-10000/part-00000-91e7b16b-1dfa-4cc0-9f73-87ffee54c538-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-91e7b16b-1dfa-4cc0-9f73-87ffee54c538-c000, Status:true
19/06/29 17:41:42 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:43 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:43 INFO log: Updated size of table pg_type to 83
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:43 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:43 INFO log: Updated size of table pg_type to 83
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-43_195_421747027226172917-1
19/06/29 17:41:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:43 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:43 INFO DAGScheduler: Got job 5 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:43 INFO DAGScheduler: Final stage: ResultStage 5 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:43 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:43 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:43 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[11] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:43 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 215.8 KB, free 365.2 MB)
19/06/29 17:41:43 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.1 KB, free 365.2 MB)
19/06/29 17:41:43 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 366.0 MB)
19/06/29 17:41:43 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:43 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
19/06/29 17:41:43 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:43 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
19/06/29 17:41:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:43 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174143_0005_m_000000_5' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-43_195_421747027226172917-1/-ext-10000/_temporary/0/task_20190629174143_0005_m_000000
19/06/29 17:41:43 INFO SparkHadoopMapRedUtil: attempt_20190629174143_0005_m_000000_5: Committed
19/06/29 17:41:43 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1979 bytes result sent to driver
19/06/29 17:41:43 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 29 ms on localhost (executor driver) (1/1)
19/06/29 17:41:43 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
19/06/29 17:41:43 INFO DAGScheduler: ResultStage 5 (main at NativeMethodAccessorImpl.java:0) finished in 0.074 s
19/06/29 17:41:43 INFO DAGScheduler: Job 5 finished: main at NativeMethodAccessorImpl.java:0, took 0.077158 s
19/06/29 17:41:43 INFO FileFormatWriter: Write Job 8f378815-6afc-4f9e-9f6e-ed3ab6add855 committed.
19/06/29 17:41:43 INFO FileFormatWriter: Finished processing stats for write job 8f378815-6afc-4f9e-9f6e-ed3ab6add855.
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:43 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-43_195_421747027226172917-1/-ext-10000/part-00000-94dbfdc4-446b-4c24-97fe-020813ec3416-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-94dbfdc4-446b-4c24-97fe-020813ec3416-c000, Status:true
19/06/29 17:41:43 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:43 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:43 INFO log: Updated size of table pg_type to 123
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:43 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:43 INFO log: Updated size of table pg_type to 123
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-43_589_4809250181360587760-1
19/06/29 17:41:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:43 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:43 INFO DAGScheduler: Got job 6 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:43 INFO DAGScheduler: Final stage: ResultStage 6 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:43 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:43 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:43 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[13] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:43 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 215.8 KB, free 365.0 MB)
19/06/29 17:41:43 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 76.1 KB, free 364.9 MB)
19/06/29 17:41:43 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 365.9 MB)
19/06/29 17:41:43 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:43 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
19/06/29 17:41:43 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:43 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
19/06/29 17:41:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:43 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174143_0006_m_000000_6' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-43_589_4809250181360587760-1/-ext-10000/_temporary/0/task_20190629174143_0006_m_000000
19/06/29 17:41:43 INFO SparkHadoopMapRedUtil: attempt_20190629174143_0006_m_000000_6: Committed
19/06/29 17:41:43 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1979 bytes result sent to driver
19/06/29 17:41:43 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 24 ms on localhost (executor driver) (1/1)
19/06/29 17:41:43 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
19/06/29 17:41:43 INFO DAGScheduler: ResultStage 6 (main at NativeMethodAccessorImpl.java:0) finished in 0.065 s
19/06/29 17:41:43 INFO DAGScheduler: Job 6 finished: main at NativeMethodAccessorImpl.java:0, took 0.068953 s
19/06/29 17:41:43 INFO FileFormatWriter: Write Job efc460f1-13d8-4960-a698-756f13aa6f93 committed.
19/06/29 17:41:43 INFO FileFormatWriter: Finished processing stats for write job efc460f1-13d8-4960-a698-756f13aa6f93.
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:43 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-43_589_4809250181360587760-1/-ext-10000/part-00000-00e82bb8-3dbe-4c67-9bfa-086af033398a-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-00e82bb8-3dbe-4c67-9bfa-086af033398a-c000, Status:true
19/06/29 17:41:43 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:43 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:43 INFO log: Updated size of table pg_type to 165
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:43 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:43 INFO log: Updated size of table pg_type to 165
19/06/29 17:41:43 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:43 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:43 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-43_966_5713071580318838333-1
19/06/29 17:41:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:44 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:44 INFO DAGScheduler: Got job 7 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:44 INFO DAGScheduler: Final stage: ResultStage 7 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:44 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:44 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:44 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[15] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:44 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 215.8 KB, free 364.7 MB)
19/06/29 17:41:44 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 76.1 KB, free 364.6 MB)
19/06/29 17:41:44 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 365.9 MB)
19/06/29 17:41:44 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:44 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
19/06/29 17:41:44 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:44 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
19/06/29 17:41:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:44 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174143_0007_m_000000_7' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-43_966_5713071580318838333-1/-ext-10000/_temporary/0/task_20190629174143_0007_m_000000
19/06/29 17:41:44 INFO SparkHadoopMapRedUtil: attempt_20190629174143_0007_m_000000_7: Committed
19/06/29 17:41:44 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1979 bytes result sent to driver
19/06/29 17:41:44 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 38 ms on localhost (executor driver) (1/1)
19/06/29 17:41:44 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
19/06/29 17:41:44 INFO DAGScheduler: ResultStage 7 (main at NativeMethodAccessorImpl.java:0) finished in 0.085 s
19/06/29 17:41:44 INFO DAGScheduler: Job 7 finished: main at NativeMethodAccessorImpl.java:0, took 0.089316 s
19/06/29 17:41:44 INFO FileFormatWriter: Write Job 16bf4ac1-f305-4339-8d20-587923ddb20a committed.
19/06/29 17:41:44 INFO FileFormatWriter: Finished processing stats for write job 16bf4ac1-f305-4339-8d20-587923ddb20a.
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:44 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:44 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-43_966_5713071580318838333-1/-ext-10000/part-00000-06db23e2-d2ff-41d7-838a-5760f7c305f1-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-06db23e2-d2ff-41d7-838a-5760f7c305f1-c000, Status:true
19/06/29 17:41:44 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:44 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:44 INFO log: Updated size of table pg_type to 205
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:44 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:44 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:44 INFO log: Updated size of table pg_type to 205
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:44 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-44_357_9062356193930470851-1
19/06/29 17:41:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:44 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:44 INFO DAGScheduler: Got job 8 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:44 INFO DAGScheduler: Final stage: ResultStage 8 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:44 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:44 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:44 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[17] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:44 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 215.8 KB, free 364.4 MB)
19/06/29 17:41:44 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 76.1 KB, free 364.3 MB)
19/06/29 17:41:44 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 365.8 MB)
19/06/29 17:41:44 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:44 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
19/06/29 17:41:44 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:44 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
19/06/29 17:41:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:44 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174144_0008_m_000000_8' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-44_357_9062356193930470851-1/-ext-10000/_temporary/0/task_20190629174144_0008_m_000000
19/06/29 17:41:44 INFO SparkHadoopMapRedUtil: attempt_20190629174144_0008_m_000000_8: Committed
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 146
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 161
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 118
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 102
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 193
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 116
19/06/29 17:41:44 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2022 bytes result sent to driver
19/06/29 17:41:44 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 71 ms on localhost (executor driver) (1/1)
19/06/29 17:41:44 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
19/06/29 17:41:44 INFO DAGScheduler: ResultStage 8 (main at NativeMethodAccessorImpl.java:0) finished in 0.100 s
19/06/29 17:41:44 INFO DAGScheduler: Job 8 finished: main at NativeMethodAccessorImpl.java:0, took 0.103527 s
19/06/29 17:41:44 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 365.9 MB)
19/06/29 17:41:44 INFO FileFormatWriter: Write Job 103f4e0f-c9e0-4f44-9e33-7377709c55c1 committed.
19/06/29 17:41:44 INFO FileFormatWriter: Finished processing stats for write job 103f4e0f-c9e0-4f44-9e33-7377709c55c1.
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 179
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 220
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 187
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 127
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 103
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 149
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 166
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 164
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 229
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 227
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 163
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 135
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 107
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 233
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 159
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 192
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 190
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 225
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 191
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 162
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 148
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 112
19/06/29 17:41:44 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:44 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-44_357_9062356193930470851-1/-ext-10000/part-00000-4d39603e-0573-4c7b-9984-4537be9a560f-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-4d39603e-0573-4c7b-9984-4537be9a560f-c000, Status:true
19/06/29 17:41:44 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 365.9 MB)
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 203
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 221
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 176
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 133
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 124
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 231
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 157
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 100
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 119
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 98
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 142
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 205
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 110
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 139
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 182
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 128
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 180
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 172
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 228
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 99
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 153
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 92
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 144
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 217
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 183
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 125
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 188
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 154
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 141
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 212
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 143
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 173
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 137
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 195
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 170
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 197
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 165
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 239
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 109
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 196
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 121
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 156
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 94
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 108
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 96
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 236
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 222
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 151
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 211
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 126
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 207
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 213
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 174
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 215
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 204
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 226
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 130
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 136
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 138
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 223
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 224
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 238
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 104
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 167
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 194
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 114
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 90
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 200
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 120
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 171
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 199
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 169
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 115
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 186
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 185
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 232
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 131
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 147
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 209
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 117
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 93
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 132
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 140
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 202
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 198
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 150
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 230
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 134
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 208
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 101
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 113
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 210
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 219
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 129
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 168
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 95
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 201
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 237
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 181
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 122
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 216
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 235
19/06/29 17:41:44 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 366.0 MB)
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 178
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 175
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 184
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 177
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 145
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 189
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 91
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 160
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 152
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 106
19/06/29 17:41:44 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 366.1 MB)
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 214
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 218
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 111
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 105
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 234
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 158
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 206
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 123
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 97
19/06/29 17:41:44 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:44 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 366.2 MB)
19/06/29 17:41:44 INFO ContextCleaner: Cleaned accumulator 155
19/06/29 17:41:44 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:44 INFO log: Updated size of table pg_type to 245
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:44 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:44 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:44 INFO log: Updated size of table pg_type to 245
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:44 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-44_769_6244025562733559340-1
19/06/29 17:41:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:44 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:44 INFO DAGScheduler: Got job 9 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:44 INFO DAGScheduler: Final stage: ResultStage 9 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:44 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:44 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:44 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[19] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:44 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 215.8 KB, free 365.5 MB)
19/06/29 17:41:44 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 76.1 KB, free 365.4 MB)
19/06/29 17:41:44 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 366.1 MB)
19/06/29 17:41:44 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:44 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
19/06/29 17:41:44 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:44 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
19/06/29 17:41:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:44 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174144_0009_m_000000_9' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-44_769_6244025562733559340-1/-ext-10000/_temporary/0/task_20190629174144_0009_m_000000
19/06/29 17:41:44 INFO SparkHadoopMapRedUtil: attempt_20190629174144_0009_m_000000_9: Committed
19/06/29 17:41:44 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 1979 bytes result sent to driver
19/06/29 17:41:44 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 53 ms on localhost (executor driver) (1/1)
19/06/29 17:41:44 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
19/06/29 17:41:44 INFO DAGScheduler: ResultStage 9 (main at NativeMethodAccessorImpl.java:0) finished in 0.081 s
19/06/29 17:41:44 INFO DAGScheduler: Job 9 finished: main at NativeMethodAccessorImpl.java:0, took 0.083093 s
19/06/29 17:41:44 INFO FileFormatWriter: Write Job 7280efef-c918-4096-8cd1-70a91940cc80 committed.
19/06/29 17:41:44 INFO FileFormatWriter: Finished processing stats for write job 7280efef-c918-4096-8cd1-70a91940cc80.
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:44 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:44 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:44 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-44_769_6244025562733559340-1/-ext-10000/part-00000-75f9602c-d945-4e0b-a22e-13ae03a2d53e-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-75f9602c-d945-4e0b-a22e-13ae03a2d53e-c000, Status:true
19/06/29 17:41:44 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:45 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:45 INFO log: Updated size of table pg_type to 285
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:45 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:45 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:45 INFO log: Updated size of table pg_type to 285
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:45 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-45_262_4560492236630824447-1
19/06/29 17:41:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:45 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:45 INFO DAGScheduler: Got job 10 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:45 INFO DAGScheduler: Final stage: ResultStage 10 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:45 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:45 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:45 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[21] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:45 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 215.8 KB, free 365.2 MB)
19/06/29 17:41:45 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 76.1 KB, free 365.2 MB)
19/06/29 17:41:45 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 366.0 MB)
19/06/29 17:41:45 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:45 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
19/06/29 17:41:45 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:45 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
19/06/29 17:41:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:45 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174145_0010_m_000000_10' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-45_262_4560492236630824447-1/-ext-10000/_temporary/0/task_20190629174145_0010_m_000000
19/06/29 17:41:45 INFO SparkHadoopMapRedUtil: attempt_20190629174145_0010_m_000000_10: Committed
19/06/29 17:41:45 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 2022 bytes result sent to driver
19/06/29 17:41:45 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 25 ms on localhost (executor driver) (1/1)
19/06/29 17:41:45 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
19/06/29 17:41:45 INFO DAGScheduler: ResultStage 10 (main at NativeMethodAccessorImpl.java:0) finished in 0.052 s
19/06/29 17:41:45 INFO DAGScheduler: Job 10 finished: main at NativeMethodAccessorImpl.java:0, took 0.054759 s
19/06/29 17:41:45 INFO FileFormatWriter: Write Job 31fc66a2-f549-4608-8cca-3835fe3fea1c committed.
19/06/29 17:41:45 INFO FileFormatWriter: Finished processing stats for write job 31fc66a2-f549-4608-8cca-3835fe3fea1c.
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:45 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:45 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-45_262_4560492236630824447-1/-ext-10000/part-00000-b9eb91ca-2ef1-44db-84e1-4d1a696a066f-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-b9eb91ca-2ef1-44db-84e1-4d1a696a066f-c000, Status:true
19/06/29 17:41:45 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:45 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:45 INFO log: Updated size of table pg_type to 323
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:45 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:45 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:45 INFO log: Updated size of table pg_type to 323
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:45 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-45_585_469872793273601453-1
19/06/29 17:41:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:45 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:45 INFO DAGScheduler: Got job 11 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:45 INFO DAGScheduler: Final stage: ResultStage 11 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:45 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:45 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:45 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[23] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:45 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 215.8 KB, free 365.0 MB)
19/06/29 17:41:45 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 76.1 KB, free 364.9 MB)
19/06/29 17:41:45 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 365.9 MB)
19/06/29 17:41:45 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:45 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
19/06/29 17:41:45 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:45 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
19/06/29 17:41:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:45 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174145_0011_m_000000_11' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-45_585_469872793273601453-1/-ext-10000/_temporary/0/task_20190629174145_0011_m_000000
19/06/29 17:41:45 INFO SparkHadoopMapRedUtil: attempt_20190629174145_0011_m_000000_11: Committed
19/06/29 17:41:45 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 1979 bytes result sent to driver
19/06/29 17:41:45 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 27 ms on localhost (executor driver) (1/1)
19/06/29 17:41:45 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
19/06/29 17:41:45 INFO DAGScheduler: ResultStage 11 (main at NativeMethodAccessorImpl.java:0) finished in 0.064 s
19/06/29 17:41:45 INFO DAGScheduler: Job 11 finished: main at NativeMethodAccessorImpl.java:0, took 0.065831 s
19/06/29 17:41:45 INFO FileFormatWriter: Write Job 9afdda4c-3e84-43c4-bc39-aed9d5d29244 committed.
19/06/29 17:41:45 INFO FileFormatWriter: Finished processing stats for write job 9afdda4c-3e84-43c4-bc39-aed9d5d29244.
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:45 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:45 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-45_585_469872793273601453-1/-ext-10000/part-00000-58ea6ae8-bbf4-471d-8688-f09e737b96d3-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-58ea6ae8-bbf4-471d-8688-f09e737b96d3-c000, Status:true
19/06/29 17:41:45 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:45 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:45 INFO log: Updated size of table pg_type to 368
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:45 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:45 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:45 INFO log: Updated size of table pg_type to 368
19/06/29 17:41:45 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:45 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-46_017_6354777621143752900-1
19/06/29 17:41:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:46 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:46 INFO DAGScheduler: Got job 12 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:46 INFO DAGScheduler: Final stage: ResultStage 12 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:46 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:46 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:46 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[25] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:46 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 215.8 KB, free 364.7 MB)
19/06/29 17:41:46 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 76.1 KB, free 364.6 MB)
19/06/29 17:41:46 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 365.9 MB)
19/06/29 17:41:46 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:46 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
19/06/29 17:41:46 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:46 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
19/06/29 17:41:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:46 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174146_0012_m_000000_12' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-46_017_6354777621143752900-1/-ext-10000/_temporary/0/task_20190629174146_0012_m_000000
19/06/29 17:41:46 INFO SparkHadoopMapRedUtil: attempt_20190629174146_0012_m_000000_12: Committed
19/06/29 17:41:46 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 1979 bytes result sent to driver
19/06/29 17:41:46 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 23 ms on localhost (executor driver) (1/1)
19/06/29 17:41:46 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
19/06/29 17:41:46 INFO DAGScheduler: ResultStage 12 (main at NativeMethodAccessorImpl.java:0) finished in 0.062 s
19/06/29 17:41:46 INFO DAGScheduler: Job 12 finished: main at NativeMethodAccessorImpl.java:0, took 0.063947 s
19/06/29 17:41:46 INFO FileFormatWriter: Write Job 0f19052c-8ae4-4644-9ef5-4e8ae06c9caa committed.
19/06/29 17:41:46 INFO FileFormatWriter: Finished processing stats for write job 0f19052c-8ae4-4644-9ef5-4e8ae06c9caa.
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:46 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-46_017_6354777621143752900-1/-ext-10000/part-00000-0b57eaae-7f05-4987-b66d-652e4970502f-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-0b57eaae-7f05-4987-b66d-652e4970502f-c000, Status:true
19/06/29 17:41:46 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:46 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:46 INFO log: Updated size of table pg_type to 413
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:46 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:46 INFO log: Updated size of table pg_type to 413
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-46_378_3183782477798780721-1
19/06/29 17:41:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:46 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:46 INFO DAGScheduler: Got job 13 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:46 INFO DAGScheduler: Final stage: ResultStage 13 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:46 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:46 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:46 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[27] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:46 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 215.8 KB, free 364.4 MB)
19/06/29 17:41:46 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 76.1 KB, free 364.3 MB)
19/06/29 17:41:46 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 365.8 MB)
19/06/29 17:41:46 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:46 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
19/06/29 17:41:46 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:46 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
19/06/29 17:41:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:46 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174146_0013_m_000000_13' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-46_378_3183782477798780721-1/-ext-10000/_temporary/0/task_20190629174146_0013_m_000000
19/06/29 17:41:46 INFO SparkHadoopMapRedUtil: attempt_20190629174146_0013_m_000000_13: Committed
19/06/29 17:41:46 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 1979 bytes result sent to driver
19/06/29 17:41:46 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 37 ms on localhost (executor driver) (1/1)
19/06/29 17:41:46 INFO DAGScheduler: ResultStage 13 (main at NativeMethodAccessorImpl.java:0) finished in 0.068 s
19/06/29 17:41:46 INFO DAGScheduler: Job 13 finished: main at NativeMethodAccessorImpl.java:0, took 0.070265 s
19/06/29 17:41:46 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
19/06/29 17:41:46 INFO FileFormatWriter: Write Job 8847ac44-b85b-42f0-b690-d27e80204959 committed.
19/06/29 17:41:46 INFO FileFormatWriter: Finished processing stats for write job 8847ac44-b85b-42f0-b690-d27e80204959.
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:46 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-46_378_3183782477798780721-1/-ext-10000/part-00000-4dfb543e-da0c-4cd4-923c-307a4c52ee8c-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-4dfb543e-da0c-4cd4-923c-307a4c52ee8c-c000, Status:true
19/06/29 17:41:46 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:46 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:46 INFO log: Updated size of table pg_type to 460
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:46 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:46 INFO log: Updated size of table pg_type to 460
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-46_705_1555020968280527852-1
19/06/29 17:41:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:46 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:46 INFO DAGScheduler: Got job 14 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:46 INFO DAGScheduler: Final stage: ResultStage 14 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:46 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:46 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:46 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[29] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:46 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 215.8 KB, free 364.1 MB)
19/06/29 17:41:46 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 76.1 KB, free 364.0 MB)
19/06/29 17:41:46 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 365.7 MB)
19/06/29 17:41:46 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:46 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
19/06/29 17:41:46 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:46 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
19/06/29 17:41:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:46 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174146_0014_m_000000_14' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-46_705_1555020968280527852-1/-ext-10000/_temporary/0/task_20190629174146_0014_m_000000
19/06/29 17:41:46 INFO SparkHadoopMapRedUtil: attempt_20190629174146_0014_m_000000_14: Committed
19/06/29 17:41:46 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 1979 bytes result sent to driver
19/06/29 17:41:46 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 22 ms on localhost (executor driver) (1/1)
19/06/29 17:41:46 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
19/06/29 17:41:46 INFO DAGScheduler: ResultStage 14 (main at NativeMethodAccessorImpl.java:0) finished in 0.050 s
19/06/29 17:41:46 INFO DAGScheduler: Job 14 finished: main at NativeMethodAccessorImpl.java:0, took 0.052597 s
19/06/29 17:41:46 INFO FileFormatWriter: Write Job f35dbb20-54a4-4cd8-9f01-951ec6765a84 committed.
19/06/29 17:41:46 INFO FileFormatWriter: Finished processing stats for write job f35dbb20-54a4-4cd8-9f01-951ec6765a84.
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:46 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-46_705_1555020968280527852-1/-ext-10000/part-00000-945dd1e8-3cbd-476b-932d-01b5aa438916-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-945dd1e8-3cbd-476b-932d-01b5aa438916-c000, Status:true
19/06/29 17:41:46 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:46 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:46 INFO log: Updated size of table pg_type to 507
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:46 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:46 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:46 INFO log: Updated size of table pg_type to 507
19/06/29 17:41:46 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-47_012_4364260846307709948-1
19/06/29 17:41:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:47 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:47 INFO DAGScheduler: Got job 15 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:47 INFO DAGScheduler: Final stage: ResultStage 15 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:47 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:47 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:47 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[31] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:47 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 215.8 KB, free 363.8 MB)
19/06/29 17:41:47 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 76.1 KB, free 363.7 MB)
19/06/29 17:41:47 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 365.6 MB)
19/06/29 17:41:47 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:47 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
19/06/29 17:41:47 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:47 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 265
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 287
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 281
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 349
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 353
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 415
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 427
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 378
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 350
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 316
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 278
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 364
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 373
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 375
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 392
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 390
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 256
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 398
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 300
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 334
19/06/29 17:41:47 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 365.7 MB)
19/06/29 17:41:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 254
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 393
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 242
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 407
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 252
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 366
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 443
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 441
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 372
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 306
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 445
19/06/29 17:41:47 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 365.8 MB)
19/06/29 17:41:47 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174147_0015_m_000000_15' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-47_012_4364260846307709948-1/-ext-10000/_temporary/0/task_20190629174147_0015_m_000000
19/06/29 17:41:47 INFO SparkHadoopMapRedUtil: attempt_20190629174147_0015_m_000000_15: Committed
19/06/29 17:41:47 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 1979 bytes result sent to driver
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 326
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 358
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 319
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 377
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 404
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 385
19/06/29 17:41:47 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 37 ms on localhost (executor driver) (1/1)
19/06/29 17:41:47 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 382
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 356
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 255
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 436
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 298
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 336
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 410
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 276
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 258
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 355
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 333
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 291
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 274
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 391
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 288
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 275
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 314
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 422
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 367
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 368
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 263
19/06/29 17:41:47 INFO DAGScheduler: ResultStage 15 (main at NativeMethodAccessorImpl.java:0) finished in 0.106 s
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 332
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 341
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 308
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 318
19/06/29 17:41:47 INFO DAGScheduler: Job 15 finished: main at NativeMethodAccessorImpl.java:0, took 0.109001 s
19/06/29 17:41:47 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 365.9 MB)
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 402
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 396
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 419
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 304
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 421
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 285
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 269
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 411
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 310
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 266
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 413
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 305
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 327
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 357
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 380
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 354
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 397
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 409
19/06/29 17:41:47 INFO FileFormatWriter: Write Job e0b5df6f-e681-45d2-9734-210f5b828552 committed.
19/06/29 17:41:47 INFO FileFormatWriter: Finished processing stats for write job e0b5df6f-e681-45d2-9734-210f5b828552.
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 365.9 MB)
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 425
19/06/29 17:41:47 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:47 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-47_012_4364260846307709948-1/-ext-10000/part-00000-67b9c6f7-f41a-48c3-8431-257003699e39-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-67b9c6f7-f41a-48c3-8431-257003699e39-c000, Status:true
19/06/29 17:41:47 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 366.0 MB)
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 438
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 387
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 260
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 241
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 329
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 370
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 386
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 337
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 297
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 315
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 248
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 335
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 346
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 426
19/06/29 17:41:47 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 366.1 MB)
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 250
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 290
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 369
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 363
19/06/29 17:41:47 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 366.2 MB)
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 312
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 429
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 268
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 289
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 395
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 406
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 418
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 245
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 251
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 273
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 389
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 299
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 338
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 295
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 259
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 383
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 284
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 412
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 246
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 271
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 405
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 317
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 401
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 277
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 360
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 449
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 362
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 352
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 351
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 447
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 430
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 437
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 440
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 328
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 359
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 348
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 301
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 249
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 414
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 313
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 420
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 330
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 394
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 446
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 302
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 435
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 286
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 403
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 293
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 347
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 417
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 381
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 408
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 340
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 400
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 442
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 325
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 432
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 439
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 321
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 331
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 434
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 309
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 345
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 270
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 262
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 343
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 244
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 257
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 324
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 424
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 339
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 431
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 322
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 282
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 428
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 344
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 243
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 296
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 280
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 279
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 371
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 376
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 283
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 374
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 379
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 267
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 272
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 264
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 311
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 384
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 399
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 307
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 365
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 303
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 253
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 292
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 388
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 294
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 240
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 433
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 423
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 448
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 320
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 247
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 342
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 261
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 416
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 323
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 361
19/06/29 17:41:47 INFO ContextCleaner: Cleaned accumulator 444
19/06/29 17:41:47 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:47 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:47 INFO log: Updated size of table pg_type to 554
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:47 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:47 INFO log: Updated size of table pg_type to 554
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-47_381_2983965724463423944-1
19/06/29 17:41:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:47 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:47 INFO DAGScheduler: Got job 16 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:47 INFO DAGScheduler: Final stage: ResultStage 16 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:47 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:47 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:47 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[33] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:47 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 215.8 KB, free 365.5 MB)
19/06/29 17:41:47 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 76.1 KB, free 365.4 MB)
19/06/29 17:41:47 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 366.1 MB)
19/06/29 17:41:47 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[33] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:47 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
19/06/29 17:41:47 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:47 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
19/06/29 17:41:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:47 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174147_0016_m_000000_16' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-47_381_2983965724463423944-1/-ext-10000/_temporary/0/task_20190629174147_0016_m_000000
19/06/29 17:41:47 INFO SparkHadoopMapRedUtil: attempt_20190629174147_0016_m_000000_16: Committed
19/06/29 17:41:47 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 1979 bytes result sent to driver
19/06/29 17:41:47 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 25 ms on localhost (executor driver) (1/1)
19/06/29 17:41:47 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
19/06/29 17:41:47 INFO DAGScheduler: ResultStage 16 (main at NativeMethodAccessorImpl.java:0) finished in 0.051 s
19/06/29 17:41:47 INFO DAGScheduler: Job 16 finished: main at NativeMethodAccessorImpl.java:0, took 0.054286 s
19/06/29 17:41:47 INFO FileFormatWriter: Write Job 0f814ab8-55f4-4223-9d83-c88ce186364e committed.
19/06/29 17:41:47 INFO FileFormatWriter: Finished processing stats for write job 0f814ab8-55f4-4223-9d83-c88ce186364e.
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:47 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-47_381_2983965724463423944-1/-ext-10000/part-00000-aeadeaf2-d568-469d-bfb4-2954d003296e-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-aeadeaf2-d568-469d-bfb4-2954d003296e-c000, Status:true
19/06/29 17:41:47 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:47 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:47 INFO log: Updated size of table pg_type to 606
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:47 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:47 INFO log: Updated size of table pg_type to 606
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-47_675_6175093687626780012-1
19/06/29 17:41:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:47 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:47 INFO DAGScheduler: Got job 17 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:47 INFO DAGScheduler: Final stage: ResultStage 17 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:47 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:47 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:47 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[35] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:47 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 215.8 KB, free 365.2 MB)
19/06/29 17:41:47 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 76.1 KB, free 365.2 MB)
19/06/29 17:41:47 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 366.0 MB)
19/06/29 17:41:47 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[35] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:47 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks
19/06/29 17:41:47 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:47 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
19/06/29 17:41:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:47 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174147_0017_m_000000_17' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-47_675_6175093687626780012-1/-ext-10000/_temporary/0/task_20190629174147_0017_m_000000
19/06/29 17:41:47 INFO SparkHadoopMapRedUtil: attempt_20190629174147_0017_m_000000_17: Committed
19/06/29 17:41:47 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 1979 bytes result sent to driver
19/06/29 17:41:47 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 19 ms on localhost (executor driver) (1/1)
19/06/29 17:41:47 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
19/06/29 17:41:47 INFO DAGScheduler: ResultStage 17 (main at NativeMethodAccessorImpl.java:0) finished in 0.043 s
19/06/29 17:41:47 INFO DAGScheduler: Job 17 finished: main at NativeMethodAccessorImpl.java:0, took 0.046168 s
19/06/29 17:41:47 INFO FileFormatWriter: Write Job f7171530-7fe2-410c-ab20-35e400b3e74d committed.
19/06/29 17:41:47 INFO FileFormatWriter: Finished processing stats for write job f7171530-7fe2-410c-ab20-35e400b3e74d.
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:47 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-47_675_6175093687626780012-1/-ext-10000/part-00000-262ebc2c-65f9-423f-b4c0-4a46ea616610-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-262ebc2c-65f9-423f-b4c0-4a46ea616610-c000, Status:true
19/06/29 17:41:47 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:47 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:47 INFO log: Updated size of table pg_type to 653
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:47 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:47 INFO log: Updated size of table pg_type to 653
19/06/29 17:41:47 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:47 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-47_957_5716786552103879363-1
19/06/29 17:41:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:48 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:48 INFO DAGScheduler: Got job 18 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:48 INFO DAGScheduler: Final stage: ResultStage 18 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:48 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:48 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:48 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[37] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:48 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 215.8 KB, free 365.0 MB)
19/06/29 17:41:48 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 76.1 KB, free 364.9 MB)
19/06/29 17:41:48 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 365.9 MB)
19/06/29 17:41:48 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[37] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:48 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
19/06/29 17:41:48 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:48 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
19/06/29 17:41:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:48 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174147_0018_m_000000_18' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-47_957_5716786552103879363-1/-ext-10000/_temporary/0/task_20190629174147_0018_m_000000
19/06/29 17:41:48 INFO SparkHadoopMapRedUtil: attempt_20190629174147_0018_m_000000_18: Committed
19/06/29 17:41:48 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 1979 bytes result sent to driver
19/06/29 17:41:48 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 25 ms on localhost (executor driver) (1/1)
19/06/29 17:41:48 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
19/06/29 17:41:48 INFO DAGScheduler: ResultStage 18 (main at NativeMethodAccessorImpl.java:0) finished in 0.056 s
19/06/29 17:41:48 INFO DAGScheduler: Job 18 finished: main at NativeMethodAccessorImpl.java:0, took 0.057252 s
19/06/29 17:41:48 INFO FileFormatWriter: Write Job 80c46cbc-6a9a-406c-9f3c-d8321ecc8b65 committed.
19/06/29 17:41:48 INFO FileFormatWriter: Finished processing stats for write job 80c46cbc-6a9a-406c-9f3c-d8321ecc8b65.
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:48 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-47_957_5716786552103879363-1/-ext-10000/part-00000-59ba5e16-5ab3-4375-8a6b-b32400a91917-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-59ba5e16-5ab3-4375-8a6b-b32400a91917-c000, Status:true
19/06/29 17:41:48 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:48 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:48 INFO log: Updated size of table pg_type to 703
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:48 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:48 INFO log: Updated size of table pg_type to 703
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-48_251_5346885948262384201-1
19/06/29 17:41:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:48 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:48 INFO DAGScheduler: Got job 19 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:48 INFO DAGScheduler: Final stage: ResultStage 19 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:48 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:48 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:48 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[39] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:48 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 215.8 KB, free 364.7 MB)
19/06/29 17:41:48 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 76.1 KB, free 364.6 MB)
19/06/29 17:41:48 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 365.9 MB)
19/06/29 17:41:48 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[39] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:48 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks
19/06/29 17:41:48 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:48 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
19/06/29 17:41:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:48 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174148_0019_m_000000_19' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-48_251_5346885948262384201-1/-ext-10000/_temporary/0/task_20190629174148_0019_m_000000
19/06/29 17:41:48 INFO SparkHadoopMapRedUtil: attempt_20190629174148_0019_m_000000_19: Committed
19/06/29 17:41:48 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 2022 bytes result sent to driver
19/06/29 17:41:48 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 24 ms on localhost (executor driver) (1/1)
19/06/29 17:41:48 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
19/06/29 17:41:48 INFO DAGScheduler: ResultStage 19 (main at NativeMethodAccessorImpl.java:0) finished in 0.058 s
19/06/29 17:41:48 INFO DAGScheduler: Job 19 finished: main at NativeMethodAccessorImpl.java:0, took 0.062701 s
19/06/29 17:41:48 INFO FileFormatWriter: Write Job e0682d7d-40a1-49a2-a537-6dd08dc1ff23 committed.
19/06/29 17:41:48 INFO FileFormatWriter: Finished processing stats for write job e0682d7d-40a1-49a2-a537-6dd08dc1ff23.
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:48 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-48_251_5346885948262384201-1/-ext-10000/part-00000-6d23ffee-9afc-4df3-ad69-07c3660f6094-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-6d23ffee-9afc-4df3-ad69-07c3660f6094-c000, Status:true
19/06/29 17:41:48 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:48 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:48 INFO log: Updated size of table pg_type to 753
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:48 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:48 INFO log: Updated size of table pg_type to 753
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-48_541_3738757042379587161-1
19/06/29 17:41:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:48 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:48 INFO DAGScheduler: Got job 20 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:48 INFO DAGScheduler: Final stage: ResultStage 20 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:48 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:48 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:48 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[41] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:48 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 215.8 KB, free 364.4 MB)
19/06/29 17:41:48 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 76.1 KB, free 364.3 MB)
19/06/29 17:41:48 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 365.8 MB)
19/06/29 17:41:48 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[41] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:48 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks
19/06/29 17:41:48 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 8172 bytes)
19/06/29 17:41:48 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
19/06/29 17:41:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:48 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174148_0020_m_000000_20' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-48_541_3738757042379587161-1/-ext-10000/_temporary/0/task_20190629174148_0020_m_000000
19/06/29 17:41:48 INFO SparkHadoopMapRedUtil: attempt_20190629174148_0020_m_000000_20: Committed
19/06/29 17:41:48 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 1979 bytes result sent to driver
19/06/29 17:41:48 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 20 ms on localhost (executor driver) (1/1)
19/06/29 17:41:48 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
19/06/29 17:41:48 INFO DAGScheduler: ResultStage 20 (main at NativeMethodAccessorImpl.java:0) finished in 0.049 s
19/06/29 17:41:48 INFO DAGScheduler: Job 20 finished: main at NativeMethodAccessorImpl.java:0, took 0.050585 s
19/06/29 17:41:48 INFO FileFormatWriter: Write Job d6ab827a-9d3f-4a9b-a6dd-772bf3c61581 committed.
19/06/29 17:41:48 INFO FileFormatWriter: Finished processing stats for write job d6ab827a-9d3f-4a9b-a6dd-772bf3c61581.
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:48 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-48_541_3738757042379587161-1/-ext-10000/part-00000-0273de89-bf30-49ce-8cfd-1bbe6e671239-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-0273de89-bf30-49ce-8cfd-1bbe6e671239-c000, Status:true
19/06/29 17:41:48 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:48 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:48 INFO log: Updated size of table pg_type to 802
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:48 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:48 INFO log: Updated size of table pg_type to 802
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-48_814_5238962033324407537-1
19/06/29 17:41:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:48 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:48 INFO DAGScheduler: Got job 21 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:48 INFO DAGScheduler: Final stage: ResultStage 21 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:48 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:48 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:48 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[43] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:48 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 215.8 KB, free 364.1 MB)
19/06/29 17:41:48 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 76.1 KB, free 364.0 MB)
19/06/29 17:41:48 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 365.7 MB)
19/06/29 17:41:48 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[43] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:48 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks
19/06/29 17:41:48 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:48 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)
19/06/29 17:41:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:48 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174148_0021_m_000000_21' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-48_814_5238962033324407537-1/-ext-10000/_temporary/0/task_20190629174148_0021_m_000000
19/06/29 17:41:48 INFO SparkHadoopMapRedUtil: attempt_20190629174148_0021_m_000000_21: Committed
19/06/29 17:41:48 INFO Executor: Finished task 0.0 in stage 21.0 (TID 21). 2022 bytes result sent to driver
19/06/29 17:41:48 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 35 ms on localhost (executor driver) (1/1)
19/06/29 17:41:48 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
19/06/29 17:41:48 INFO DAGScheduler: ResultStage 21 (main at NativeMethodAccessorImpl.java:0) finished in 0.065 s
19/06/29 17:41:48 INFO DAGScheduler: Job 21 finished: main at NativeMethodAccessorImpl.java:0, took 0.069171 s
19/06/29 17:41:48 INFO FileFormatWriter: Write Job 4e0b8e35-9638-4fb8-a98c-befd9468a546 committed.
19/06/29 17:41:48 INFO FileFormatWriter: Finished processing stats for write job 4e0b8e35-9638-4fb8-a98c-befd9468a546.
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:48 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:48 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-48_814_5238962033324407537-1/-ext-10000/part-00000-03f5103f-c03b-4b93-8951-46fba369aaf0-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-03f5103f-c03b-4b93-8951-46fba369aaf0-c000, Status:true
19/06/29 17:41:49 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:49 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:49 INFO log: Updated size of table pg_type to 845
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:49 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:49 INFO log: Updated size of table pg_type to 845
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-49_161_2682537377555750596-1
19/06/29 17:41:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:49 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:49 INFO DAGScheduler: Got job 22 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:49 INFO DAGScheduler: Final stage: ResultStage 22 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:49 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:49 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:49 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[45] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:49 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 215.8 KB, free 363.8 MB)
19/06/29 17:41:49 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 76.1 KB, free 363.7 MB)
19/06/29 17:41:49 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 365.6 MB)
19/06/29 17:41:49 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[45] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:49 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks
19/06/29 17:41:49 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22, localhost, executor driver, partition 0, PROCESS_LOCAL, 8180 bytes)
19/06/29 17:41:49 INFO Executor: Running task 0.0 in stage 22.0 (TID 22)
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 512
19/06/29 17:41:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 559
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 518
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 626
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 469
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 514
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 562
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 617
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 619
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 489
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 620
19/06/29 17:41:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 525
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 644
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 589
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 570
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 538
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 466
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 603
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 650
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 590
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 461
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 542
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 577
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 566
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 612
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 523
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 486
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 598
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 473
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 496
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 586
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 459
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 532
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 594
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 641
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 592
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 564
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 657
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 499
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 543
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 584
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 558
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 521
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 501
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 565
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 643
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 567
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 547
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 493
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 463
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 470
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 636
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 456
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 475
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 587
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 629
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 527
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 544
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 623
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 498
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 545
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 579
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 462
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 455
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 607
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 526
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 621
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 647
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 465
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 504
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 635
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 571
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 601
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 561
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 511
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 637
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 658
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 548
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 557
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 572
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 588
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 560
19/06/29 17:41:49 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 365.7 MB)
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 536
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 507
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 596
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 597
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 481
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 628
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 649
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 476
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 517
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 639
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 648
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 485
19/06/29 17:41:49 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 365.8 MB)
19/06/29 17:41:49 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174149_0022_m_000000_22' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-49_161_2682537377555750596-1/-ext-10000/_temporary/0/task_20190629174149_0022_m_000000
19/06/29 17:41:49 INFO SparkHadoopMapRedUtil: attempt_20190629174149_0022_m_000000_22: Committed
19/06/29 17:41:49 INFO Executor: Finished task 0.0 in stage 22.0 (TID 22). 2022 bytes result sent to driver
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 575
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 595
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 554
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 604
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 581
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 606
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 580
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 468
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 555
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 642
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 602
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 631
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 472
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 551
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 464
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 622
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 634
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 508
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 520
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 608
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 651
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 573
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 630
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 633
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 585
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 625
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 531
19/06/29 17:41:49 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 81 ms on localhost (executor driver) (1/1)
19/06/29 17:41:49 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
19/06/29 17:41:49 INFO DAGScheduler: ResultStage 22 (main at NativeMethodAccessorImpl.java:0) finished in 0.112 s
19/06/29 17:41:49 INFO DAGScheduler: Job 22 finished: main at NativeMethodAccessorImpl.java:0, took 0.114308 s
19/06/29 17:41:49 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 365.9 MB)
19/06/29 17:41:49 INFO FileFormatWriter: Write Job 2a02233f-ddc4-4657-ba1e-d7b58632dc0d committed.
19/06/29 17:41:49 INFO FileFormatWriter: Finished processing stats for write job 2a02233f-ddc4-4657-ba1e-d7b58632dc0d.
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 576
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 503
19/06/29 17:41:49 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:49 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-49_161_2682537377555750596-1/-ext-10000/part-00000-a684f4f5-a096-4aef-a9be-8d32bded7138-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-a684f4f5-a096-4aef-a9be-8d32bded7138-c000, Status:true
19/06/29 17:41:49 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 365.9 MB)
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 500
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 611
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 483
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 613
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 539
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 492
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 533
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 506
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 530
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 627
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 460
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 605
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 458
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 553
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 488
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 610
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 549
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 454
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 534
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 582
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 452
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 550
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 569
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 479
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 541
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 640
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 490
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 509
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 519
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 546
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 654
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 540
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 450
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 593
19/06/29 17:41:49 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 366.0 MB)
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 516
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 515
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 655
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 471
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 453
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 477
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 487
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 457
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 599
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 480
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 556
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 645
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 646
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 497
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 638
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 478
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 502
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 583
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 451
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 484
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 568
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 535
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 537
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 653
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 529
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 609
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 552
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 563
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 474
19/06/29 17:41:49 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:49 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 366.1 MB)
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 578
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 494
19/06/29 17:41:49 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 366.2 MB)
19/06/29 17:41:49 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:49 INFO log: Updated size of table pg_type to 897
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 624
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 482
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 524
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 616
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 522
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 513
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 528
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 510
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 656
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 600
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 652
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 614
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 659
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 615
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 491
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 495
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 574
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 618
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 591
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 632
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 467
19/06/29 17:41:49 INFO ContextCleaner: Cleaned accumulator 505
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:49 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:49 INFO log: Updated size of table pg_type to 897
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-49_540_6533809721856274900-1
19/06/29 17:41:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:49 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:49 INFO DAGScheduler: Got job 23 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:49 INFO DAGScheduler: Final stage: ResultStage 23 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:49 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:49 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:49 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[47] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:49 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 215.8 KB, free 365.5 MB)
19/06/29 17:41:49 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 76.1 KB, free 365.4 MB)
19/06/29 17:41:49 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 366.1 MB)
19/06/29 17:41:49 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[47] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:49 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
19/06/29 17:41:49 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23, localhost, executor driver, partition 0, PROCESS_LOCAL, 8172 bytes)
19/06/29 17:41:49 INFO Executor: Running task 0.0 in stage 23.0 (TID 23)
19/06/29 17:41:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:49 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174149_0023_m_000000_23' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-49_540_6533809721856274900-1/-ext-10000/_temporary/0/task_20190629174149_0023_m_000000
19/06/29 17:41:49 INFO SparkHadoopMapRedUtil: attempt_20190629174149_0023_m_000000_23: Committed
19/06/29 17:41:49 INFO Executor: Finished task 0.0 in stage 23.0 (TID 23). 1979 bytes result sent to driver
19/06/29 17:41:49 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 22 ms on localhost (executor driver) (1/1)
19/06/29 17:41:49 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
19/06/29 17:41:49 INFO DAGScheduler: ResultStage 23 (main at NativeMethodAccessorImpl.java:0) finished in 0.054 s
19/06/29 17:41:49 INFO DAGScheduler: Job 23 finished: main at NativeMethodAccessorImpl.java:0, took 0.056595 s
19/06/29 17:41:49 INFO FileFormatWriter: Write Job e0775bdd-a6e5-4411-b3fb-0a3333155b91 committed.
19/06/29 17:41:49 INFO FileFormatWriter: Finished processing stats for write job e0775bdd-a6e5-4411-b3fb-0a3333155b91.
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:49 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-49_540_6533809721856274900-1/-ext-10000/part-00000-ac48e1e2-0963-484e-82ff-12bb7cf5f6a8-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-ac48e1e2-0963-484e-82ff-12bb7cf5f6a8-c000, Status:true
19/06/29 17:41:49 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:49 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:49 INFO log: Updated size of table pg_type to 951
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:49 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:49 INFO log: Updated size of table pg_type to 951
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-49_832_2950588901275591970-1
19/06/29 17:41:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:49 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:49 INFO DAGScheduler: Got job 24 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:49 INFO DAGScheduler: Final stage: ResultStage 24 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:49 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:49 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:49 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[49] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:49 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 215.8 KB, free 365.2 MB)
19/06/29 17:41:49 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 76.1 KB, free 365.2 MB)
19/06/29 17:41:49 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 366.0 MB)
19/06/29 17:41:49 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[49] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:49 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks
19/06/29 17:41:49 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:49 INFO Executor: Running task 0.0 in stage 24.0 (TID 24)
19/06/29 17:41:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:49 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174149_0024_m_000000_24' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-49_832_2950588901275591970-1/-ext-10000/_temporary/0/task_20190629174149_0024_m_000000
19/06/29 17:41:49 INFO SparkHadoopMapRedUtil: attempt_20190629174149_0024_m_000000_24: Committed
19/06/29 17:41:49 INFO Executor: Finished task 0.0 in stage 24.0 (TID 24). 1979 bytes result sent to driver
19/06/29 17:41:49 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 19 ms on localhost (executor driver) (1/1)
19/06/29 17:41:49 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
19/06/29 17:41:49 INFO DAGScheduler: ResultStage 24 (main at NativeMethodAccessorImpl.java:0) finished in 0.049 s
19/06/29 17:41:49 INFO DAGScheduler: Job 24 finished: main at NativeMethodAccessorImpl.java:0, took 0.050785 s
19/06/29 17:41:49 INFO FileFormatWriter: Write Job 2fd30753-c21c-4856-a135-b21dd5469e26 committed.
19/06/29 17:41:49 INFO FileFormatWriter: Finished processing stats for write job 2fd30753-c21c-4856-a135-b21dd5469e26.
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:49 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:49 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-49_832_2950588901275591970-1/-ext-10000/part-00000-839146c8-5e9c-4a46-8a47-0dbe4b980e71-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-839146c8-5e9c-4a46-8a47-0dbe4b980e71-c000, Status:true
19/06/29 17:41:49 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:50 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:50 INFO log: Updated size of table pg_type to 1000
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:50 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:50 INFO log: Updated size of table pg_type to 1000
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-50_112_2674020490788280539-1
19/06/29 17:41:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:50 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:50 INFO DAGScheduler: Got job 25 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:50 INFO DAGScheduler: Final stage: ResultStage 25 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:50 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:50 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:50 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[51] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:50 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 215.8 KB, free 365.0 MB)
19/06/29 17:41:50 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 76.1 KB, free 364.9 MB)
19/06/29 17:41:50 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 365.9 MB)
19/06/29 17:41:50 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[51] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:50 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks
19/06/29 17:41:50 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 25, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:50 INFO Executor: Running task 0.0 in stage 25.0 (TID 25)
19/06/29 17:41:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:50 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174150_0025_m_000000_25' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-50_112_2674020490788280539-1/-ext-10000/_temporary/0/task_20190629174150_0025_m_000000
19/06/29 17:41:50 INFO SparkHadoopMapRedUtil: attempt_20190629174150_0025_m_000000_25: Committed
19/06/29 17:41:50 INFO Executor: Finished task 0.0 in stage 25.0 (TID 25). 1979 bytes result sent to driver
19/06/29 17:41:50 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 25) in 31 ms on localhost (executor driver) (1/1)
19/06/29 17:41:50 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
19/06/29 17:41:50 INFO DAGScheduler: ResultStage 25 (main at NativeMethodAccessorImpl.java:0) finished in 0.057 s
19/06/29 17:41:50 INFO DAGScheduler: Job 25 finished: main at NativeMethodAccessorImpl.java:0, took 0.058873 s
19/06/29 17:41:50 INFO FileFormatWriter: Write Job ee3f64ab-bf7d-4188-96fb-4060cca090a1 committed.
19/06/29 17:41:50 INFO FileFormatWriter: Finished processing stats for write job ee3f64ab-bf7d-4188-96fb-4060cca090a1.
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:50 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-50_112_2674020490788280539-1/-ext-10000/part-00000-f553916b-877c-43c0-bb3c-5d9ca3fe3b32-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-f553916b-877c-43c0-bb3c-5d9ca3fe3b32-c000, Status:true
19/06/29 17:41:50 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:50 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:50 INFO log: Updated size of table pg_type to 1052
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:50 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:50 INFO log: Updated size of table pg_type to 1052
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-50_374_2621118808471275394-1
19/06/29 17:41:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:50 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:50 INFO DAGScheduler: Got job 26 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:50 INFO DAGScheduler: Final stage: ResultStage 26 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:50 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:50 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:50 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[53] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:50 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 215.8 KB, free 364.7 MB)
19/06/29 17:41:50 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 76.1 KB, free 364.6 MB)
19/06/29 17:41:50 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 365.9 MB)
19/06/29 17:41:50 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[53] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:50 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
19/06/29 17:41:50 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 26, localhost, executor driver, partition 0, PROCESS_LOCAL, 8172 bytes)
19/06/29 17:41:50 INFO Executor: Running task 0.0 in stage 26.0 (TID 26)
19/06/29 17:41:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:50 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174150_0026_m_000000_26' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-50_374_2621118808471275394-1/-ext-10000/_temporary/0/task_20190629174150_0026_m_000000
19/06/29 17:41:50 INFO SparkHadoopMapRedUtil: attempt_20190629174150_0026_m_000000_26: Committed
19/06/29 17:41:50 INFO Executor: Finished task 0.0 in stage 26.0 (TID 26). 1979 bytes result sent to driver
19/06/29 17:41:50 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 26) in 24 ms on localhost (executor driver) (1/1)
19/06/29 17:41:50 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
19/06/29 17:41:50 INFO DAGScheduler: ResultStage 26 (main at NativeMethodAccessorImpl.java:0) finished in 0.054 s
19/06/29 17:41:50 INFO DAGScheduler: Job 26 finished: main at NativeMethodAccessorImpl.java:0, took 0.056171 s
19/06/29 17:41:50 INFO FileFormatWriter: Write Job a655c61b-318f-4f2e-a46d-da9c107eca16 committed.
19/06/29 17:41:50 INFO FileFormatWriter: Finished processing stats for write job a655c61b-318f-4f2e-a46d-da9c107eca16.
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:50 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-50_374_2621118808471275394-1/-ext-10000/part-00000-f89e8bbc-7c2f-460f-9da8-71844d646132-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-f89e8bbc-7c2f-460f-9da8-71844d646132-c000, Status:true
19/06/29 17:41:50 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:50 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:50 INFO log: Updated size of table pg_type to 1101
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:50 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:50 INFO log: Updated size of table pg_type to 1101
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-50_624_8458380066877363383-1
19/06/29 17:41:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:50 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:50 INFO DAGScheduler: Got job 27 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:50 INFO DAGScheduler: Final stage: ResultStage 27 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:50 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:50 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:50 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[55] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:50 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 215.8 KB, free 364.4 MB)
19/06/29 17:41:50 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 76.1 KB, free 364.3 MB)
19/06/29 17:41:50 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 365.8 MB)
19/06/29 17:41:50 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[55] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:50 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks
19/06/29 17:41:50 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 27, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:50 INFO Executor: Running task 0.0 in stage 27.0 (TID 27)
19/06/29 17:41:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:50 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174150_0027_m_000000_27' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-50_624_8458380066877363383-1/-ext-10000/_temporary/0/task_20190629174150_0027_m_000000
19/06/29 17:41:50 INFO SparkHadoopMapRedUtil: attempt_20190629174150_0027_m_000000_27: Committed
19/06/29 17:41:50 INFO Executor: Finished task 0.0 in stage 27.0 (TID 27). 1979 bytes result sent to driver
19/06/29 17:41:50 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 27) in 23 ms on localhost (executor driver) (1/1)
19/06/29 17:41:50 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
19/06/29 17:41:50 INFO DAGScheduler: ResultStage 27 (main at NativeMethodAccessorImpl.java:0) finished in 0.049 s
19/06/29 17:41:50 INFO DAGScheduler: Job 27 finished: main at NativeMethodAccessorImpl.java:0, took 0.051081 s
19/06/29 17:41:50 INFO FileFormatWriter: Write Job 66c51433-abc1-4796-80d5-faddf90d9f32 committed.
19/06/29 17:41:50 INFO FileFormatWriter: Finished processing stats for write job 66c51433-abc1-4796-80d5-faddf90d9f32.
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:50 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-50_624_8458380066877363383-1/-ext-10000/part-00000-2d68ebc0-cb0d-4c6f-ae0a-ee5da0d43fb3-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-2d68ebc0-cb0d-4c6f-ae0a-ee5da0d43fb3-c000, Status:true
19/06/29 17:41:50 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:50 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:50 INFO log: Updated size of table pg_type to 1143
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:50 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:50 INFO log: Updated size of table pg_type to 1143
19/06/29 17:41:50 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:50 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-50_887_7789969649745415909-1
19/06/29 17:41:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:50 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:50 INFO DAGScheduler: Got job 28 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:50 INFO DAGScheduler: Final stage: ResultStage 28 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:50 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:50 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:50 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[57] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:50 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 215.8 KB, free 364.1 MB)
19/06/29 17:41:50 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 76.1 KB, free 364.0 MB)
19/06/29 17:41:50 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 365.7 MB)
19/06/29 17:41:50 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[57] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:50 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks
19/06/29 17:41:50 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 28, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:50 INFO Executor: Running task 0.0 in stage 28.0 (TID 28)
19/06/29 17:41:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:50 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174150_0028_m_000000_28' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-50_887_7789969649745415909-1/-ext-10000/_temporary/0/task_20190629174150_0028_m_000000
19/06/29 17:41:50 INFO SparkHadoopMapRedUtil: attempt_20190629174150_0028_m_000000_28: Committed
19/06/29 17:41:50 INFO Executor: Finished task 0.0 in stage 28.0 (TID 28). 1979 bytes result sent to driver
19/06/29 17:41:50 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 28) in 34 ms on localhost (executor driver) (1/1)
19/06/29 17:41:50 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
19/06/29 17:41:50 INFO DAGScheduler: ResultStage 28 (main at NativeMethodAccessorImpl.java:0) finished in 0.058 s
19/06/29 17:41:50 INFO DAGScheduler: Job 28 finished: main at NativeMethodAccessorImpl.java:0, took 0.060157 s
19/06/29 17:41:51 INFO FileFormatWriter: Write Job 0e2440bd-7cd2-4eb9-9ccd-5b61f399653b committed.
19/06/29 17:41:51 INFO FileFormatWriter: Finished processing stats for write job 0e2440bd-7cd2-4eb9-9ccd-5b61f399653b.
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:51 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:51 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-50_887_7789969649745415909-1/-ext-10000/part-00000-df8596fc-7ab1-4e66-b22b-298c1de35d20-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-df8596fc-7ab1-4e66-b22b-298c1de35d20-c000, Status:true
19/06/29 17:41:51 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:51 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:51 INFO log: Updated size of table pg_type to 1184
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:51 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:51 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:51 INFO log: Updated size of table pg_type to 1184
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:51 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-51_164_3651593313816010259-1
19/06/29 17:41:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:51 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
19/06/29 17:41:51 INFO DAGScheduler: Got job 29 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/06/29 17:41:51 INFO DAGScheduler: Final stage: ResultStage 29 (main at NativeMethodAccessorImpl.java:0)
19/06/29 17:41:51 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:51 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:51 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[59] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
19/06/29 17:41:51 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 215.8 KB, free 363.8 MB)
19/06/29 17:41:51 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 76.1 KB, free 363.7 MB)
19/06/29 17:41:51 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 35758d068699:33063 (size: 76.1 KB, free: 365.6 MB)
19/06/29 17:41:51 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[59] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:51 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks
19/06/29 17:41:51 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 29, localhost, executor driver, partition 0, PROCESS_LOCAL, 8164 bytes)
19/06/29 17:41:51 INFO Executor: Running task 0.0 in stage 29.0 (TID 29)
19/06/29 17:41:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:51 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174151_0029_m_000000_29' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-51_164_3651593313816010259-1/-ext-10000/_temporary/0/task_20190629174151_0029_m_000000
19/06/29 17:41:51 INFO SparkHadoopMapRedUtil: attempt_20190629174151_0029_m_000000_29: Committed
19/06/29 17:41:51 INFO Executor: Finished task 0.0 in stage 29.0 (TID 29). 1979 bytes result sent to driver
19/06/29 17:41:51 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 29) in 20 ms on localhost (executor driver) (1/1)
19/06/29 17:41:51 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
19/06/29 17:41:51 INFO DAGScheduler: ResultStage 29 (main at NativeMethodAccessorImpl.java:0) finished in 0.045 s
19/06/29 17:41:51 INFO DAGScheduler: Job 29 finished: main at NativeMethodAccessorImpl.java:0, took 0.047141 s
19/06/29 17:41:51 INFO FileFormatWriter: Write Job 4cb79e1b-aa9d-4cae-ac7f-1d8fc9bcc0f8 committed.
19/06/29 17:41:51 INFO FileFormatWriter: Finished processing stats for write job 4cb79e1b-aa9d-4cae-ac7f-1d8fc9bcc0f8.
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:51 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:51 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/.hive-staging_hive_2019-06-29_17-41-51_164_3651593313816010259-1/-ext-10000/part-00000-55529520-1228-4f12-9e08-94389397378e-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_type/part-00000-55529520-1228-4f12-9e08-94389397378e-c000, Status:true
19/06/29 17:41:51 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:51 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:51 INFO log: Updated size of table pg_type to 1231
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:51 INFO HiveMetaStore: 0: alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_type newtbl=pg_type	
19/06/29 17:41:51 INFO log: Updating table stats fast for pg_type
19/06/29 17:41:51 INFO log: Updated size of table pg_type to 1231
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_index
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_index	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_index
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_index	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 841
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 776
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 837
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_index
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_index	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 365.7 MB)
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_index
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_index	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: create_table: Table(tableName:pg_index, dbName:pg_catalog, owner:root, createTime:1561830111, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:indrelid, type:int, comment:null), FieldSchema(name:indexrelid, type:int, comment:null), FieldSchema(name:indisprimary, type:boolean, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_index, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"indrelid","type":"integer","nullable":true,"metadata":{}},{"name":"indexrelid","type":"integer","nullable":true,"metadata":{}},{"name":"indisprimary","type":"boolean","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_index, dbName:pg_catalog, owner:root, createTime:1561830111, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:indrelid, type:int, comment:null), FieldSchema(name:indexrelid, type:int, comment:null), FieldSchema(name:indisprimary, type:boolean, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_index, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"indrelid","type":"integer","nullable":true,"metadata":{}},{"name":"indexrelid","type":"integer","nullable":true,"metadata":{}},{"name":"indisprimary","type":"boolean","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:41:51 WARN HiveMetaStore: Location: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_index specified for non-external table:pg_index
19/06/29 17:41:51 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_index
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 674
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 778
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 834
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 797
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 877
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 885
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 729
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 827
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 846
19/06/29 17:41:51 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 365.8 MB)
19/06/29 17:41:51 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 365.9 MB)
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 742
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 711
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 751
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 865
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 867
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 875
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 887
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 784
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 669
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 679
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 709
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 855
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 805
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 706
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 843
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 895
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 862
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 884
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 760
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 668
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 735
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 794
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 774
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 759
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 714
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 748
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 763
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 868
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 756
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 664
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 766
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 722
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 857
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 856
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 854
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 721
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 874
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 749
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 693
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 836
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 853
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 681
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 762
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 785
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 782
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 757
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 775
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 818
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 898
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 845
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 882
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 866
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 724
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 740
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 708
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 670
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 787
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 765
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 773
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 890
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 734
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 745
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 666
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 847
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 826
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 820
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 772
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 771
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 701
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 684
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 790
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 660
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 685
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_description
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_description	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_description
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_description	
19/06/29 17:41:51 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 365.9 MB)
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_description
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_description	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 824
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 727
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 717
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 764
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 667
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 699
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 873
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 671
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 817
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 730
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 777
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 688
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 815
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 733
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 741
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 731
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 891
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 366.0 MB)
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_description
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_description	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 796
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 863
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 786
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 686
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 835
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 687
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 809
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 899
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 859
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 896
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 704
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 673
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 838
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 833
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 788
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 828
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 872
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 743
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 770
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 677
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 811
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 881
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 690
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 888
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 661
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 682
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 739
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 744
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 792
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 832
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 683
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 808
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 858
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 852
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 806
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 831
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 869
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 728
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 830
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 768
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 781
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 676
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 747
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 799
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 810
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 783
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 697
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 813
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 876
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 702
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 715
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 848
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 791
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 707
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 754
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 761
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 889
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 675
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 802
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 803
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 880
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 842
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 793
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 812
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 700
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 886
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 822
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 769
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 825
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 897
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 689
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 719
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 780
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 750
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 737
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 840
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 713
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 720
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 801
19/06/29 17:41:51 INFO HiveMetaStore: 0: create_table: Table(tableName:pg_description, dbName:pg_catalog, owner:root, createTime:1561830111, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:objoid, type:int, comment:null), FieldSchema(name:classoid, type:int, comment:null), FieldSchema(name:objsubid, type:int, comment:null), FieldSchema(name:description, type:string, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_description, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"objoid","type":"integer","nullable":true,"metadata":{}},{"name":"classoid","type":"integer","nullable":true,"metadata":{}},{"name":"objsubid","type":"integer","nullable":true,"metadata":{}},{"name":"description","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_description, dbName:pg_catalog, owner:root, createTime:1561830111, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:objoid, type:int, comment:null), FieldSchema(name:classoid, type:int, comment:null), FieldSchema(name:objsubid, type:int, comment:null), FieldSchema(name:description, type:string, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_description, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"objoid","type":"integer","nullable":true,"metadata":{}},{"name":"classoid","type":"integer","nullable":true,"metadata":{}},{"name":"objsubid","type":"integer","nullable":true,"metadata":{}},{"name":"description","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:41:51 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 366.1 MB)
19/06/29 17:41:51 WARN HiveMetaStore: Location: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_description specified for non-external table:pg_description
19/06/29 17:41:51 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_description
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 894
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 691
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 678
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 698
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 804
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 732
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 795
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 829
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 849
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 850
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 871
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 692
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 746
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 779
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 823
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 839
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 883
19/06/29 17:41:51 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 366.2 MB)
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 767
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 851
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 893
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 736
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 752
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 695
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 892
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 870
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 755
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 816
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 680
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 662
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 807
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 860
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 819
19/06/29 17:41:51 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 35758d068699:33063 in memory (size: 76.1 KB, free: 366.2 MB)
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 814
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 878
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 879
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 665
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 710
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 672
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 712
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 716
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 758
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 861
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 723
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 789
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 844
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 864
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 718
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 725
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 705
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 821
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 694
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 753
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 696
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 800
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 726
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 798
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 703
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 663
19/06/29 17:41:51 INFO ContextCleaner: Cleaned accumulator 738
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_depend
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_depend	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_depend
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_depend	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_depend
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_depend	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_depend
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_depend	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: create_table: Table(tableName:pg_depend, dbName:pg_catalog, owner:root, createTime:1561830111, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:objid, type:int, comment:null), FieldSchema(name:classid, type:int, comment:null), FieldSchema(name:refobjid, type:int, comment:null), FieldSchema(name:refclassid, type:int, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_depend, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"objid","type":"integer","nullable":true,"metadata":{}},{"name":"classid","type":"integer","nullable":true,"metadata":{}},{"name":"refobjid","type":"integer","nullable":true,"metadata":{}},{"name":"refclassid","type":"integer","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_depend, dbName:pg_catalog, owner:root, createTime:1561830111, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:objid, type:int, comment:null), FieldSchema(name:classid, type:int, comment:null), FieldSchema(name:refobjid, type:int, comment:null), FieldSchema(name:refclassid, type:int, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_depend, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"objid","type":"integer","nullable":true,"metadata":{}},{"name":"classid","type":"integer","nullable":true,"metadata":{}},{"name":"refobjid","type":"integer","nullable":true,"metadata":{}},{"name":"refclassid","type":"integer","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:41:51 WARN HiveMetaStore: Location: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_depend specified for non-external table:pg_depend
19/06/29 17:41:51 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_depend
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_constraint
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_constraint	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_constraint
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_constraint	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_constraint
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_constraint	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_constraint
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_constraint	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: create_table: Table(tableName:pg_constraint, dbName:pg_catalog, owner:root, createTime:1561830111, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:confupdtype, type:string, comment:null), FieldSchema(name:confdeltype, type:string, comment:null), FieldSchema(name:conname, type:string, comment:null), FieldSchema(name:condeferrable, type:boolean, comment:null), FieldSchema(name:condeferred, type:boolean, comment:null), FieldSchema(name:conkey, type:array<int>, comment:null), FieldSchema(name:confkey, type:array<int>, comment:null), FieldSchema(name:confrelid, type:int, comment:null), FieldSchema(name:conrelid, type:int, comment:null), FieldSchema(name:contype, type:string, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_constraint, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"confupdtype","type":"string","nullable":true,"metadata":{}},{"name":"confdeltype","type":"string","nullable":true,"metadata":{}},{"name":"conname","type":"string","nullable":true,"metadata":{}},{"name":"condeferrable","type":"boolean","nullable":true,"metadata":{}},{"name":"condeferred","type":"boolean","nullable":true,"metadata":{}},{"name":"conkey","type":{"type":"array","elementType":"integer","containsNull":true},"nullable":true,"metadata":{}},{"name":"confkey","type":{"type":"array","elementType":"integer","containsNull":true},"nullable":true,"metadata":{}},{"name":"confrelid","type":"integer","nullable":true,"metadata":{}},{"name":"conrelid","type":"integer","nullable":true,"metadata":{}},{"name":"contype","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_constraint, dbName:pg_catalog, owner:root, createTime:1561830111, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:confupdtype, type:string, comment:null), FieldSchema(name:confdeltype, type:string, comment:null), FieldSchema(name:conname, type:string, comment:null), FieldSchema(name:condeferrable, type:boolean, comment:null), FieldSchema(name:condeferred, type:boolean, comment:null), FieldSchema(name:conkey, type:array<int>, comment:null), FieldSchema(name:confkey, type:array<int>, comment:null), FieldSchema(name:confrelid, type:int, comment:null), FieldSchema(name:conrelid, type:int, comment:null), FieldSchema(name:contype, type:string, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_constraint, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"confupdtype","type":"string","nullable":true,"metadata":{}},{"name":"confdeltype","type":"string","nullable":true,"metadata":{}},{"name":"conname","type":"string","nullable":true,"metadata":{}},{"name":"condeferrable","type":"boolean","nullable":true,"metadata":{}},{"name":"condeferred","type":"boolean","nullable":true,"metadata":{}},{"name":"conkey","type":{"type":"array","elementType":"integer","containsNull":true},"nullable":true,"metadata":{}},{"name":"confkey","type":{"type":"array","elementType":"integer","containsNull":true},"nullable":true,"metadata":{}},{"name":"confrelid","type":"integer","nullable":true,"metadata":{}},{"name":"conrelid","type":"integer","nullable":true,"metadata":{}},{"name":"contype","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:41:51 WARN HiveMetaStore: Location: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_constraint specified for non-external table:pg_constraint
19/06/29 17:41:51 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_constraint
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_attrdef
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attrdef	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_attrdef
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attrdef	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_attrdef
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attrdef	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_attrdef
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attrdef	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: create_table: Table(tableName:pg_attrdef, dbName:pg_catalog, owner:root, createTime:1561830111, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:adrelid, type:int, comment:null), FieldSchema(name:adnum, type:smallint, comment:null), FieldSchema(name:adbin, type:string, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_attrdef, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"adrelid","type":"integer","nullable":true,"metadata":{}},{"name":"adnum","type":"short","nullable":true,"metadata":{}},{"name":"adbin","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_attrdef, dbName:pg_catalog, owner:root, createTime:1561830111, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:adrelid, type:int, comment:null), FieldSchema(name:adnum, type:smallint, comment:null), FieldSchema(name:adbin, type:string, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_attrdef, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"adrelid","type":"integer","nullable":true,"metadata":{}},{"name":"adnum","type":"short","nullable":true,"metadata":{}},{"name":"adbin","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:41:51 WARN HiveMetaStore: Location: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_attrdef specified for non-external table:pg_attrdef
19/06/29 17:41:51 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_attrdef
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_inherits
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_inherits	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_inherits
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_inherits	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_inherits
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_inherits	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_inherits
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_inherits	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: create_table: Table(tableName:pg_inherits, dbName:pg_catalog, owner:root, createTime:1561830111, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:inhrelid, type:int, comment:null), FieldSchema(name:inhparent, type:int, comment:null), FieldSchema(name:inhseqno, type:int, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_inherits, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"inhrelid","type":"integer","nullable":true,"metadata":{}},{"name":"inhparent","type":"integer","nullable":true,"metadata":{}},{"name":"inhseqno","type":"integer","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_inherits, dbName:pg_catalog, owner:root, createTime:1561830111, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:inhrelid, type:int, comment:null), FieldSchema(name:inhparent, type:int, comment:null), FieldSchema(name:inhseqno, type:int, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_inherits, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"inhrelid","type":"integer","nullable":true,"metadata":{}},{"name":"inhparent","type":"integer","nullable":true,"metadata":{}},{"name":"inhseqno","type":"integer","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:41:51 WARN HiveMetaStore: Location: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_inherits specified for non-external table:pg_inherits
19/06/29 17:41:51 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_inherits
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_collation
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_collation	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_collation
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_collation	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_collation
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_collation	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_collation
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_collation	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: create_table: Table(tableName:pg_collation, dbName:pg_catalog, owner:root, createTime:1561830111, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:collname, type:string, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_collation, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"collname","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_collation, dbName:pg_catalog, owner:root, createTime:1561830111, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:collname, type:string, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_collation, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"collname","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:41:51 WARN HiveMetaStore: Location: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_collation specified for non-external table:pg_collation
19/06/29 17:41:51 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_collation
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_policy
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_policy	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_policy
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_policy	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_policy
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_policy	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_policy
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_policy	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:51 INFO HiveMetaStore: 0: create_table: Table(tableName:pg_policy, dbName:pg_catalog, owner:root, createTime:1561830111, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:polname, type:string, comment:null), FieldSchema(name:polrelid, type:int, comment:null), FieldSchema(name:polcmd, type:string, comment:null), FieldSchema(name:polroles, type:string, comment:null), FieldSchema(name:polqual, type:string, comment:null), FieldSchema(name:polwithcheck, type:string, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_policy, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"polname","type":"string","nullable":true,"metadata":{}},{"name":"polrelid","type":"integer","nullable":true,"metadata":{}},{"name":"polcmd","type":"string","nullable":true,"metadata":{}},{"name":"polroles","type":"string","nullable":true,"metadata":{}},{"name":"polqual","type":"string","nullable":true,"metadata":{}},{"name":"polwithcheck","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_policy, dbName:pg_catalog, owner:root, createTime:1561830111, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:polname, type:string, comment:null), FieldSchema(name:polrelid, type:int, comment:null), FieldSchema(name:polcmd, type:string, comment:null), FieldSchema(name:polroles, type:string, comment:null), FieldSchema(name:polqual, type:string, comment:null), FieldSchema(name:polwithcheck, type:string, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_policy, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"polname","type":"string","nullable":true,"metadata":{}},{"name":"polrelid","type":"integer","nullable":true,"metadata":{}},{"name":"polcmd","type":"string","nullable":true,"metadata":{}},{"name":"polroles","type":"string","nullable":true,"metadata":{}},{"name":"polqual","type":"string","nullable":true,"metadata":{}},{"name":"polwithcheck","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:41:51 WARN HiveMetaStore: Location: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_policy specified for non-external table:pg_policy
19/06/29 17:41:51 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_policy
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_type
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_type	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_attrdef
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attrdef	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_constraint
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_constraint	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_depend
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_depend	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_description
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_description	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_index
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_index	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_inherits
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_inherits	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_namespace
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_namespace	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_policy
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_policy	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_collation
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_collation	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_roles
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_roles	
19/06/29 17:41:51 INFO HiveMetaStore: 0: get_table : db=pg_catalog tbl=pg_user
19/06/29 17:41:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_user	
19/06/29 17:41:52 INFO LoggingHandler: [id: 0x9e0f4d54] REGISTERED
19/06/29 17:41:52 INFO LoggingHandler: [id: 0x9e0f4d54] BIND: 0.0.0.0/0.0.0.0:54322
19/06/29 17:41:52 INFO LoggingHandler: [id: 0x9e0f4d54, L:/0.0.0.0:54322] ACTIVE
19/06/29 17:41:52 INFO PgProtocolService: Start running the SQL server (port=54322, workerThreads=4)
19/06/29 17:41:55 INFO LoggingHandler: [id: 0x9e0f4d54, L:/0.0.0.0:54322] READ: [id: 0xc6b89f9f, L:/172.17.0.2:54322 - R:/172.17.0.1:58486]
19/06/29 17:41:55 INFO LoggingHandler: [id: 0x9e0f4d54, L:/0.0.0.0:54322] READ COMPLETE
19/06/29 17:41:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:41:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:41:55 INFO HiveMetaStore: 1: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19/06/29 17:41:55 INFO ObjectStore: ObjectStore, initialize called
19/06/29 17:41:55 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19/06/29 17:41:55 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/06/29 17:41:55 INFO ObjectStore: Initialized ObjectStore
19/06/29 17:41:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:41:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:41:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:41:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:41:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:41:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:41:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:55 INFO HiveMetaStore: 1: create_table: Table(tableName:pg_database, dbName:pg_catalog, owner:root, createTime:1561830115, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:datname, type:string, comment:null), FieldSchema(name:datdba, type:int, comment:null), FieldSchema(name:encoding, type:int, comment:null), FieldSchema(name:datcollate, type:string, comment:null), FieldSchema(name:datctype, type:string, comment:null), FieldSchema(name:datacl, type:array<string>, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"datname","type":"string","nullable":true,"metadata":{}},{"name":"datdba","type":"integer","nullable":true,"metadata":{}},{"name":"encoding","type":"integer","nullable":true,"metadata":{}},{"name":"datcollate","type":"string","nullable":true,"metadata":{}},{"name":"datctype","type":"string","nullable":true,"metadata":{}},{"name":"datacl","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:41:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_database, dbName:pg_catalog, owner:root, createTime:1561830115, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:datname, type:string, comment:null), FieldSchema(name:datdba, type:int, comment:null), FieldSchema(name:encoding, type:int, comment:null), FieldSchema(name:datcollate, type:string, comment:null), FieldSchema(name:datctype, type:string, comment:null), FieldSchema(name:datacl, type:array<string>, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"datname","type":"string","nullable":true,"metadata":{}},{"name":"datdba","type":"integer","nullable":true,"metadata":{}},{"name":"encoding","type":"integer","nullable":true,"metadata":{}},{"name":"datcollate","type":"string","nullable":true,"metadata":{}},{"name":"datctype","type":"string","nullable":true,"metadata":{}},{"name":"datacl","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:41:55 WARN HiveMetaStore: Location: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database specified for non-external table:pg_database
19/06/29 17:41:55 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_databases: *
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_databases: *	
19/06/29 17:41:56 INFO PgMetadata: Registering a database `default` in a system catalog `pg_database`
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:41:56 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/.hive-staging_hive_2019-06-29_17-41-56_062_4590425194356216493-2
19/06/29 17:41:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:56 INFO CodeGenerator: Code generated in 31.007103 ms
19/06/29 17:41:56 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:41:56 INFO DAGScheduler: Got job 30 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:41:56 INFO DAGScheduler: Final stage: ResultStage 30 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:41:56 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:56 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:56 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[61] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:41:56 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 214.9 KB, free 365.8 MB)
19/06/29 17:41:56 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 75.6 KB, free 365.7 MB)
19/06/29 17:41:56 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 35758d068699:33063 (size: 75.6 KB, free: 366.2 MB)
19/06/29 17:41:56 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[61] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:56 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks
19/06/29 17:41:56 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 30, localhost, executor driver, partition 0, PROCESS_LOCAL, 8092 bytes)
19/06/29 17:41:56 INFO Executor: Running task 0.0 in stage 30.0 (TID 30)
19/06/29 17:41:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:56 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174156_0030_m_000000_30' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/.hive-staging_hive_2019-06-29_17-41-56_062_4590425194356216493-2/-ext-10000/_temporary/0/task_20190629174156_0030_m_000000
19/06/29 17:41:56 INFO SparkHadoopMapRedUtil: attempt_20190629174156_0030_m_000000_30: Committed
19/06/29 17:41:56 INFO Executor: Finished task 0.0 in stage 30.0 (TID 30). 1979 bytes result sent to driver
19/06/29 17:41:56 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 30) in 25 ms on localhost (executor driver) (1/1)
19/06/29 17:41:56 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
19/06/29 17:41:56 INFO DAGScheduler: ResultStage 30 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.062 s
19/06/29 17:41:56 INFO DAGScheduler: Job 30 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.064552 s
19/06/29 17:41:56 INFO FileFormatWriter: Write Job c9b2c831-034a-4d34-9f1e-1dc53a78729a committed.
19/06/29 17:41:56 INFO FileFormatWriter: Finished processing stats for write job c9b2c831-034a-4d34-9f1e-1dc53a78729a.
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:41:56 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:56 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/.hive-staging_hive_2019-06-29_17-41-56_062_4590425194356216493-2/-ext-10000/part-00000-e39948e5-e73c-42b4-a853-2bb770b9c8c3-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/part-00000-e39948e5-e73c-42b4-a853-2bb770b9c8c3-c000, Status:true
19/06/29 17:41:56 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_database newtbl=pg_database
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_database newtbl=pg_database	
19/06/29 17:41:56 INFO log: Updating table stats fast for pg_database
19/06/29 17:41:56 INFO log: Updated size of table pg_database to 17
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:41:56 INFO PgMetadata: Registering a database `pg_catalog` in a system catalog `pg_database`
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:41:56 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/.hive-staging_hive_2019-06-29_17-41-56_332_1635492445092840264-2
19/06/29 17:41:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:56 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:41:56 INFO DAGScheduler: Got job 31 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:41:56 INFO DAGScheduler: Final stage: ResultStage 31 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:41:56 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:56 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:56 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[63] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:41:56 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 214.9 KB, free 365.5 MB)
19/06/29 17:41:56 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 75.6 KB, free 365.4 MB)
19/06/29 17:41:56 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 35758d068699:33063 (size: 75.6 KB, free: 366.1 MB)
19/06/29 17:41:56 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[63] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:56 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks
19/06/29 17:41:56 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 31, localhost, executor driver, partition 0, PROCESS_LOCAL, 8100 bytes)
19/06/29 17:41:56 INFO Executor: Running task 0.0 in stage 31.0 (TID 31)
19/06/29 17:41:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:56 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174156_0031_m_000000_31' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/.hive-staging_hive_2019-06-29_17-41-56_332_1635492445092840264-2/-ext-10000/_temporary/0/task_20190629174156_0031_m_000000
19/06/29 17:41:56 INFO SparkHadoopMapRedUtil: attempt_20190629174156_0031_m_000000_31: Committed
19/06/29 17:41:56 INFO Executor: Finished task 0.0 in stage 31.0 (TID 31). 1979 bytes result sent to driver
19/06/29 17:41:56 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 31) in 28 ms on localhost (executor driver) (1/1)
19/06/29 17:41:56 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
19/06/29 17:41:56 INFO DAGScheduler: ResultStage 31 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.070 s
19/06/29 17:41:56 INFO DAGScheduler: Job 31 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.071878 s
19/06/29 17:41:56 INFO FileFormatWriter: Write Job 2c424d33-e0cc-4766-89da-cb73d301aebf committed.
19/06/29 17:41:56 INFO FileFormatWriter: Finished processing stats for write job 2c424d33-e0cc-4766-89da-cb73d301aebf.
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:41:56 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:56 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/.hive-staging_hive_2019-06-29_17-41-56_332_1635492445092840264-2/-ext-10000/part-00000-d25319aa-007b-4fc2-81d7-97d95cfcc371-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/part-00000-d25319aa-007b-4fc2-81d7-97d95cfcc371-c000, Status:true
19/06/29 17:41:56 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_database newtbl=pg_database
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_database newtbl=pg_database	
19/06/29 17:41:56 INFO log: Updating table stats fast for pg_database
19/06/29 17:41:56 INFO log: Updated size of table pg_database to 37
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:41:56 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_database newtbl=pg_database
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_database newtbl=pg_database	
19/06/29 17:41:56 INFO log: Updating table stats fast for pg_database
19/06/29 17:41:56 INFO log: Updated size of table pg_database to 37
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:56 INFO HiveMetaStore: 1: create_table: Table(tableName:pg_class, dbName:pg_catalog, owner:root, createTime:1561830116, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:reltablespace, type:int, comment:null), FieldSchema(name:relname, type:string, comment:null), FieldSchema(name:reloftype, type:int, comment:null), FieldSchema(name:relpersistence, type:string, comment:null), FieldSchema(name:relkind, type:string, comment:null), FieldSchema(name:relnamespace, type:int, comment:null), FieldSchema(name:relowner, type:int, comment:null), FieldSchema(name:relacl, type:array<string>, comment:null), FieldSchema(name:relchecks, type:smallint, comment:null), FieldSchema(name:reltoastrelid, type:int, comment:null), FieldSchema(name:relhasindex, type:boolean, comment:null), FieldSchema(name:relhasrules, type:boolean, comment:null), FieldSchema(name:relhastriggers, type:boolean, comment:null), FieldSchema(name:relrowsecurity, type:boolean, comment:null), FieldSchema(name:relforcerowsecurity, type:boolean, comment:null), FieldSchema(name:relreplident, type:string, comment:null), FieldSchema(name:reltriggers, type:smallint, comment:null), FieldSchema(name:relhasoids, type:boolean, comment:null), FieldSchema(name:relispartition, type:boolean, comment:null), FieldSchema(name:relpartbound, type:string, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_class, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"reltablespace","type":"integer","nullable":true,"metadata":{}},{"name":"relname","type":"string","nullable":true,"metadata":{}},{"name":"reloftype","type":"integer","nullable":true,"metadata":{}},{"name":"relpersistence","type":"string","nullable":true,"metadata":{}},{"name":"relkind","type":"string","nullable":true,"metadata":{}},{"name":"relnamespace","type":"integer","nullable":true,"metadata":{}},{"name":"relowner","type":"integer","nullable":true,"metadata":{}},{"name":"relacl","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}},{"name":"relchecks","type":"short","nullable":true,"metadata":{}},{"name":"reltoastrelid","type":"integer","nullable":true,"metadata":{}},{"name":"relhasindex","type":"boolean","nullable":true,"metadata":{}},{"name":"relhasrules","type":"boolean","nullable":true,"metadata":{}},{"name":"relhastriggers","type":"boolean","nullable":true,"metadata":{}},{"name":"relrowsecurity","type":"boolean","nullable":true,"metadata":{}},{"name":"relforcerowsecurity","type":"boolean","nullable":true,"metadata":{}},{"name":"relreplident","type":"string","nullable":true,"metadata":{}},{"name":"reltriggers","type":"short","nullable":true,"metadata":{}},{"name":"relhasoids","type":"boolean","nullable":true,"metadata":{}},{"name":"relispartition","type":"boolean","nullable":true,"metadata":{}},{"name":"relpartbound","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_class, dbName:pg_catalog, owner:root, createTime:1561830116, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:reltablespace, type:int, comment:null), FieldSchema(name:relname, type:string, comment:null), FieldSchema(name:reloftype, type:int, comment:null), FieldSchema(name:relpersistence, type:string, comment:null), FieldSchema(name:relkind, type:string, comment:null), FieldSchema(name:relnamespace, type:int, comment:null), FieldSchema(name:relowner, type:int, comment:null), FieldSchema(name:relacl, type:array<string>, comment:null), FieldSchema(name:relchecks, type:smallint, comment:null), FieldSchema(name:reltoastrelid, type:int, comment:null), FieldSchema(name:relhasindex, type:boolean, comment:null), FieldSchema(name:relhasrules, type:boolean, comment:null), FieldSchema(name:relhastriggers, type:boolean, comment:null), FieldSchema(name:relrowsecurity, type:boolean, comment:null), FieldSchema(name:relforcerowsecurity, type:boolean, comment:null), FieldSchema(name:relreplident, type:string, comment:null), FieldSchema(name:reltriggers, type:smallint, comment:null), FieldSchema(name:relhasoids, type:boolean, comment:null), FieldSchema(name:relispartition, type:boolean, comment:null), FieldSchema(name:relpartbound, type:string, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_class, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"reltablespace","type":"integer","nullable":true,"metadata":{}},{"name":"relname","type":"string","nullable":true,"metadata":{}},{"name":"reloftype","type":"integer","nullable":true,"metadata":{}},{"name":"relpersistence","type":"string","nullable":true,"metadata":{}},{"name":"relkind","type":"string","nullable":true,"metadata":{}},{"name":"relnamespace","type":"integer","nullable":true,"metadata":{}},{"name":"relowner","type":"integer","nullable":true,"metadata":{}},{"name":"relacl","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}},{"name":"relchecks","type":"short","nullable":true,"metadata":{}},{"name":"reltoastrelid","type":"integer","nullable":true,"metadata":{}},{"name":"relhasindex","type":"boolean","nullable":true,"metadata":{}},{"name":"relhasrules","type":"boolean","nullable":true,"metadata":{}},{"name":"relhastriggers","type":"boolean","nullable":true,"metadata":{}},{"name":"relrowsecurity","type":"boolean","nullable":true,"metadata":{}},{"name":"relforcerowsecurity","type":"boolean","nullable":true,"metadata":{}},{"name":"relreplident","type":"string","nullable":true,"metadata":{}},{"name":"reltriggers","type":"short","nullable":true,"metadata":{}},{"name":"relhasoids","type":"boolean","nullable":true,"metadata":{}},{"name":"relispartition","type":"boolean","nullable":true,"metadata":{}},{"name":"relpartbound","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:41:56 WARN HiveMetaStore: Location: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_class specified for non-external table:pg_class
19/06/29 17:41:56 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_class
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:56 INFO HiveMetaStore: 1: create_table: Table(tableName:pg_attribute, dbName:pg_catalog, owner:root, createTime:1561830116, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:attrelid, type:int, comment:null), FieldSchema(name:attname, type:string, comment:null), FieldSchema(name:atttypid, type:int, comment:null), FieldSchema(name:attnotnull, type:boolean, comment:null), FieldSchema(name:atthasdef, type:boolean, comment:null), FieldSchema(name:atttypmod, type:int, comment:null), FieldSchema(name:attlen, type:int, comment:null), FieldSchema(name:attnum, type:int, comment:null), FieldSchema(name:attidentity, type:string, comment:null), FieldSchema(name:attisdropped, type:boolean, comment:null), FieldSchema(name:attcollation, type:int, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_attribute, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"attrelid","type":"integer","nullable":true,"metadata":{}},{"name":"attname","type":"string","nullable":true,"metadata":{}},{"name":"atttypid","type":"integer","nullable":true,"metadata":{}},{"name":"attnotnull","type":"boolean","nullable":true,"metadata":{}},{"name":"atthasdef","type":"boolean","nullable":true,"metadata":{}},{"name":"atttypmod","type":"integer","nullable":true,"metadata":{}},{"name":"attlen","type":"integer","nullable":true,"metadata":{}},{"name":"attnum","type":"integer","nullable":true,"metadata":{}},{"name":"attidentity","type":"string","nullable":true,"metadata":{}},{"name":"attisdropped","type":"boolean","nullable":true,"metadata":{}},{"name":"attcollation","type":"integer","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_attribute, dbName:pg_catalog, owner:root, createTime:1561830116, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:attrelid, type:int, comment:null), FieldSchema(name:attname, type:string, comment:null), FieldSchema(name:atttypid, type:int, comment:null), FieldSchema(name:attnotnull, type:boolean, comment:null), FieldSchema(name:atthasdef, type:boolean, comment:null), FieldSchema(name:atttypmod, type:int, comment:null), FieldSchema(name:attlen, type:int, comment:null), FieldSchema(name:attnum, type:int, comment:null), FieldSchema(name:attidentity, type:string, comment:null), FieldSchema(name:attisdropped, type:boolean, comment:null), FieldSchema(name:attcollation, type:int, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_attribute, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"attrelid","type":"integer","nullable":true,"metadata":{}},{"name":"attname","type":"string","nullable":true,"metadata":{}},{"name":"atttypid","type":"integer","nullable":true,"metadata":{}},{"name":"attnotnull","type":"boolean","nullable":true,"metadata":{}},{"name":"atthasdef","type":"boolean","nullable":true,"metadata":{}},{"name":"atttypmod","type":"integer","nullable":true,"metadata":{}},{"name":"attlen","type":"integer","nullable":true,"metadata":{}},{"name":"attnum","type":"integer","nullable":true,"metadata":{}},{"name":"attidentity","type":"string","nullable":true,"metadata":{}},{"name":"attisdropped","type":"boolean","nullable":true,"metadata":{}},{"name":"attcollation","type":"integer","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:41:56 WARN HiveMetaStore: Location: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_attribute specified for non-external table:pg_attribute
19/06/29 17:41:56 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_attribute
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: default
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: default
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_tables: db=default pat=*
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:56 INFO HiveMetaStore: 1: create_table: Table(tableName:pg_proc, dbName:pg_catalog, owner:root, createTime:1561830116, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:proname, type:string, comment:null), FieldSchema(name:prorettype, type:int, comment:null), FieldSchema(name:proargtypes, type:array<int>, comment:null), FieldSchema(name:pronamespace, type:int, comment:null), FieldSchema(name:proisagg, type:boolean, comment:null), FieldSchema(name:proiswindow, type:boolean, comment:null), FieldSchema(name:proretset, type:boolean, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"proname","type":"string","nullable":true,"metadata":{}},{"name":"prorettype","type":"integer","nullable":true,"metadata":{}},{"name":"proargtypes","type":{"type":"array","elementType":"integer","containsNull":true},"nullable":true,"metadata":{}},{"name":"pronamespace","type":"integer","nullable":true,"metadata":{}},{"name":"proisagg","type":"boolean","nullable":true,"metadata":{}},{"name":"proiswindow","type":"boolean","nullable":true,"metadata":{}},{"name":"proretset","type":"boolean","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_proc, dbName:pg_catalog, owner:root, createTime:1561830116, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:proname, type:string, comment:null), FieldSchema(name:prorettype, type:int, comment:null), FieldSchema(name:proargtypes, type:array<int>, comment:null), FieldSchema(name:pronamespace, type:int, comment:null), FieldSchema(name:proisagg, type:boolean, comment:null), FieldSchema(name:proiswindow, type:boolean, comment:null), FieldSchema(name:proretset, type:boolean, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"proname","type":"string","nullable":true,"metadata":{}},{"name":"prorettype","type":"integer","nullable":true,"metadata":{}},{"name":"proargtypes","type":{"type":"array","elementType":"integer","containsNull":true},"nullable":true,"metadata":{}},{"name":"pronamespace","type":"integer","nullable":true,"metadata":{}},{"name":"proisagg","type":"boolean","nullable":true,"metadata":{}},{"name":"proiswindow","type":"boolean","nullable":true,"metadata":{}},{"name":"proretset","type":"boolean","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:41:56 WARN HiveMetaStore: Location: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc specified for non-external table:pg_proc
19/06/29 17:41:56 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: default
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_database: default
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_functions: db=default pat=*
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19/06/29 17:41:56 INFO PgMetadata: Registering a function `ANY` in a system catalog `pg_proc`
19/06/29 17:41:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:56 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-56_927_6602415730935652285-2
19/06/29 17:41:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:56 INFO CodeGenerator: Code generated in 18.425921 ms
19/06/29 17:41:56 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:41:56 INFO DAGScheduler: Got job 32 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:41:56 INFO DAGScheduler: Final stage: ResultStage 32 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:41:56 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:56 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:56 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[65] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:41:57 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 215.3 KB, free 365.2 MB)
19/06/29 17:41:57 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 76.0 KB, free 365.2 MB)
19/06/29 17:41:57 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 35758d068699:33063 (size: 76.0 KB, free: 366.0 MB)
19/06/29 17:41:57 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[65] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:57 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks
19/06/29 17:41:57 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 32, localhost, executor driver, partition 0, PROCESS_LOCAL, 8108 bytes)
19/06/29 17:41:57 INFO Executor: Running task 0.0 in stage 32.0 (TID 32)
19/06/29 17:41:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:57 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174156_0032_m_000000_32' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-56_927_6602415730935652285-2/-ext-10000/_temporary/0/task_20190629174156_0032_m_000000
19/06/29 17:41:57 INFO SparkHadoopMapRedUtil: attempt_20190629174156_0032_m_000000_32: Committed
19/06/29 17:41:57 INFO Executor: Finished task 0.0 in stage 32.0 (TID 32). 1979 bytes result sent to driver
19/06/29 17:41:57 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 32) in 24 ms on localhost (executor driver) (1/1)
19/06/29 17:41:57 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
19/06/29 17:41:57 INFO DAGScheduler: ResultStage 32 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.088 s
19/06/29 17:41:57 INFO DAGScheduler: Job 32 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.090477 s
19/06/29 17:41:57 INFO FileFormatWriter: Write Job 3c974fcb-ff0a-4beb-a223-fb04a4b00d82 committed.
19/06/29 17:41:57 INFO FileFormatWriter: Finished processing stats for write job 3c974fcb-ff0a-4beb-a223-fb04a4b00d82.
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:57 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-56_927_6602415730935652285-2/-ext-10000/part-00000-6c1ac5ee-4b1c-49bf-8dd9-4e932b4776d0-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-6c1ac5ee-4b1c-49bf-8dd9-4e932b4776d0-c000, Status:true
19/06/29 17:41:57 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:41:57 INFO log: Updating table stats fast for pg_proc
19/06/29 17:41:57 INFO log: Updated size of table pg_proc to 37
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO PgMetadata: Registering a function `array_in` in a system catalog `pg_proc`
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-57_227_7531280156976325100-2
19/06/29 17:41:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:57 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:41:57 INFO DAGScheduler: Got job 33 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:41:57 INFO DAGScheduler: Final stage: ResultStage 33 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:41:57 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:57 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:57 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[67] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:41:57 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 215.3 KB, free 365.0 MB)
19/06/29 17:41:57 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 76.0 KB, free 364.9 MB)
19/06/29 17:41:57 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 35758d068699:33063 (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:41:57 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[67] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:57 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks
19/06/29 17:41:57 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 33, localhost, executor driver, partition 0, PROCESS_LOCAL, 8108 bytes)
19/06/29 17:41:57 INFO Executor: Running task 0.0 in stage 33.0 (TID 33)
19/06/29 17:41:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:57 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174157_0033_m_000000_33' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-57_227_7531280156976325100-2/-ext-10000/_temporary/0/task_20190629174157_0033_m_000000
19/06/29 17:41:57 INFO SparkHadoopMapRedUtil: attempt_20190629174157_0033_m_000000_33: Committed
19/06/29 17:41:57 INFO Executor: Finished task 0.0 in stage 33.0 (TID 33). 1979 bytes result sent to driver
19/06/29 17:41:57 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 33) in 20 ms on localhost (executor driver) (1/1)
19/06/29 17:41:57 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
19/06/29 17:41:57 INFO DAGScheduler: ResultStage 33 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.049 s
19/06/29 17:41:57 INFO DAGScheduler: Job 33 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.052375 s
19/06/29 17:41:57 INFO FileFormatWriter: Write Job a70c52bd-5f4b-4491-99a1-0e103e49393a committed.
19/06/29 17:41:57 INFO FileFormatWriter: Finished processing stats for write job a70c52bd-5f4b-4491-99a1-0e103e49393a.
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:57 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-57_227_7531280156976325100-2/-ext-10000/part-00000-21907820-bd5c-458f-a2d1-70e25c249acc-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-21907820-bd5c-458f-a2d1-70e25c249acc-c000, Status:true
19/06/29 17:41:57 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:41:57 INFO log: Updating table stats fast for pg_proc
19/06/29 17:41:57 INFO log: Updated size of table pg_proc to 78
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:41:57 INFO log: Updating table stats fast for pg_proc
19/06/29 17:41:57 INFO log: Updated size of table pg_proc to 78
19/06/29 17:41:57 INFO PgMetadata: Registering a function `array_upper` in a system catalog `pg_proc`
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-57_479_397597714184677609-2
19/06/29 17:41:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:57 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:41:57 INFO DAGScheduler: Got job 34 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:41:57 INFO DAGScheduler: Final stage: ResultStage 34 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:41:57 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:57 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:57 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[69] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:41:57 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 215.3 KB, free 364.7 MB)
19/06/29 17:41:57 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 75.9 KB, free 364.6 MB)
19/06/29 17:41:57 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 35758d068699:33063 (size: 75.9 KB, free: 365.9 MB)
19/06/29 17:41:57 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[69] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:57 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks
19/06/29 17:41:57 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 34, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:41:57 INFO Executor: Running task 0.0 in stage 34.0 (TID 34)
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 965
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 950
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 991
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 961
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 1001
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 1008
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 990
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 995
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 933
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 954
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 911
19/06/29 17:41:57 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 35758d068699:33063 in memory (size: 75.6 KB, free: 365.9 MB)
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 952
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 1018
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 927
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 1014
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 975
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 949
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 972
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 912
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 935
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 940
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 982
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 963
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 959
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 908
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 923
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 981
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 946
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 919
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 944
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 1006
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 966
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 962
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 951
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 964
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 970
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 1003
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 921
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 945
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 978
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 993
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 1005
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 960
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 999
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 1010
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 988
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 998
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 938
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 987
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 939
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 905
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 903
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 1012
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 918
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 957
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 984
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 941
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 910
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 917
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 974
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 989
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 924
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 907
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 980
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 920
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 1009
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 901
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 1016
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 986
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 902
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 1013
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 971
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 922
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 915
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 931
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 943
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 967
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 968
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 1002
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 913
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 900
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 1011
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 934
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 930
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 926
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 936
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 942
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 929
19/06/29 17:41:57 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 35758d068699:33063 in memory (size: 75.6 KB, free: 366.0 MB)
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 1015
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 948
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 958
19/06/29 17:41:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:57 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 35758d068699:33063 in memory (size: 76.0 KB, free: 366.1 MB)
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 969
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 994
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 985
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 914
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 996
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 1007
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 956
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 976
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 909
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 973
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 983
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 992
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 1017
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 997
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 1004
19/06/29 17:41:57 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174157_0034_m_000000_34' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-57_479_397597714184677609-2/-ext-10000/_temporary/0/task_20190629174157_0034_m_000000
19/06/29 17:41:57 INFO SparkHadoopMapRedUtil: attempt_20190629174157_0034_m_000000_34: Committed
19/06/29 17:41:57 INFO Executor: Finished task 0.0 in stage 34.0 (TID 34). 1979 bytes result sent to driver
19/06/29 17:41:57 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 34) in 29 ms on localhost (executor driver) (1/1)
19/06/29 17:41:57 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
19/06/29 17:41:57 INFO DAGScheduler: ResultStage 34 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.097 s
19/06/29 17:41:57 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 35758d068699:33063 in memory (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:41:57 INFO DAGScheduler: Job 34 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.099336 s
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 947
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 925
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 904
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 932
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 916
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 1000
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 977
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 953
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 906
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 1019
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 955
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 979
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 937
19/06/29 17:41:57 INFO ContextCleaner: Cleaned accumulator 928
19/06/29 17:41:57 INFO FileFormatWriter: Write Job c9800cb5-7bca-4833-af4b-0e91b0e12741 committed.
19/06/29 17:41:57 INFO FileFormatWriter: Finished processing stats for write job c9800cb5-7bca-4833-af4b-0e91b0e12741.
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:57 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-57_479_397597714184677609-2/-ext-10000/part-00000-d015a458-e19b-41cf-8894-f25bc634ed89-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-d015a458-e19b-41cf-8894-f25bc634ed89-c000, Status:true
19/06/29 17:41:57 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:41:57 INFO log: Updating table stats fast for pg_proc
19/06/29 17:41:57 INFO log: Updated size of table pg_proc to 123
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:41:57 INFO log: Updating table stats fast for pg_proc
19/06/29 17:41:57 INFO log: Updated size of table pg_proc to 123
19/06/29 17:41:57 INFO PgMetadata: Registering a function `current_schema` in a system catalog `pg_proc`
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-57_861_8601833651786817848-2
19/06/29 17:41:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:57 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:41:57 INFO DAGScheduler: Got job 35 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:41:57 INFO DAGScheduler: Final stage: ResultStage 35 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:41:57 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:57 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:57 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[71] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:41:57 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 215.3 KB, free 365.5 MB)
19/06/29 17:41:57 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 76.0 KB, free 365.4 MB)
19/06/29 17:41:57 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 35758d068699:33063 (size: 76.0 KB, free: 366.1 MB)
19/06/29 17:41:57 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[71] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:57 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks
19/06/29 17:41:57 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 35, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:41:57 INFO Executor: Running task 0.0 in stage 35.0 (TID 35)
19/06/29 17:41:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:57 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174157_0035_m_000000_35' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-57_861_8601833651786817848-2/-ext-10000/_temporary/0/task_20190629174157_0035_m_000000
19/06/29 17:41:57 INFO SparkHadoopMapRedUtil: attempt_20190629174157_0035_m_000000_35: Committed
19/06/29 17:41:57 INFO Executor: Finished task 0.0 in stage 35.0 (TID 35). 1979 bytes result sent to driver
19/06/29 17:41:57 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 35) in 22 ms on localhost (executor driver) (1/1)
19/06/29 17:41:57 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
19/06/29 17:41:57 INFO DAGScheduler: ResultStage 35 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.046 s
19/06/29 17:41:57 INFO DAGScheduler: Job 35 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.048227 s
19/06/29 17:41:57 INFO FileFormatWriter: Write Job 49c101a9-e1ee-444f-9b9b-6e77831ee427 committed.
19/06/29 17:41:57 INFO FileFormatWriter: Finished processing stats for write job 49c101a9-e1ee-444f-9b9b-6e77831ee427.
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:57 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:57 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-57_861_8601833651786817848-2/-ext-10000/part-00000-02ca7155-2efc-4e47-8eb0-fc49cb31b258-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-02ca7155-2efc-4e47-8eb0-fc49cb31b258-c000, Status:true
19/06/29 17:41:57 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:41:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:41:58 INFO log: Updating table stats fast for pg_proc
19/06/29 17:41:58 INFO log: Updated size of table pg_proc to 171
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:41:58 INFO log: Updating table stats fast for pg_proc
19/06/29 17:41:58 INFO log: Updated size of table pg_proc to 171
19/06/29 17:41:58 INFO PgMetadata: Registering a function `current_schemas` in a system catalog `pg_proc`
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-58_112_5716590544122412027-2
19/06/29 17:41:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:58 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:41:58 INFO DAGScheduler: Got job 36 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:41:58 INFO DAGScheduler: Final stage: ResultStage 36 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:41:58 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:58 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:58 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[73] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:41:58 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 215.3 KB, free 365.2 MB)
19/06/29 17:41:58 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 76.0 KB, free 365.2 MB)
19/06/29 17:41:58 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 35758d068699:33063 (size: 76.0 KB, free: 366.0 MB)
19/06/29 17:41:58 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[73] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:58 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks
19/06/29 17:41:58 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 36, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:41:58 INFO Executor: Running task 0.0 in stage 36.0 (TID 36)
19/06/29 17:41:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:58 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174158_0036_m_000000_36' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-58_112_5716590544122412027-2/-ext-10000/_temporary/0/task_20190629174158_0036_m_000000
19/06/29 17:41:58 INFO SparkHadoopMapRedUtil: attempt_20190629174158_0036_m_000000_36: Committed
19/06/29 17:41:58 INFO Executor: Finished task 0.0 in stage 36.0 (TID 36). 1979 bytes result sent to driver
19/06/29 17:41:58 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 36) in 24 ms on localhost (executor driver) (1/1)
19/06/29 17:41:58 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
19/06/29 17:41:58 INFO DAGScheduler: ResultStage 36 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.078 s
19/06/29 17:41:58 INFO DAGScheduler: Job 36 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.079670 s
19/06/29 17:41:58 INFO FileFormatWriter: Write Job 97adf963-b5b8-4c7c-9b67-abbcd129450e committed.
19/06/29 17:41:58 INFO FileFormatWriter: Finished processing stats for write job 97adf963-b5b8-4c7c-9b67-abbcd129450e.
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:58 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-58_112_5716590544122412027-2/-ext-10000/part-00000-81d65530-7c05-4fa9-adba-20fdfea38916-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-81d65530-7c05-4fa9-adba-20fdfea38916-c000, Status:true
19/06/29 17:41:58 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:41:58 INFO log: Updating table stats fast for pg_proc
19/06/29 17:41:58 INFO log: Updated size of table pg_proc to 220
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:41:58 INFO log: Updating table stats fast for pg_proc
19/06/29 17:41:58 INFO log: Updated size of table pg_proc to 220
19/06/29 17:41:58 INFO PgMetadata: Registering a function `pg_catalog.array_to_string` in a system catalog `pg_proc`
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-58_402_2597739269153276411-2
19/06/29 17:41:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:58 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:41:58 INFO DAGScheduler: Got job 37 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:41:58 INFO DAGScheduler: Final stage: ResultStage 37 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:41:58 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:58 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:58 INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[75] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:41:58 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 215.3 KB, free 365.0 MB)
19/06/29 17:41:58 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 76.0 KB, free 364.9 MB)
19/06/29 17:41:58 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 35758d068699:33063 (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:41:58 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[75] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:58 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks
19/06/29 17:41:58 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 37, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:41:58 INFO Executor: Running task 0.0 in stage 37.0 (TID 37)
19/06/29 17:41:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:58 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174158_0037_m_000000_37' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-58_402_2597739269153276411-2/-ext-10000/_temporary/0/task_20190629174158_0037_m_000000
19/06/29 17:41:58 INFO SparkHadoopMapRedUtil: attempt_20190629174158_0037_m_000000_37: Committed
19/06/29 17:41:58 INFO Executor: Finished task 0.0 in stage 37.0 (TID 37). 1979 bytes result sent to driver
19/06/29 17:41:58 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 37) in 20 ms on localhost (executor driver) (1/1)
19/06/29 17:41:58 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
19/06/29 17:41:58 INFO DAGScheduler: ResultStage 37 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.043 s
19/06/29 17:41:58 INFO DAGScheduler: Job 37 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.045023 s
19/06/29 17:41:58 INFO FileFormatWriter: Write Job a6b532da-2bc8-4bb5-9f35-14580b8e3ebd committed.
19/06/29 17:41:58 INFO FileFormatWriter: Finished processing stats for write job a6b532da-2bc8-4bb5-9f35-14580b8e3ebd.
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:58 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-58_402_2597739269153276411-2/-ext-10000/part-00000-a1c19afd-7435-4187-a9d7-8c3cc5dc8582-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-a1c19afd-7435-4187-a9d7-8c3cc5dc8582-c000, Status:true
19/06/29 17:41:58 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:41:58 INFO log: Updating table stats fast for pg_proc
19/06/29 17:41:58 INFO log: Updated size of table pg_proc to 268
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:41:58 INFO log: Updating table stats fast for pg_proc
19/06/29 17:41:58 INFO log: Updated size of table pg_proc to 268
19/06/29 17:41:58 INFO PgMetadata: Registering a function `pg_catalog.format_type` in a system catalog `pg_proc`
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-58_640_3606726375834735247-2
19/06/29 17:41:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:58 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:41:58 INFO DAGScheduler: Got job 38 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:41:58 INFO DAGScheduler: Final stage: ResultStage 38 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:41:58 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:58 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:58 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[77] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:41:58 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 215.3 KB, free 364.7 MB)
19/06/29 17:41:58 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 76.0 KB, free 364.6 MB)
19/06/29 17:41:58 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 35758d068699:33063 (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:41:58 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[77] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:58 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks
19/06/29 17:41:58 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 38, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:41:58 INFO Executor: Running task 0.0 in stage 38.0 (TID 38)
19/06/29 17:41:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:58 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174158_0038_m_000000_38' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-58_640_3606726375834735247-2/-ext-10000/_temporary/0/task_20190629174158_0038_m_000000
19/06/29 17:41:58 INFO SparkHadoopMapRedUtil: attempt_20190629174158_0038_m_000000_38: Committed
19/06/29 17:41:58 INFO Executor: Finished task 0.0 in stage 38.0 (TID 38). 1979 bytes result sent to driver
19/06/29 17:41:58 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 38) in 22 ms on localhost (executor driver) (1/1)
19/06/29 17:41:58 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
19/06/29 17:41:58 INFO DAGScheduler: ResultStage 38 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.046 s
19/06/29 17:41:58 INFO DAGScheduler: Job 38 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.048003 s
19/06/29 17:41:58 INFO FileFormatWriter: Write Job 30ae51a2-d78c-4f38-8997-b6d2a7226121 committed.
19/06/29 17:41:58 INFO FileFormatWriter: Finished processing stats for write job 30ae51a2-d78c-4f38-8997-b6d2a7226121.
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:58 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-58_640_3606726375834735247-2/-ext-10000/part-00000-f46a4129-6cc1-4560-b32e-44e13fdfd49a-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-f46a4129-6cc1-4560-b32e-44e13fdfd49a-c000, Status:true
19/06/29 17:41:58 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:41:58 INFO log: Updating table stats fast for pg_proc
19/06/29 17:41:58 INFO log: Updated size of table pg_proc to 313
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:41:58 INFO log: Updating table stats fast for pg_proc
19/06/29 17:41:58 INFO log: Updated size of table pg_proc to 313
19/06/29 17:41:58 INFO PgMetadata: Registering a function `pg_catalog.obj_description` in a system catalog `pg_proc`
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-58_879_4851108952432984905-2
19/06/29 17:41:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:58 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:41:58 INFO DAGScheduler: Got job 39 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:41:58 INFO DAGScheduler: Final stage: ResultStage 39 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:41:58 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:58 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:58 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[79] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:41:58 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 215.3 KB, free 364.4 MB)
19/06/29 17:41:58 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 76.0 KB, free 364.3 MB)
19/06/29 17:41:58 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 35758d068699:33063 (size: 76.0 KB, free: 365.8 MB)
19/06/29 17:41:58 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[79] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:58 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks
19/06/29 17:41:58 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 39, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:41:58 INFO Executor: Running task 0.0 in stage 39.0 (TID 39)
19/06/29 17:41:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:58 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174158_0039_m_000000_39' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-58_879_4851108952432984905-2/-ext-10000/_temporary/0/task_20190629174158_0039_m_000000
19/06/29 17:41:58 INFO SparkHadoopMapRedUtil: attempt_20190629174158_0039_m_000000_39: Committed
19/06/29 17:41:58 INFO Executor: Finished task 0.0 in stage 39.0 (TID 39). 1979 bytes result sent to driver
19/06/29 17:41:58 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 39) in 22 ms on localhost (executor driver) (1/1)
19/06/29 17:41:58 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
19/06/29 17:41:58 INFO DAGScheduler: ResultStage 39 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.049 s
19/06/29 17:41:58 INFO DAGScheduler: Job 39 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.055311 s
19/06/29 17:41:58 INFO FileFormatWriter: Write Job bfefb8b1-abb8-42b5-b92a-aa9d9188347f committed.
19/06/29 17:41:58 INFO FileFormatWriter: Finished processing stats for write job bfefb8b1-abb8-42b5-b92a-aa9d9188347f.
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:58 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:58 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-58_879_4851108952432984905-2/-ext-10000/part-00000-d48b6fcb-5fcb-4930-babc-19435c3f8c8b-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-d48b6fcb-5fcb-4930-babc-19435c3f8c8b-c000, Status:true
19/06/29 17:41:59 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:41:59 INFO log: Updating table stats fast for pg_proc
19/06/29 17:41:59 INFO log: Updated size of table pg_proc to 362
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:41:59 INFO log: Updating table stats fast for pg_proc
19/06/29 17:41:59 INFO log: Updated size of table pg_proc to 362
19/06/29 17:41:59 INFO PgMetadata: Registering a function `pg_catalog.oidvectortypes` in a system catalog `pg_proc`
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-59_123_9077162060846010139-2
19/06/29 17:41:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:59 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:41:59 INFO DAGScheduler: Got job 40 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:41:59 INFO DAGScheduler: Final stage: ResultStage 40 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:41:59 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:59 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:59 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[81] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:41:59 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 215.3 KB, free 364.1 MB)
19/06/29 17:41:59 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 76.0 KB, free 364.0 MB)
19/06/29 17:41:59 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 35758d068699:33063 (size: 76.0 KB, free: 365.7 MB)
19/06/29 17:41:59 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[81] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:59 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks
19/06/29 17:41:59 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 40, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:41:59 INFO Executor: Running task 0.0 in stage 40.0 (TID 40)
19/06/29 17:41:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:59 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174159_0040_m_000000_40' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-59_123_9077162060846010139-2/-ext-10000/_temporary/0/task_20190629174159_0040_m_000000
19/06/29 17:41:59 INFO SparkHadoopMapRedUtil: attempt_20190629174159_0040_m_000000_40: Committed
19/06/29 17:41:59 INFO Executor: Finished task 0.0 in stage 40.0 (TID 40). 1979 bytes result sent to driver
19/06/29 17:41:59 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 40) in 30 ms on localhost (executor driver) (1/1)
19/06/29 17:41:59 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
19/06/29 17:41:59 INFO DAGScheduler: ResultStage 40 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.061 s
19/06/29 17:41:59 INFO DAGScheduler: Job 40 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.062804 s
19/06/29 17:41:59 INFO FileFormatWriter: Write Job 78b55f8c-c366-4acc-9051-2f3f49e44fb8 committed.
19/06/29 17:41:59 INFO FileFormatWriter: Finished processing stats for write job 78b55f8c-c366-4acc-9051-2f3f49e44fb8.
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:59 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-59_123_9077162060846010139-2/-ext-10000/part-00000-8258e892-84bd-4ade-bea6-40c12e6f739b-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-8258e892-84bd-4ade-bea6-40c12e6f739b-c000, Status:true
19/06/29 17:41:59 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:41:59 INFO log: Updating table stats fast for pg_proc
19/06/29 17:41:59 INFO log: Updated size of table pg_proc to 410
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:41:59 INFO log: Updating table stats fast for pg_proc
19/06/29 17:41:59 INFO log: Updated size of table pg_proc to 410
19/06/29 17:41:59 INFO PgMetadata: Registering a function `pg_catalog.pg_encoding_to_char` in a system catalog `pg_proc`
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-59_457_459888455831385805-2
19/06/29 17:41:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:59 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:41:59 INFO DAGScheduler: Got job 41 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:41:59 INFO DAGScheduler: Final stage: ResultStage 41 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:41:59 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:59 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:59 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[83] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:41:59 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 215.3 KB, free 363.8 MB)
19/06/29 17:41:59 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 75.9 KB, free 363.7 MB)
19/06/29 17:41:59 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 35758d068699:33063 (size: 75.9 KB, free: 365.6 MB)
19/06/29 17:41:59 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[83] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:59 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks
19/06/29 17:41:59 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 41, localhost, executor driver, partition 0, PROCESS_LOCAL, 8124 bytes)
19/06/29 17:41:59 INFO Executor: Running task 0.0 in stage 41.0 (TID 41)
19/06/29 17:41:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:59 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174159_0041_m_000000_41' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-59_457_459888455831385805-2/-ext-10000/_temporary/0/task_20190629174159_0041_m_000000
19/06/29 17:41:59 INFO SparkHadoopMapRedUtil: attempt_20190629174159_0041_m_000000_41: Committed
19/06/29 17:41:59 INFO Executor: Finished task 0.0 in stage 41.0 (TID 41). 2022 bytes result sent to driver
19/06/29 17:41:59 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 41) in 28 ms on localhost (executor driver) (1/1)
19/06/29 17:41:59 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
19/06/29 17:41:59 INFO DAGScheduler: ResultStage 41 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.056 s
19/06/29 17:41:59 INFO DAGScheduler: Job 41 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.058608 s
19/06/29 17:41:59 INFO FileFormatWriter: Write Job 34c86ccb-f57f-4237-97e9-6fdbb5cb1256 committed.
19/06/29 17:41:59 INFO FileFormatWriter: Finished processing stats for write job 34c86ccb-f57f-4237-97e9-6fdbb5cb1256.
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:59 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-59_457_459888455831385805-2/-ext-10000/part-00000-e3483a15-9774-4897-bb06-fc55c74046b1-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-e3483a15-9774-4897-bb06-fc55c74046b1-c000, Status:true
19/06/29 17:41:59 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:41:59 INFO log: Updating table stats fast for pg_proc
19/06/29 17:41:59 INFO log: Updated size of table pg_proc to 463
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:41:59 INFO log: Updating table stats fast for pg_proc
19/06/29 17:41:59 INFO log: Updated size of table pg_proc to 463
19/06/29 17:41:59 INFO PgMetadata: Registering a function `pg_catalog.pg_function_is_visible` in a system catalog `pg_proc`
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-59_714_3125823897158695004-2
19/06/29 17:41:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:59 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:41:59 INFO DAGScheduler: Got job 42 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:41:59 INFO DAGScheduler: Final stage: ResultStage 42 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:41:59 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:41:59 INFO DAGScheduler: Missing parents: List()
19/06/29 17:41:59 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[85] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:41:59 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 215.3 KB, free 363.5 MB)
19/06/29 17:41:59 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 76.0 KB, free 363.5 MB)
19/06/29 17:41:59 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 35758d068699:33063 (size: 76.0 KB, free: 365.6 MB)
19/06/29 17:41:59 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1161
19/06/29 17:41:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[85] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:41:59 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks
19/06/29 17:41:59 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 42, localhost, executor driver, partition 0, PROCESS_LOCAL, 8124 bytes)
19/06/29 17:41:59 INFO Executor: Running task 0.0 in stage 42.0 (TID 42)
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1098
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1239
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1118
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1064
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1087
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1170
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1025
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1165
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1026
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1032
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1193
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1031
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1085
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1075
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1090
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1245
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1116
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1188
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1218
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1158
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1220
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1021
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1049
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1045
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1070
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1237
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1185
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1054
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1078
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1059
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1120
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1258
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1144
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1133
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1096
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1164
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1191
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1230
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1080
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1050
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1243
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1250
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1094
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1216
19/06/29 17:41:59 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 35758d068699:33063 in memory (size: 76.0 KB, free: 365.6 MB)
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1247
19/06/29 17:41:59 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 35758d068699:33063 in memory (size: 76.0 KB, free: 365.7 MB)
19/06/29 17:41:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1173
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1100
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1076
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1146
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1053
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1113
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1153
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1036
19/06/29 17:41:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1111
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1129
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1199
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1244
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1104
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1211
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1184
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1222
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1166
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1254
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1236
19/06/29 17:41:59 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 35758d068699:33063 in memory (size: 75.9 KB, free: 365.8 MB)
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1066
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1190
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1248
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1179
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1169
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1029
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1205
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1052
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1102
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1034
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1022
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1123
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1172
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1178
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1039
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1103
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1074
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1209
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1130
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1083
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1159
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1121
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1152
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1238
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1071
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1051
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1156
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1192
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1255
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1082
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1047
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1139
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1215
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1048
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1154
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1136
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1037
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1033
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1253
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1126
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1235
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1219
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1079
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1124
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1147
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1196
19/06/29 17:41:59 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 35758d068699:33063 in memory (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1122
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1194
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1095
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1183
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1088
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1077
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1162
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1177
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1221
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1226
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1097
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1187
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1072
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1204
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1240
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1149
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1043
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1229
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1106
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1148
19/06/29 17:41:59 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 35758d068699:33063 in memory (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1174
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1117
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1182
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1225
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1233
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1180
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1210
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1040
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1175
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1202
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1206
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1112
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1134
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1223
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1228
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1135
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1231
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1141
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1201
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1213
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1143
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1115
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1131
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1058
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1198
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1137
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1208
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1168
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1234
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1200
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1157
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1214
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1252
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1035
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1160
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1151
19/06/29 17:41:59 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 35758d068699:33063 in memory (size: 76.0 KB, free: 366.0 MB)
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1073
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1142
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1027
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1056
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1107
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1189
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1101
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1140
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1181
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1024
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1114
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1062
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1241
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1020
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1176
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1161
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1259
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1028
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1105
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1044
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1207
19/06/29 17:41:59 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174159_0042_m_000000_42' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-59_714_3125823897158695004-2/-ext-10000/_temporary/0/task_20190629174159_0042_m_000000
19/06/29 17:41:59 INFO SparkHadoopMapRedUtil: attempt_20190629174159_0042_m_000000_42: Committed
19/06/29 17:41:59 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 35758d068699:33063 in memory (size: 76.0 KB, free: 366.1 MB)
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1138
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1249
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1167
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1057
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1099
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1127
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1061
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1081
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1145
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1046
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1110
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1257
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1091
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1055
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1217
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1093
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1197
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1251
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1067
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1195
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1109
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1086
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1132
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1186
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1038
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1256
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1171
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1119
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1041
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1227
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1060
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1232
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1246
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1125
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1150
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1069
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1068
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1203
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1023
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1128
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1030
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1063
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1065
19/06/29 17:41:59 INFO Executor: Finished task 0.0 in stage 42.0 (TID 42). 2022 bytes result sent to driver
19/06/29 17:41:59 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 35758d068699:33063 in memory (size: 75.9 KB, free: 366.2 MB)
19/06/29 17:41:59 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 42) in 94 ms on localhost (executor driver) (1/1)
19/06/29 17:41:59 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
19/06/29 17:41:59 INFO DAGScheduler: ResultStage 42 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.126 s
19/06/29 17:41:59 INFO DAGScheduler: Job 42 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.128102 s
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1163
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1155
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1242
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1092
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1108
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1089
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1212
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1084
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1042
19/06/29 17:41:59 INFO ContextCleaner: Cleaned accumulator 1224
19/06/29 17:41:59 INFO FileFormatWriter: Write Job c7a3c501-fa2a-48d3-b039-6d3155446006 committed.
19/06/29 17:41:59 INFO FileFormatWriter: Finished processing stats for write job c7a3c501-fa2a-48d3-b039-6d3155446006.
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:41:59 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-41-59_714_3125823897158695004-2/-ext-10000/part-00000-29d221f3-20a4-467e-bd69-ed69f25a7a17-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-29d221f3-20a4-467e-bd69-ed69f25a7a17-c000, Status:true
19/06/29 17:41:59 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:41:59 INFO log: Updating table stats fast for pg_proc
19/06/29 17:41:59 INFO log: Updated size of table pg_proc to 519
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:41:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:41:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:42:00 INFO log: Updating table stats fast for pg_proc
19/06/29 17:42:00 INFO log: Updated size of table pg_proc to 519
19/06/29 17:42:00 INFO PgMetadata: Registering a function `pg_catalog.pg_get_expr` in a system catalog `pg_proc`
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-42-00_076_4248236094113568740-2
19/06/29 17:42:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:42:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:42:00 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:42:00 INFO DAGScheduler: Got job 43 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:42:00 INFO DAGScheduler: Final stage: ResultStage 43 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:42:00 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:42:00 INFO DAGScheduler: Missing parents: List()
19/06/29 17:42:00 INFO DAGScheduler: Submitting ResultStage 43 (MapPartitionsRDD[87] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:42:00 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 215.3 KB, free 365.5 MB)
19/06/29 17:42:00 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 76.0 KB, free 365.4 MB)
19/06/29 17:42:00 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 35758d068699:33063 (size: 76.0 KB, free: 366.1 MB)
19/06/29 17:42:00 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1161
19/06/29 17:42:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[87] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:42:00 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks
19/06/29 17:42:00 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 43, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:42:00 INFO Executor: Running task 0.0 in stage 43.0 (TID 43)
19/06/29 17:42:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:42:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:42:00 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174200_0043_m_000000_43' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-42-00_076_4248236094113568740-2/-ext-10000/_temporary/0/task_20190629174200_0043_m_000000
19/06/29 17:42:00 INFO SparkHadoopMapRedUtil: attempt_20190629174200_0043_m_000000_43: Committed
19/06/29 17:42:00 INFO Executor: Finished task 0.0 in stage 43.0 (TID 43). 1979 bytes result sent to driver
19/06/29 17:42:00 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 43) in 29 ms on localhost (executor driver) (1/1)
19/06/29 17:42:00 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool 
19/06/29 17:42:00 INFO DAGScheduler: ResultStage 43 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.054 s
19/06/29 17:42:00 INFO DAGScheduler: Job 43 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.056160 s
19/06/29 17:42:00 INFO FileFormatWriter: Write Job c097faed-9841-4945-a746-abd4afd2b672 committed.
19/06/29 17:42:00 INFO FileFormatWriter: Finished processing stats for write job c097faed-9841-4945-a746-abd4afd2b672.
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:42:00 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-42-00_076_4248236094113568740-2/-ext-10000/part-00000-975142e3-035f-4107-b3dc-964f9a47c3ca-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-975142e3-035f-4107-b3dc-964f9a47c3ca-c000, Status:true
19/06/29 17:42:00 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:42:00 INFO log: Updating table stats fast for pg_proc
19/06/29 17:42:00 INFO log: Updated size of table pg_proc to 564
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:42:00 INFO log: Updating table stats fast for pg_proc
19/06/29 17:42:00 INFO log: Updated size of table pg_proc to 564
19/06/29 17:42:00 INFO PgMetadata: Registering a function `pg_catalog.pg_get_function_arguments` in a system catalog `pg_proc`
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-42-00_321_6809018060895243044-2
19/06/29 17:42:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:42:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:42:00 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:42:00 INFO DAGScheduler: Got job 44 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:42:00 INFO DAGScheduler: Final stage: ResultStage 44 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:42:00 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:42:00 INFO DAGScheduler: Missing parents: List()
19/06/29 17:42:00 INFO DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[89] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:42:00 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 215.3 KB, free 365.2 MB)
19/06/29 17:42:00 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 76.0 KB, free 365.2 MB)
19/06/29 17:42:00 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 35758d068699:33063 (size: 76.0 KB, free: 366.0 MB)
19/06/29 17:42:00 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1161
19/06/29 17:42:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[89] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:42:00 INFO TaskSchedulerImpl: Adding task set 44.0 with 1 tasks
19/06/29 17:42:00 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 44, localhost, executor driver, partition 0, PROCESS_LOCAL, 8132 bytes)
19/06/29 17:42:00 INFO Executor: Running task 0.0 in stage 44.0 (TID 44)
19/06/29 17:42:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:42:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:42:00 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174200_0044_m_000000_44' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-42-00_321_6809018060895243044-2/-ext-10000/_temporary/0/task_20190629174200_0044_m_000000
19/06/29 17:42:00 INFO SparkHadoopMapRedUtil: attempt_20190629174200_0044_m_000000_44: Committed
19/06/29 17:42:00 INFO Executor: Finished task 0.0 in stage 44.0 (TID 44). 1979 bytes result sent to driver
19/06/29 17:42:00 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 44) in 23 ms on localhost (executor driver) (1/1)
19/06/29 17:42:00 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool 
19/06/29 17:42:00 INFO DAGScheduler: ResultStage 44 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.048 s
19/06/29 17:42:00 INFO DAGScheduler: Job 44 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.050368 s
19/06/29 17:42:00 INFO FileFormatWriter: Write Job 24804c4f-01c2-499c-94e7-a8fa94a58a11 committed.
19/06/29 17:42:00 INFO FileFormatWriter: Finished processing stats for write job 24804c4f-01c2-499c-94e7-a8fa94a58a11.
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:42:00 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-42-00_321_6809018060895243044-2/-ext-10000/part-00000-57d596a3-cb51-4748-b569-68325eb07fb7-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-57d596a3-cb51-4748-b569-68325eb07fb7-c000, Status:true
19/06/29 17:42:00 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:42:00 INFO log: Updating table stats fast for pg_proc
19/06/29 17:42:00 INFO log: Updated size of table pg_proc to 623
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:42:00 INFO log: Updating table stats fast for pg_proc
19/06/29 17:42:00 INFO log: Updated size of table pg_proc to 623
19/06/29 17:42:00 INFO PgMetadata: Registering a function `pg_catalog.pg_get_function_result` in a system catalog `pg_proc`
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-42-00_599_6388171470006100111-2
19/06/29 17:42:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:42:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:42:00 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:42:00 INFO DAGScheduler: Got job 45 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:42:00 INFO DAGScheduler: Final stage: ResultStage 45 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:42:00 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:42:00 INFO DAGScheduler: Missing parents: List()
19/06/29 17:42:00 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[91] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:42:00 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 215.3 KB, free 365.0 MB)
19/06/29 17:42:00 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 76.0 KB, free 364.9 MB)
19/06/29 17:42:00 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 35758d068699:33063 (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:42:00 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1161
19/06/29 17:42:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[91] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:42:00 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks
19/06/29 17:42:00 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 45, localhost, executor driver, partition 0, PROCESS_LOCAL, 8124 bytes)
19/06/29 17:42:00 INFO Executor: Running task 0.0 in stage 45.0 (TID 45)
19/06/29 17:42:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:42:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:42:00 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174200_0045_m_000000_45' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-42-00_599_6388171470006100111-2/-ext-10000/_temporary/0/task_20190629174200_0045_m_000000
19/06/29 17:42:00 INFO SparkHadoopMapRedUtil: attempt_20190629174200_0045_m_000000_45: Committed
19/06/29 17:42:00 INFO Executor: Finished task 0.0 in stage 45.0 (TID 45). 1979 bytes result sent to driver
19/06/29 17:42:00 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 45) in 22 ms on localhost (executor driver) (1/1)
19/06/29 17:42:00 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
19/06/29 17:42:00 INFO DAGScheduler: ResultStage 45 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.044 s
19/06/29 17:42:00 INFO DAGScheduler: Job 45 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.046098 s
19/06/29 17:42:00 INFO FileFormatWriter: Write Job 8e032603-f8ee-4672-9e32-9c11b7e8c83b committed.
19/06/29 17:42:00 INFO FileFormatWriter: Finished processing stats for write job 8e032603-f8ee-4672-9e32-9c11b7e8c83b.
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:42:00 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-42-00_599_6388171470006100111-2/-ext-10000/part-00000-3ab74090-d727-47b6-957d-49ef09e16c8c-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-3ab74090-d727-47b6-957d-49ef09e16c8c-c000, Status:true
19/06/29 17:42:00 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:42:00 INFO log: Updating table stats fast for pg_proc
19/06/29 17:42:00 INFO log: Updated size of table pg_proc to 679
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:42:00 INFO log: Updating table stats fast for pg_proc
19/06/29 17:42:00 INFO log: Updated size of table pg_proc to 679
19/06/29 17:42:00 INFO PgMetadata: Registering a function `pg_catalog.pg_get_userbyid` in a system catalog `pg_proc`
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-42-00_834_6314402706307902962-2
19/06/29 17:42:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:42:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:42:00 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:42:00 INFO DAGScheduler: Got job 46 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:42:00 INFO DAGScheduler: Final stage: ResultStage 46 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:42:00 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:42:00 INFO DAGScheduler: Missing parents: List()
19/06/29 17:42:00 INFO DAGScheduler: Submitting ResultStage 46 (MapPartitionsRDD[93] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:42:00 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 215.3 KB, free 364.7 MB)
19/06/29 17:42:00 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 76.0 KB, free 364.6 MB)
19/06/29 17:42:00 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 35758d068699:33063 (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:42:00 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1161
19/06/29 17:42:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[93] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:42:00 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks
19/06/29 17:42:00 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 46, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:42:00 INFO Executor: Running task 0.0 in stage 46.0 (TID 46)
19/06/29 17:42:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:42:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:42:00 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174200_0046_m_000000_46' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-42-00_834_6314402706307902962-2/-ext-10000/_temporary/0/task_20190629174200_0046_m_000000
19/06/29 17:42:00 INFO SparkHadoopMapRedUtil: attempt_20190629174200_0046_m_000000_46: Committed
19/06/29 17:42:00 INFO Executor: Finished task 0.0 in stage 46.0 (TID 46). 1979 bytes result sent to driver
19/06/29 17:42:00 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 46) in 22 ms on localhost (executor driver) (1/1)
19/06/29 17:42:00 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool 
19/06/29 17:42:00 INFO DAGScheduler: ResultStage 46 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.047 s
19/06/29 17:42:00 INFO DAGScheduler: Job 46 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.048588 s
19/06/29 17:42:00 INFO FileFormatWriter: Write Job 82ae4215-34b8-4a0e-94d1-191b0038843a committed.
19/06/29 17:42:00 INFO FileFormatWriter: Finished processing stats for write job 82ae4215-34b8-4a0e-94d1-191b0038843a.
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:42:00 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-42-00_834_6314402706307902962-2/-ext-10000/part-00000-f030e74f-1b06-4785-8900-704692b087cc-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-f030e74f-1b06-4785-8900-704692b087cc-c000, Status:true
19/06/29 17:42:00 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:42:00 INFO log: Updating table stats fast for pg_proc
19/06/29 17:42:00 INFO log: Updated size of table pg_proc to 728
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:42:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:01 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:42:01 INFO log: Updating table stats fast for pg_proc
19/06/29 17:42:01 INFO log: Updated size of table pg_proc to 728
19/06/29 17:42:01 INFO PgMetadata: Registering a function `pg_catalog.pg_table_is_visible` in a system catalog `pg_proc`
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:01 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-42-01_080_5314776912242756938-2
19/06/29 17:42:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:42:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:42:01 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:42:01 INFO DAGScheduler: Got job 47 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:42:01 INFO DAGScheduler: Final stage: ResultStage 47 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:42:01 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:42:01 INFO DAGScheduler: Missing parents: List()
19/06/29 17:42:01 INFO DAGScheduler: Submitting ResultStage 47 (MapPartitionsRDD[95] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:42:01 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 215.3 KB, free 364.4 MB)
19/06/29 17:42:01 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 76.0 KB, free 364.3 MB)
19/06/29 17:42:01 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 35758d068699:33063 (size: 76.0 KB, free: 365.8 MB)
19/06/29 17:42:01 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1161
19/06/29 17:42:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 47 (MapPartitionsRDD[95] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:42:01 INFO TaskSchedulerImpl: Adding task set 47.0 with 1 tasks
19/06/29 17:42:01 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 47, localhost, executor driver, partition 0, PROCESS_LOCAL, 8124 bytes)
19/06/29 17:42:01 INFO Executor: Running task 0.0 in stage 47.0 (TID 47)
19/06/29 17:42:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:42:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:42:01 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174201_0047_m_000000_47' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-42-01_080_5314776912242756938-2/-ext-10000/_temporary/0/task_20190629174201_0047_m_000000
19/06/29 17:42:01 INFO SparkHadoopMapRedUtil: attempt_20190629174201_0047_m_000000_47: Committed
19/06/29 17:42:01 INFO Executor: Finished task 0.0 in stage 47.0 (TID 47). 2022 bytes result sent to driver
19/06/29 17:42:01 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 47) in 21 ms on localhost (executor driver) (1/1)
19/06/29 17:42:01 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool 
19/06/29 17:42:01 INFO DAGScheduler: ResultStage 47 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.045 s
19/06/29 17:42:01 INFO DAGScheduler: Job 47 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.047356 s
19/06/29 17:42:01 INFO FileFormatWriter: Write Job 8ecb652e-94f9-4d38-95c2-9a1ea3fbe209 committed.
19/06/29 17:42:01 INFO FileFormatWriter: Finished processing stats for write job 8ecb652e-94f9-4d38-95c2-9a1ea3fbe209.
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:01 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:42:01 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-42-01_080_5314776912242756938-2/-ext-10000/part-00000-ab1cb3a4-d058-4c77-b4ac-211165c772f2-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-ab1cb3a4-d058-4c77-b4ac-211165c772f2-c000, Status:true
19/06/29 17:42:01 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:42:01 INFO log: Updating table stats fast for pg_proc
19/06/29 17:42:01 INFO log: Updated size of table pg_proc to 781
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:01 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:42:01 INFO log: Updating table stats fast for pg_proc
19/06/29 17:42:01 INFO log: Updated size of table pg_proc to 781
19/06/29 17:42:01 INFO PgMetadata: Registering a function `pg_catalog.regtype` in a system catalog `pg_proc`
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:01 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-42-01_296_2538952306595643986-2
19/06/29 17:42:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:42:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:42:01 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:42:01 INFO DAGScheduler: Got job 48 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:42:01 INFO DAGScheduler: Final stage: ResultStage 48 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:42:01 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:42:01 INFO DAGScheduler: Missing parents: List()
19/06/29 17:42:01 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[97] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:42:01 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 215.3 KB, free 364.1 MB)
19/06/29 17:42:01 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 76.0 KB, free 364.0 MB)
19/06/29 17:42:01 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 35758d068699:33063 (size: 76.0 KB, free: 365.7 MB)
19/06/29 17:42:01 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1161
19/06/29 17:42:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[97] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:42:01 INFO TaskSchedulerImpl: Adding task set 48.0 with 1 tasks
19/06/29 17:42:01 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 48, localhost, executor driver, partition 0, PROCESS_LOCAL, 8108 bytes)
19/06/29 17:42:01 INFO Executor: Running task 0.0 in stage 48.0 (TID 48)
19/06/29 17:42:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:42:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:42:01 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174201_0048_m_000000_48' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-42-01_296_2538952306595643986-2/-ext-10000/_temporary/0/task_20190629174201_0048_m_000000
19/06/29 17:42:01 INFO SparkHadoopMapRedUtil: attempt_20190629174201_0048_m_000000_48: Committed
19/06/29 17:42:01 INFO Executor: Finished task 0.0 in stage 48.0 (TID 48). 1979 bytes result sent to driver
19/06/29 17:42:01 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 48) in 27 ms on localhost (executor driver) (1/1)
19/06/29 17:42:01 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool 
19/06/29 17:42:01 INFO DAGScheduler: ResultStage 48 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.051 s
19/06/29 17:42:01 INFO DAGScheduler: Job 48 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.053164 s
19/06/29 17:42:01 INFO FileFormatWriter: Write Job 6854eab2-1d18-40e6-a589-ebc6c87d2831 committed.
19/06/29 17:42:01 INFO FileFormatWriter: Finished processing stats for write job 6854eab2-1d18-40e6-a589-ebc6c87d2831.
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:01 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:42:01 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-42-01_296_2538952306595643986-2/-ext-10000/part-00000-69802a6c-be4a-45ed-bbd1-5851a7b2da33-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-69802a6c-be4a-45ed-bbd1-5851a7b2da33-c000, Status:true
19/06/29 17:42:01 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:42:01 INFO log: Updating table stats fast for pg_proc
19/06/29 17:42:01 INFO log: Updated size of table pg_proc to 822
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:42:01 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:42:01 INFO log: Updating table stats fast for pg_proc
19/06/29 17:42:01 INFO log: Updated size of table pg_proc to 822
19/06/29 17:42:01 INFO PgV3MessageHandler: Open a session (sessionId=0, channelId=1898583466 userName=fdb hostAddr=35758d068699)
19/06/29 17:42:01 INFO PgWireProtocol: Query: statements=set DateStyle to 'ISO'
19/06/29 17:42:01 INFO OperationImpl: Running query with d8c92806-f599-4b7a-b34a-4ca2efd870ed:
Query:
set DateStyle to 'ISO'
Analyzed Plan:
SetCommand (DateStyle to 'ISO',None)

       
19/06/29 17:42:01 INFO PgWireProtocol: Query: statements=set extra_float_digits to 2
19/06/29 17:42:01 INFO OperationImpl: Running query with ed243e33-28c7-4c87-8fad-4f05bfa6d93e:
Query:
set extra_float_digits to 2
Analyzed Plan:
SetCommand (extra_float_digits to 2,None)

       
19/06/29 17:42:01 INFO PgWireProtocol: Query: statements=select oid, typbasetype from pg_type where typname = 'lo'
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1395
19/06/29 17:42:01 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 35758d068699:33063 in memory (size: 76.0 KB, free: 365.8 MB)
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1435
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1465
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1360
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1264
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1344
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1295
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1351
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1392
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1305
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1361
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1313
19/06/29 17:42:01 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 35758d068699:33063 in memory (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1334
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1455
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1362
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1401
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1300
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1399
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1364
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1303
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1260
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1314
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1352
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1417
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1354
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1357
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1451
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1406
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1383
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1458
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1273
19/06/29 17:42:01 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 35758d068699:33063 in memory (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1367
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1388
19/06/29 17:42:01 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 35758d068699:33063 in memory (size: 76.0 KB, free: 366.0 MB)
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1384
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1422
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1404
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1373
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1323
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1464
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1296
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1460
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1402
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1377
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1270
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1289
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1339
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1328
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1265
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1441
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1280
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1372
19/06/29 17:42:01 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 35758d068699:33063 in memory (size: 76.0 KB, free: 366.1 MB)
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1312
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1297
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1436
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1408
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1405
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1285
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1428
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1350
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1426
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1359
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1397
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1403
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1448
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1400
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1459
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1398
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1394
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1413
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1298
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1450
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1447
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1294
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1288
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1301
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1432
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1337
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1331
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1387
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1412
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1279
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1452
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1271
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1302
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1431
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1278
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1469
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1421
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1444
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1420
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1358
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1308
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1353
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1322
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1316
19/06/29 17:42:01 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 35758d068699:33063 in memory (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1371
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1340
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1438
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1329
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1343
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1385
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1306
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1266
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1315
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1442
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1393
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1324
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1355
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1293
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1272
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1467
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1318
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1309
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1320
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1317
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1411
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1457
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1291
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1286
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1335
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1283
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1380
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1463
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1368
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1419
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1326
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1423
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1282
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1341
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1370
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1365
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1319
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1410
19/06/29 17:42:01 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 35758d068699:33063 in memory (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1468
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1262
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1330
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1304
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1287
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1356
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1348
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1461
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1376
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1269
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1369
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1347
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1299
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1363
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1327
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1275
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1307
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1418
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1345
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1281
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1425
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1453
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1284
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1366
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1336
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1276
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1342
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1333
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1429
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1416
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1321
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1267
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1389
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1310
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1311
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1449
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1268
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1456
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1466
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1261
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1332
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1446
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1427
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1439
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1430
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1443
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1424
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1409
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1381
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1374
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1414
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1396
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1390
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1375
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1290
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1407
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1378
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1382
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1292
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1391
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1274
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1415
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1440
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1325
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1263
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1434
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1437
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1454
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1445
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1277
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1346
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1349
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1338
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1462
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1379
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1433
19/06/29 17:42:01 INFO ContextCleaner: Cleaned accumulator 1386
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_table : db=default tbl=pg_type
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=pg_type	
19/06/29 17:42:01 ERROR PgV3MessageHandler: Exception detected in 'Query': org.apache.spark.sql.AnalysisException: Table or view not found: pg_type; line 1 pos 29
	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:733)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:685)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:715)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:708)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:708)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:654)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)
	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
	at scala.collection.immutable.List.foldLeft(List.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)
	at org.apache.spark.sql.fdb.service.OperationImpl$$anonfun$org$apache$spark$sql$fdb$service$OperationImpl$$analyzedPlan$1.apply(ExecutorImpl.scala:59)
	at org.apache.spark.sql.fdb.service.OperationImpl$$anonfun$org$apache$spark$sql$fdb$service$OperationImpl$$analyzedPlan$1.apply(ExecutorImpl.scala:59)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.fdb.service.OperationImpl.org$apache$spark$sql$fdb$service$OperationImpl$$analyzedPlan$lzycompute(ExecutorImpl.scala:58)
	at org.apache.spark.sql.fdb.service.OperationImpl.org$apache$spark$sql$fdb$service$OperationImpl$$analyzedPlan(ExecutorImpl.scala:58)
	at org.apache.spark.sql.fdb.service.OperationImpl.outputSchema$lzycompute(ExecutorImpl.scala:90)
	at org.apache.spark.sql.fdb.service.OperationImpl.outputSchema(ExecutorImpl.scala:89)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgWireProtocol$$anonfun$21$$anonfun$apply$24.apply(protocol.scala:615)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgWireProtocol$$anonfun$21$$anonfun$apply$24.apply(protocol.scala:592)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgV3MessageHandler$$anonfun$handleV3Messages$1.apply(protocol.scala:1284)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgV3MessageHandler$$anonfun$handleV3Messages$1.apply(protocol.scala:1275)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgV3MessageHandler.handleV3Messages(protocol.scala:1275)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgV3MessageHandler.channelRead0(protocol.scala:1084)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgV3MessageHandler.channelRead0(protocol.scala:1071)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'pg_type' not found in database 'default';
	at org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)
	at org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.hive.client.HiveClient$class.getTable(HiveClient.scala:81)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:83)
	at org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:118)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:700)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:700)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:699)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:706)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:730)
	... 79 more

19/06/29 17:42:01 INFO PgWireProtocol: Query: statements=select NULL, nspname, NULL from pg_catalog.pg_namespace n where true and nspname not in ('pg_catalog', 'information_schema', 'pg_toast', 'pg_temp_1') order by nspname
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_namespace
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_namespace	
19/06/29 17:42:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_namespace
19/06/29 17:42:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_namespace	
19/06/29 17:42:02 INFO OperationImpl: Running query with 931f3e6d-cc75-4dbb-94ca-951c5c9026fb:
Query:
select NULL, nspname, NULL from pg_catalog.pg_namespace n where true and nspname not in ('pg_catalog', 'information_schema', 'pg_toast', 'pg_temp_1') order by nspname
Analyzed Plan:
Sort [nspname#2399 ASC NULLS FIRST], true
+- Project [null AS NULL#2400, nspname#2399, null AS NULL#2401]
   +- Filter (true && NOT nspname#2399 IN (pg_catalog,information_schema,pg_toast,pg_temp_1))
      +- SubqueryAlias `n`
         +- SubqueryAlias `pg_catalog`.`pg_namespace`
            +- HiveTableRelation `pg_catalog`.`pg_namespace`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [oid#2398, nspname#2399]

       
19/06/29 17:42:02 INFO CodeGenerator: Code generated in 32.367273 ms
19/06/29 17:42:02 INFO CodeGenerator: Code generated in 21.919253 ms
19/06/29 17:42:02 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 281.3 KB, free 365.7 MB)
19/06/29 17:42:02 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 23.6 KB, free 365.7 MB)
19/06/29 17:42:02 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 35758d068699:33063 (size: 23.6 KB, free: 366.2 MB)
19/06/29 17:42:02 INFO SparkContext: Created broadcast 49 from 
19/06/29 17:42:02 INFO CodeGenerator: Code generated in 20.994655 ms
19/06/29 17:42:02 INFO FileInputFormat: Total input paths to process : 1
19/06/29 17:42:02 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:42:02 INFO DAGScheduler: Got job 49 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:42:02 INFO DAGScheduler: Final stage: ResultStage 49 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:42:02 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:42:02 INFO DAGScheduler: Missing parents: List()
19/06/29 17:42:02 INFO DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[105] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:42:02 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 12.7 KB, free 365.7 MB)
19/06/29 17:42:02 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 6.1 KB, free 365.7 MB)
19/06/29 17:42:02 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 35758d068699:33063 (size: 6.1 KB, free: 366.2 MB)
19/06/29 17:42:02 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1161
19/06/29 17:42:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[105] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:42:02 INFO TaskSchedulerImpl: Adding task set 49.0 with 1 tasks
19/06/29 17:42:02 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 49, localhost, executor driver, partition 0, PROCESS_LOCAL, 7979 bytes)
19/06/29 17:42:02 INFO Executor: Running task 0.0 in stage 49.0 (TID 49)
19/06/29 17:42:02 INFO HadoopRDD: Input split: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_namespace/part-00000-f5a923f9-fba6-47cf-beaf-480ccf41f0bb-c000:0+11
19/06/29 17:42:02 INFO CodeGenerator: Code generated in 9.328698 ms
19/06/29 17:42:02 INFO Executor: Finished task 0.0 in stage 49.0 (TID 49). 1697 bytes result sent to driver
19/06/29 17:42:02 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 49) in 94 ms on localhost (executor driver) (1/1)
19/06/29 17:42:02 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool 
19/06/29 17:42:02 INFO DAGScheduler: ResultStage 49 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.113 s
19/06/29 17:42:02 INFO DAGScheduler: Job 49 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.117151 s
19/06/29 17:42:02 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:42:02 INFO DAGScheduler: Registering RDD 106 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:42:02 INFO DAGScheduler: Got job 50 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:42:02 INFO DAGScheduler: Final stage: ResultStage 51 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:42:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 50)
19/06/29 17:42:02 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 50)
19/06/29 17:42:02 INFO DAGScheduler: Submitting ShuffleMapStage 50 (MapPartitionsRDD[106] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:42:02 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 14.7 KB, free 365.7 MB)
19/06/29 17:42:02 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 7.2 KB, free 365.7 MB)
19/06/29 17:42:02 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 35758d068699:33063 (size: 7.2 KB, free: 366.2 MB)
19/06/29 17:42:02 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1161
19/06/29 17:42:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 50 (MapPartitionsRDD[106] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:42:02 INFO TaskSchedulerImpl: Adding task set 50.0 with 1 tasks
19/06/29 17:42:02 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 50, localhost, executor driver, partition 0, PROCESS_LOCAL, 7968 bytes)
19/06/29 17:42:02 INFO Executor: Running task 0.0 in stage 50.0 (TID 50)
19/06/29 17:42:02 INFO HadoopRDD: Input split: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_namespace/part-00000-f5a923f9-fba6-47cf-beaf-480ccf41f0bb-c000:0+11
19/06/29 17:42:02 INFO Executor: Finished task 0.0 in stage 50.0 (TID 50). 1535 bytes result sent to driver
19/06/29 17:42:02 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 50) in 72 ms on localhost (executor driver) (1/1)
19/06/29 17:42:02 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool 
19/06/29 17:42:02 INFO DAGScheduler: ShuffleMapStage 50 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.092 s
19/06/29 17:42:02 INFO DAGScheduler: looking for newly runnable stages
19/06/29 17:42:02 INFO DAGScheduler: running: Set()
19/06/29 17:42:02 INFO DAGScheduler: waiting: Set(ResultStage 51)
19/06/29 17:42:02 INFO DAGScheduler: failed: Set()
19/06/29 17:42:02 INFO DAGScheduler: Submitting ResultStage 51 (MapPartitionsRDD[109] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:42:02 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 15.1 KB, free 365.7 MB)
19/06/29 17:42:02 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 7.9 KB, free 365.7 MB)
19/06/29 17:42:02 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 35758d068699:33063 (size: 7.9 KB, free: 366.2 MB)
19/06/29 17:42:02 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1161
19/06/29 17:42:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[109] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:42:02 INFO TaskSchedulerImpl: Adding task set 51.0 with 1 tasks
19/06/29 17:42:02 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 51, localhost, executor driver, partition 0, ANY, 7767 bytes)
19/06/29 17:42:02 INFO Executor: Running task 0.0 in stage 51.0 (TID 51)
19/06/29 17:42:02 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
19/06/29 17:42:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
19/06/29 17:42:02 INFO CodeGenerator: Code generated in 13.313621 ms
19/06/29 17:42:02 INFO Executor: Finished task 0.0 in stage 51.0 (TID 51). 2165 bytes result sent to driver
19/06/29 17:42:02 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 51) in 108 ms on localhost (executor driver) (1/1)
19/06/29 17:42:02 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool 
19/06/29 17:42:02 INFO DAGScheduler: ResultStage 51 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.120 s
19/06/29 17:42:02 INFO DAGScheduler: Job 50 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.240916 s
19/06/29 17:42:02 INFO PgWireProtocol: Query: statements=SET search_path TO spark,public
19/06/29 17:42:02 INFO OperationImpl: Running query with c2b384c0-bff2-4740-9587-f649d786b7a5:
Query:
SET search_path TO spark,public
Analyzed Plan:
SetCommand (search_path TO spark,public,None)

       
19/06/29 17:42:02 INFO PgWireProtocol: Query: statements=select relname, nspname, relkind from pg_catalog.pg_class c, pg_catalog.pg_namespace n where relkind in ('r', 'v') and nspname like 'spark' and relname like '%' and nspname not in ('pg_catalog', 'information_schema', 'pg_toast', 'pg_temp_1') and n.oid = relnamespace order by nspname, relname
19/06/29 17:42:03 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:42:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:42:03 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:42:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:42:03 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:42:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:42:03 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:42:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:42:03 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_namespace
19/06/29 17:42:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_namespace	
19/06/29 17:42:03 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_namespace
19/06/29 17:42:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_namespace	
19/06/29 17:42:03 INFO OperationImpl: Running query with 41c27f4a-3989-4920-af93-21047555e756:
Query:
select relname, nspname, relkind from pg_catalog.pg_class c, pg_catalog.pg_namespace n where relkind in ('r', 'v') and nspname like 'spark' and relname like '%' and nspname not in ('pg_catalog', 'information_schema', 'pg_toast', 'pg_temp_1') and n.oid = relnamespace order by nspname, relname
Analyzed Plan:
Sort [nspname#2431 ASC NULLS FIRST, relname#2411 ASC NULLS FIRST], true
+- Project [relname#2411, nspname#2431, relkind#2414]
   +- Filter (((relkind#2414 IN (r,v) && nspname#2431 LIKE spark) && relname#2411 LIKE %) && (NOT nspname#2431 IN (pg_catalog,information_schema,pg_toast,pg_temp_1) && (oid#2430 = relnamespace#2415)))
      +- Join Inner
         :- SubqueryAlias `c`
         :  +- SubqueryAlias `pg_catalog`.`pg_class`
         :     +- HiveTableRelation `pg_catalog`.`pg_class`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [oid#2409, reltablespace#2410, relname#2411, reloftype#2412, relpersistence#2413, relkind#2414, relnamespace#2415, relowner#2416, relacl#2417, relchecks#2418, reltoastrelid#2419, relhasindex#2420, relhasrules#2421, relhastriggers#2422, relrowsecurity#2423, relforcerowsecurity#2424, relreplident#2425, reltriggers#2426, relhasoids#2427, relispartition#2428, relpartbound#2429]
         +- SubqueryAlias `n`
            +- SubqueryAlias `pg_catalog`.`pg_namespace`
               +- HiveTableRelation `pg_catalog`.`pg_namespace`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [oid#2430, nspname#2431]

       
19/06/29 17:42:03 INFO CodeGenerator: Code generated in 25.808746 ms
19/06/29 17:42:03 INFO CodeGenerator: Code generated in 12.031218 ms
19/06/29 17:42:03 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 281.3 KB, free 365.4 MB)
19/06/29 17:42:03 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 23.6 KB, free 365.4 MB)
19/06/29 17:42:03 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 35758d068699:33063 (size: 23.6 KB, free: 366.2 MB)
19/06/29 17:42:03 INFO SparkContext: Created broadcast 53 from 
19/06/29 17:42:03 INFO FileInputFormat: Total input paths to process : 1
19/06/29 17:42:03 INFO SparkContext: Starting job: run at ThreadPoolExecutor.java:1142
19/06/29 17:42:03 INFO DAGScheduler: Got job 51 (run at ThreadPoolExecutor.java:1142) with 1 output partitions
19/06/29 17:42:03 INFO DAGScheduler: Final stage: ResultStage 52 (run at ThreadPoolExecutor.java:1142)
19/06/29 17:42:03 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:42:03 INFO DAGScheduler: Missing parents: List()
19/06/29 17:42:03 INFO DAGScheduler: Submitting ResultStage 52 (MapPartitionsRDD[115] at run at ThreadPoolExecutor.java:1142), which has no missing parents
19/06/29 17:42:03 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 12.4 KB, free 365.3 MB)
19/06/29 17:42:03 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 6.0 KB, free 365.3 MB)
19/06/29 17:42:03 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 35758d068699:33063 (size: 6.0 KB, free: 366.2 MB)
19/06/29 17:42:03 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1161
19/06/29 17:42:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[115] at run at ThreadPoolExecutor.java:1142) (first 15 tasks are for partitions Vector(0))
19/06/29 17:42:03 INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks
19/06/29 17:42:03 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 52, localhost, executor driver, partition 0, PROCESS_LOCAL, 7979 bytes)
19/06/29 17:42:03 INFO Executor: Running task 0.0 in stage 52.0 (TID 52)
19/06/29 17:42:03 INFO HadoopRDD: Input split: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_namespace/part-00000-f5a923f9-fba6-47cf-beaf-480ccf41f0bb-c000:0+11
19/06/29 17:42:03 INFO CodeGenerator: Code generated in 13.738172 ms
19/06/29 17:42:03 INFO Executor: Finished task 0.0 in stage 52.0 (TID 52). 1452 bytes result sent to driver
19/06/29 17:42:03 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 52) in 37 ms on localhost (executor driver) (1/1)
19/06/29 17:42:03 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool 
19/06/29 17:42:03 INFO DAGScheduler: ResultStage 52 (run at ThreadPoolExecutor.java:1142) finished in 0.046 s
19/06/29 17:42:03 INFO DAGScheduler: Job 51 finished: run at ThreadPoolExecutor.java:1142, took 0.049442 s
19/06/29 17:42:03 INFO CodeGenerator: Code generated in 11.337026 ms
19/06/29 17:42:03 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 1024.0 KB, free 364.3 MB)
19/06/29 17:42:03 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 212.0 B, free 364.3 MB)
19/06/29 17:42:03 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 35758d068699:33063 (size: 212.0 B, free: 366.2 MB)
19/06/29 17:42:03 INFO SparkContext: Created broadcast 55 from run at ThreadPoolExecutor.java:1142
19/06/29 17:42:03 INFO CodeGenerator: Code generated in 30.555825 ms
19/06/29 17:42:03 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 282.1 KB, free 364.1 MB)
19/06/29 17:42:03 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 23.9 KB, free 364.0 MB)
19/06/29 17:42:03 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 35758d068699:33063 (size: 23.9 KB, free: 366.1 MB)
19/06/29 17:42:03 INFO SparkContext: Created broadcast 56 from 
19/06/29 17:42:03 INFO CodeGenerator: Code generated in 12.409536 ms
19/06/29 17:42:03 INFO FileInputFormat: Total input paths to process : 0
19/06/29 17:42:03 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:42:03 INFO DAGScheduler: Job 52 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.000048 s
19/06/29 17:42:03 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:42:03 INFO DAGScheduler: Registering RDD 124 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:42:03 INFO DAGScheduler: Got job 53 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:42:03 INFO DAGScheduler: Final stage: ResultStage 54 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:42:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 53)
19/06/29 17:42:03 INFO DAGScheduler: Missing parents: List()
19/06/29 17:42:03 INFO DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[127] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:42:03 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 21.3 KB, free 364.0 MB)
19/06/29 17:42:03 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 10.5 KB, free 364.0 MB)
19/06/29 17:42:03 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 35758d068699:33063 (size: 10.5 KB, free: 366.1 MB)
19/06/29 17:42:03 INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1161
19/06/29 17:42:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 54 (MapPartitionsRDD[127] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:42:03 INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks
19/06/29 17:42:03 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 53, localhost, executor driver, partition 0, PROCESS_LOCAL, 7767 bytes)
19/06/29 17:42:03 INFO Executor: Running task 0.0 in stage 54.0 (TID 53)
19/06/29 17:42:03 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
19/06/29 17:42:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
19/06/29 17:42:03 INFO Executor: Finished task 0.0 in stage 54.0 (TID 53). 2733 bytes result sent to driver
19/06/29 17:42:03 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 53) in 16 ms on localhost (executor driver) (1/1)
19/06/29 17:42:03 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool 
19/06/29 17:42:03 INFO DAGScheduler: ResultStage 54 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.025 s
19/06/29 17:42:03 INFO DAGScheduler: Job 53 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.028685 s
19/06/29 17:42:03 INFO PgWireProtocol: Query: statements=SET statement_timeout TO 120000
19/06/29 17:42:03 INFO OperationImpl: Running query with 3e70555f-69ad-4434-aa0a-f6ef33e0adde:
Query:
SET statement_timeout TO 120000
Analyzed Plan:
SetCommand (statement_timeout TO 120000,None)

       
19/06/29 17:42:20 ERROR SQLServer: RECEIVED SIGNAL TERM
19/06/29 17:42:20 WARN SessionManager: SessionManager stopped though, 1 opened sessions still existed
19/06/29 17:42:20 INFO SparkContext: Invoking stop() from shutdown hook
19/06/29 17:42:20 INFO SparkContext: SparkContext already stopped.
19/06/29 17:42:20 INFO SparkUI: Stopped Spark web UI at http://35758d068699:4040
19/06/29 17:42:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/06/29 17:42:20 INFO MemoryStore: MemoryStore cleared
19/06/29 17:42:20 INFO BlockManager: BlockManager stopped
19/06/29 17:42:20 INFO BlockManagerMaster: BlockManagerMaster stopped
19/06/29 17:42:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/06/29 17:42:20 INFO SparkContext: Successfully stopped SparkContext
19/06/29 17:42:20 INFO ShutdownHookManager: Shutdown hook called
19/06/29 17:42:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-c00cfa47-5932-4f10-8386-408ee7ef0bb0
19/06/29 17:42:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-64e949b5-3d90-4892-a29e-1de73ecd0be4
