Spark Command: /opt/jdk1.8.0_112/bin/java -cp ../thirdparty/spark-2.4.3-bin-hadoop2.7/conf/:/opt/fusiondb/sbin/../thirdparty/spark-2.4.3-bin-hadoop2.7/jars/* -Xmx1g org.apache.spark.deploy.SparkSubmit --conf spark.sql.server.port=54322 --conf spark.sql.server.psql.enabled=true --conf spark.sql.server.binaryTransferMode=false --properties-file /opt/fusiondb/sbin/../conf/spark-defaults.conf --class org.apache.spark.sql.fdb.SQLServer --name FusionDB SQL Server /opt/fusiondb/sbin/../assembly/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar
========================================
19/06/29 17:50:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
19/06/29 17:50:04 INFO SQLServer: Started daemon with process name: 2809@35758d068699
19/06/29 17:50:04 INFO SignalUtils: Registered signal handler for TERM
19/06/29 17:50:04 INFO SignalUtils: Registered signal handler for HUP
19/06/29 17:50:04 INFO SignalUtils: Registered signal handler for INT
19/06/29 17:50:04 INFO SQLServer: Spark properties passed to the SQL server:
  key=spark.sql.crossJoin.enabled value=true
  key=spark.sql.server.port value=54322
  key=spark.app.name value=FusionDB SQL Server
  key=spark.sql.server.binaryTransferMode value=false
  key=spark.master value=local[*]
  key=spark.submit.deployMode value=client
  key=spark.sql.server.psql.enabled value=true
  key=spark.jars value=file:/opt/fusiondb/sbin/../assembly/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar
       
19/06/29 17:50:04 INFO RecurringTimer: Started timer for Idle Session Cleaner at time 1561830900000
19/06/29 17:50:04 INFO SparkContext: Running Spark version 2.4.3
19/06/29 17:50:04 INFO SparkContext: Submitted application: FusionDB SQL Server
19/06/29 17:50:05 INFO SecurityManager: Changing view acls to: root
19/06/29 17:50:05 INFO SecurityManager: Changing modify acls to: root
19/06/29 17:50:05 INFO SecurityManager: Changing view acls groups to: 
19/06/29 17:50:05 INFO SecurityManager: Changing modify acls groups to: 
19/06/29 17:50:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
19/06/29 17:50:05 INFO Utils: Successfully started service 'sparkDriver' on port 32823.
19/06/29 17:50:05 INFO SparkEnv: Registering MapOutputTracker
19/06/29 17:50:05 INFO SparkEnv: Registering BlockManagerMaster
19/06/29 17:50:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/06/29 17:50:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/06/29 17:50:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6a00b48c-3906-44a3-8a1d-19e630987a18
19/06/29 17:50:05 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
19/06/29 17:50:05 INFO SparkEnv: Registering OutputCommitCoordinator
19/06/29 17:50:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/06/29 17:50:05 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://35758d068699:4040
19/06/29 17:50:05 INFO SparkContext: Added JAR file:/opt/fusiondb/sbin/../assembly/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar at spark://35758d068699:32823/jars/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar with timestamp 1561830605772
19/06/29 17:50:05 INFO Executor: Starting executor ID driver on host localhost
19/06/29 17:50:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41799.
19/06/29 17:50:05 INFO NettyBlockTransferService: Server created on 35758d068699:41799
19/06/29 17:50:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/06/29 17:50:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 35758d068699, 41799, None)
19/06/29 17:50:05 INFO BlockManagerMasterEndpoint: Registering block manager 35758d068699:41799 with 366.3 MB RAM, BlockManagerId(driver, 35758d068699, 41799, None)
19/06/29 17:50:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 35758d068699, 41799, None)
19/06/29 17:50:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 35758d068699, 41799, None)
19/06/29 17:50:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/fusiondb/sbin/spark-warehouse/').
19/06/29 17:50:06 INFO SharedState: Warehouse path is 'file:/opt/fusiondb/sbin/spark-warehouse/'.
19/06/29 17:50:06 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
19/06/29 17:50:07 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19/06/29 17:50:08 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19/06/29 17:50:08 INFO ObjectStore: ObjectStore, initialize called
19/06/29 17:50:08 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
19/06/29 17:50:08 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
19/06/29 17:50:10 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19/06/29 17:50:13 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:50:13 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:50:13 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:50:13 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:50:13 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19/06/29 17:50:13 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/06/29 17:50:13 INFO ObjectStore: Initialized ObjectStore
19/06/29 17:50:13 INFO HiveMetaStore: Added admin role in metastore
19/06/29 17:50:13 INFO HiveMetaStore: Added public role in metastore
19/06/29 17:50:13 INFO HiveMetaStore: No user is added in admin role, since config is empty
19/06/29 17:50:13 INFO HiveMetaStore: 0: get_all_databases
19/06/29 17:50:13 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
19/06/29 17:50:13 INFO HiveMetaStore: 0: get_functions: db=default pat=*
19/06/29 17:50:13 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19/06/29 17:50:13 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:50:13 INFO HiveMetaStore: 0: get_functions: db=pg_catalog pat=*
19/06/29 17:50:13 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=pg_catalog pat=*	
19/06/29 17:50:14 INFO SessionState: Created local directory: /tmp/f5e8e3f1-5018-4804-a3c9-9a16635101e4_resources
19/06/29 17:50:14 INFO SessionState: Created HDFS directory: /tmp/hive/root/f5e8e3f1-5018-4804-a3c9-9a16635101e4
19/06/29 17:50:14 INFO SessionState: Created local directory: /tmp/root/f5e8e3f1-5018-4804-a3c9-9a16635101e4
19/06/29 17:50:14 INFO SessionState: Created HDFS directory: /tmp/hive/root/f5e8e3f1-5018-4804-a3c9-9a16635101e4/_tmp_space.db
19/06/29 17:50:14 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/opt/fusiondb/sbin/spark-warehouse/
19/06/29 17:50:14 INFO HiveMetaStore: 0: get_database: default
19/06/29 17:50:14 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/06/29 17:50:14 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:50:14 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:50:14 INFO LoggingHandler: [id: 0xbe203b54] REGISTERED
19/06/29 17:50:14 INFO LoggingHandler: [id: 0xbe203b54] BIND: 0.0.0.0/0.0.0.0:54322
19/06/29 17:50:14 INFO LoggingHandler: [id: 0xbe203b54, L:/0.0.0.0:54322] ACTIVE
19/06/29 17:50:14 INFO PgProtocolService: Start running the SQL server (port=54322, workerThreads=4)
19/06/29 17:55:46 INFO LoggingHandler: [id: 0xbe203b54, L:/0.0.0.0:54322] READ: [id: 0x1da3d7bb, L:/172.17.0.2:54322 - R:/172.17.0.1:58544]
19/06/29 17:55:46 INFO LoggingHandler: [id: 0xbe203b54, L:/0.0.0.0:54322] READ COMPLETE
19/06/29 17:55:48 INFO HiveMetaStore: 1: get_database: global_temp
19/06/29 17:55:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
19/06/29 17:55:48 INFO HiveMetaStore: 1: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19/06/29 17:55:48 INFO ObjectStore: ObjectStore, initialize called
19/06/29 17:55:48 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19/06/29 17:55:48 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/06/29 17:55:48 INFO ObjectStore: Initialized ObjectStore
19/06/29 17:55:48 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
19/06/29 17:55:48 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:49 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:49 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:49 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:49 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:49 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:49 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:49 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:49 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:49 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:49 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:49 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:49 INFO HiveMetaStore: 1: drop_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=drop_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:49 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:55:49 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:55:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:55:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:55:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:55:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:55:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:55:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:55:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:55:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:55:50 INFO hivemetastoressimpl: deleting  file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database
19/06/29 17:55:50 INFO deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
19/06/29 17:55:50 INFO TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
19/06/29 17:55:50 INFO hivemetastoressimpl: Deleted the diretory file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database
19/06/29 17:55:50 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:50 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:50 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:50 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:50 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:50 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:50 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:51 INFO HiveMetaStore: 1: create_table: Table(tableName:pg_database, dbName:pg_catalog, owner:root, createTime:1561830950, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:datname, type:string, comment:null), FieldSchema(name:datdba, type:int, comment:null), FieldSchema(name:encoding, type:int, comment:null), FieldSchema(name:datcollate, type:string, comment:null), FieldSchema(name:datctype, type:string, comment:null), FieldSchema(name:datacl, type:array<string>, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"datname","type":"string","nullable":true,"metadata":{}},{"name":"datdba","type":"integer","nullable":true,"metadata":{}},{"name":"encoding","type":"integer","nullable":true,"metadata":{}},{"name":"datcollate","type":"string","nullable":true,"metadata":{}},{"name":"datctype","type":"string","nullable":true,"metadata":{}},{"name":"datacl","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:55:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_database, dbName:pg_catalog, owner:root, createTime:1561830950, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:datname, type:string, comment:null), FieldSchema(name:datdba, type:int, comment:null), FieldSchema(name:encoding, type:int, comment:null), FieldSchema(name:datcollate, type:string, comment:null), FieldSchema(name:datctype, type:string, comment:null), FieldSchema(name:datacl, type:array<string>, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"datname","type":"string","nullable":true,"metadata":{}},{"name":"datdba","type":"integer","nullable":true,"metadata":{}},{"name":"encoding","type":"integer","nullable":true,"metadata":{}},{"name":"datcollate","type":"string","nullable":true,"metadata":{}},{"name":"datctype","type":"string","nullable":true,"metadata":{}},{"name":"datacl","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:55:51 WARN HiveMetaStore: Location: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database specified for non-external table:pg_database
19/06/29 17:55:51 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database
19/06/29 17:55:51 INFO HiveMetaStore: 1: get_databases: *
19/06/29 17:55:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_databases: *	
19/06/29 17:55:51 INFO PgMetadata: Registering a database `default` in a system catalog `pg_database`
19/06/29 17:55:51 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:51 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:51 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/.hive-staging_hive_2019-06-29_17-55-51_714_7264829829580898997-1
19/06/29 17:55:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:52 INFO CodeGenerator: Code generated in 361.342013 ms
19/06/29 17:55:52 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:55:52 INFO DAGScheduler: Got job 0 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:55:52 INFO DAGScheduler: Final stage: ResultStage 0 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:55:52 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:55:52 INFO DAGScheduler: Missing parents: List()
19/06/29 17:55:52 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:55:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 214.9 KB, free 366.1 MB)
19/06/29 17:55:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 75.6 KB, free 366.0 MB)
19/06/29 17:55:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 35758d068699:41799 (size: 75.6 KB, free: 366.2 MB)
19/06/29 17:55:53 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
19/06/29 17:55:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:55:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
19/06/29 17:55:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8092 bytes)
19/06/29 17:55:53 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
19/06/29 17:55:53 INFO Executor: Fetching spark://35758d068699:32823/jars/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar with timestamp 1561830605772
19/06/29 17:55:53 INFO TransportClientFactory: Successfully created connection to 35758d068699/172.17.0.2:32823 after 40 ms (0 ms spent in bootstraps)
19/06/29 17:55:53 INFO Utils: Fetching spark://35758d068699:32823/jars/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar to /tmp/spark-2f29e61f-233b-422e-a591-5e9a3f62c2b0/userFiles-d0507e06-4db1-4c6d-b8ee-1839cd8f9058/fetchFileTemp3576177945653361838.tmp
19/06/29 17:55:53 INFO Executor: Adding file:/tmp/spark-2f29e61f-233b-422e-a591-5e9a3f62c2b0/userFiles-d0507e06-4db1-4c6d-b8ee-1839cd8f9058/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar to class loader
19/06/29 17:55:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:54 INFO FileOutputCommitter: Saved output of task 'attempt_20190629175552_0000_m_000000_0' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/.hive-staging_hive_2019-06-29_17-55-51_714_7264829829580898997-1/-ext-10000/_temporary/0/task_20190629175552_0000_m_000000
19/06/29 17:55:54 INFO SparkHadoopMapRedUtil: attempt_20190629175552_0000_m_000000_0: Committed
19/06/29 17:55:54 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2022 bytes result sent to driver
19/06/29 17:55:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 915 ms on localhost (executor driver) (1/1)
19/06/29 17:55:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
19/06/29 17:55:54 INFO DAGScheduler: ResultStage 0 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 1.163 s
19/06/29 17:55:54 INFO DAGScheduler: Job 0 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 1.257288 s
19/06/29 17:55:54 INFO FileFormatWriter: Write Job 898547e0-43a6-4c4a-85a0-74346957b7e7 committed.
19/06/29 17:55:54 INFO FileFormatWriter: Finished processing stats for write job 898547e0-43a6-4c4a-85a0-74346957b7e7.
19/06/29 17:55:54 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:54 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:54 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:55:54 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/.hive-staging_hive_2019-06-29_17-55-51_714_7264829829580898997-1/-ext-10000/part-00000-7600e8eb-dd00-4d74-8346-518d505a5e23-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/part-00000-7600e8eb-dd00-4d74-8346-518d505a5e23-c000, Status:true
19/06/29 17:55:54 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_database newtbl=pg_database
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_database newtbl=pg_database	
19/06/29 17:55:54 INFO log: Updating table stats fast for pg_database
19/06/29 17:55:54 INFO log: Updated size of table pg_database to 17
19/06/29 17:55:54 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:54 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:54 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:54 INFO PgMetadata: Registering a database `pg_catalog` in a system catalog `pg_database`
19/06/29 17:55:54 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:54 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/.hive-staging_hive_2019-06-29_17-55-54_513_5799185754789436586-1
19/06/29 17:55:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:54 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:55:54 INFO DAGScheduler: Got job 1 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:55:54 INFO DAGScheduler: Final stage: ResultStage 1 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:55:54 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:55:54 INFO DAGScheduler: Missing parents: List()
19/06/29 17:55:54 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:55:54 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 214.9 KB, free 365.8 MB)
19/06/29 17:55:54 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 75.6 KB, free 365.7 MB)
19/06/29 17:55:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 35758d068699:41799 (size: 75.6 KB, free: 366.2 MB)
19/06/29 17:55:54 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
19/06/29 17:55:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:55:54 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
19/06/29 17:55:54 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 8100 bytes)
19/06/29 17:55:54 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
19/06/29 17:55:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:54 INFO FileOutputCommitter: Saved output of task 'attempt_20190629175554_0001_m_000000_1' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/.hive-staging_hive_2019-06-29_17-55-54_513_5799185754789436586-1/-ext-10000/_temporary/0/task_20190629175554_0001_m_000000
19/06/29 17:55:54 INFO SparkHadoopMapRedUtil: attempt_20190629175554_0001_m_000000_1: Committed
19/06/29 17:55:54 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1979 bytes result sent to driver
19/06/29 17:55:54 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 40 ms on localhost (executor driver) (1/1)
19/06/29 17:55:54 INFO DAGScheduler: ResultStage 1 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.086 s
19/06/29 17:55:54 INFO DAGScheduler: Job 1 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.090049 s
19/06/29 17:55:54 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
19/06/29 17:55:54 INFO FileFormatWriter: Write Job 31d597a6-f60d-44f6-a3b1-0d56ee87db5b committed.
19/06/29 17:55:54 INFO FileFormatWriter: Finished processing stats for write job 31d597a6-f60d-44f6-a3b1-0d56ee87db5b.
19/06/29 17:55:54 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:54 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:54 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:55:54 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/.hive-staging_hive_2019-06-29_17-55-54_513_5799185754789436586-1/-ext-10000/part-00000-66d9bda8-dfd9-45d4-98c9-7ee532ae3a4b-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/part-00000-66d9bda8-dfd9-45d4-98c9-7ee532ae3a4b-c000, Status:true
19/06/29 17:55:54 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_database newtbl=pg_database
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_database newtbl=pg_database	
19/06/29 17:55:54 INFO log: Updating table stats fast for pg_database
19/06/29 17:55:54 INFO log: Updated size of table pg_database to 37
19/06/29 17:55:54 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:54 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:54 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:54 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:54 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:54 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:54 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:55:54 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_database newtbl=pg_database
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_database newtbl=pg_database	
19/06/29 17:55:54 INFO log: Updating table stats fast for pg_database
19/06/29 17:55:54 INFO log: Updated size of table pg_database to 37
19/06/29 17:55:54 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:55:54 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:54 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:55:54 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:55:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:55:55 INFO HiveMetaStore: 1: drop_table : db=pg_catalog tbl=pg_class
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=drop_table : db=pg_catalog tbl=pg_class	
19/06/29 17:55:55 INFO hivemetastoressimpl: deleting  file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_class
19/06/29 17:55:55 INFO deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
19/06/29 17:55:55 INFO TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
19/06/29 17:55:55 INFO hivemetastoressimpl: Deleted the diretory file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_class
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: create_table: Table(tableName:pg_class, dbName:pg_catalog, owner:root, createTime:1561830955, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:reltablespace, type:int, comment:null), FieldSchema(name:relname, type:string, comment:null), FieldSchema(name:reloftype, type:int, comment:null), FieldSchema(name:relpersistence, type:string, comment:null), FieldSchema(name:relkind, type:string, comment:null), FieldSchema(name:relnamespace, type:int, comment:null), FieldSchema(name:relowner, type:int, comment:null), FieldSchema(name:relacl, type:array<string>, comment:null), FieldSchema(name:relchecks, type:smallint, comment:null), FieldSchema(name:reltoastrelid, type:int, comment:null), FieldSchema(name:relhasindex, type:boolean, comment:null), FieldSchema(name:relhasrules, type:boolean, comment:null), FieldSchema(name:relhastriggers, type:boolean, comment:null), FieldSchema(name:relrowsecurity, type:boolean, comment:null), FieldSchema(name:relforcerowsecurity, type:boolean, comment:null), FieldSchema(name:relreplident, type:string, comment:null), FieldSchema(name:reltriggers, type:smallint, comment:null), FieldSchema(name:relhasoids, type:boolean, comment:null), FieldSchema(name:relispartition, type:boolean, comment:null), FieldSchema(name:relpartbound, type:string, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_class, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"reltablespace","type":"integer","nullable":true,"metadata":{}},{"name":"relname","type":"string","nullable":true,"metadata":{}},{"name":"reloftype","type":"integer","nullable":true,"metadata":{}},{"name":"relpersistence","type":"string","nullable":true,"metadata":{}},{"name":"relkind","type":"string","nullable":true,"metadata":{}},{"name":"relnamespace","type":"integer","nullable":true,"metadata":{}},{"name":"relowner","type":"integer","nullable":true,"metadata":{}},{"name":"relacl","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}},{"name":"relchecks","type":"short","nullable":true,"metadata":{}},{"name":"reltoastrelid","type":"integer","nullable":true,"metadata":{}},{"name":"relhasindex","type":"boolean","nullable":true,"metadata":{}},{"name":"relhasrules","type":"boolean","nullable":true,"metadata":{}},{"name":"relhastriggers","type":"boolean","nullable":true,"metadata":{}},{"name":"relrowsecurity","type":"boolean","nullable":true,"metadata":{}},{"name":"relforcerowsecurity","type":"boolean","nullable":true,"metadata":{}},{"name":"relreplident","type":"string","nullable":true,"metadata":{}},{"name":"reltriggers","type":"short","nullable":true,"metadata":{}},{"name":"relhasoids","type":"boolean","nullable":true,"metadata":{}},{"name":"relispartition","type":"boolean","nullable":true,"metadata":{}},{"name":"relpartbound","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_class, dbName:pg_catalog, owner:root, createTime:1561830955, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:reltablespace, type:int, comment:null), FieldSchema(name:relname, type:string, comment:null), FieldSchema(name:reloftype, type:int, comment:null), FieldSchema(name:relpersistence, type:string, comment:null), FieldSchema(name:relkind, type:string, comment:null), FieldSchema(name:relnamespace, type:int, comment:null), FieldSchema(name:relowner, type:int, comment:null), FieldSchema(name:relacl, type:array<string>, comment:null), FieldSchema(name:relchecks, type:smallint, comment:null), FieldSchema(name:reltoastrelid, type:int, comment:null), FieldSchema(name:relhasindex, type:boolean, comment:null), FieldSchema(name:relhasrules, type:boolean, comment:null), FieldSchema(name:relhastriggers, type:boolean, comment:null), FieldSchema(name:relrowsecurity, type:boolean, comment:null), FieldSchema(name:relforcerowsecurity, type:boolean, comment:null), FieldSchema(name:relreplident, type:string, comment:null), FieldSchema(name:reltriggers, type:smallint, comment:null), FieldSchema(name:relhasoids, type:boolean, comment:null), FieldSchema(name:relispartition, type:boolean, comment:null), FieldSchema(name:relpartbound, type:string, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_class, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"reltablespace","type":"integer","nullable":true,"metadata":{}},{"name":"relname","type":"string","nullable":true,"metadata":{}},{"name":"reloftype","type":"integer","nullable":true,"metadata":{}},{"name":"relpersistence","type":"string","nullable":true,"metadata":{}},{"name":"relkind","type":"string","nullable":true,"metadata":{}},{"name":"relnamespace","type":"integer","nullable":true,"metadata":{}},{"name":"relowner","type":"integer","nullable":true,"metadata":{}},{"name":"relacl","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}},{"name":"relchecks","type":"short","nullable":true,"metadata":{}},{"name":"reltoastrelid","type":"integer","nullable":true,"metadata":{}},{"name":"relhasindex","type":"boolean","nullable":true,"metadata":{}},{"name":"relhasrules","type":"boolean","nullable":true,"metadata":{}},{"name":"relhastriggers","type":"boolean","nullable":true,"metadata":{}},{"name":"relrowsecurity","type":"boolean","nullable":true,"metadata":{}},{"name":"relforcerowsecurity","type":"boolean","nullable":true,"metadata":{}},{"name":"relreplident","type":"string","nullable":true,"metadata":{}},{"name":"reltriggers","type":"short","nullable":true,"metadata":{}},{"name":"relhasoids","type":"boolean","nullable":true,"metadata":{}},{"name":"relispartition","type":"boolean","nullable":true,"metadata":{}},{"name":"relpartbound","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:55:55 WARN HiveMetaStore: Location: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_class specified for non-external table:pg_class
19/06/29 17:55:55 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_class
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:55:55 INFO HiveMetaStore: 1: drop_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=drop_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 27
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 31
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 21
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 13
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 24
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 46
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 29
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 41
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 55
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 36
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 37
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 7
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 17
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 8
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 23
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 34
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 38
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 43
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 19
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 5
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 39
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 57
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 18
19/06/29 17:55:55 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 35758d068699:41799 in memory (size: 75.6 KB, free: 366.2 MB)
19/06/29 17:55:55 INFO hivemetastoressimpl: deleting  file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_attribute
19/06/29 17:55:55 INFO TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
19/06/29 17:55:55 INFO hivemetastoressimpl: Deleted the diretory file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_attribute
19/06/29 17:55:55 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 35758d068699:41799 in memory (size: 75.6 KB, free: 366.3 MB)
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 50
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 45
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 42
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 44
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 47
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 30
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 49
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 59
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 12
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 28
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 20
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 56
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 6
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 26
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 25
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 40
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 51
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 58
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 16
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 52
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 9
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 14
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 11
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 33
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 48
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 35
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 15
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 53
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 10
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 22
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 32
19/06/29 17:55:55 INFO ContextCleaner: Cleaned accumulator 54
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: create_table: Table(tableName:pg_attribute, dbName:pg_catalog, owner:root, createTime:1561830955, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:attrelid, type:int, comment:null), FieldSchema(name:attname, type:string, comment:null), FieldSchema(name:atttypid, type:int, comment:null), FieldSchema(name:attnotnull, type:boolean, comment:null), FieldSchema(name:atthasdef, type:boolean, comment:null), FieldSchema(name:atttypmod, type:int, comment:null), FieldSchema(name:attlen, type:int, comment:null), FieldSchema(name:attnum, type:int, comment:null), FieldSchema(name:attidentity, type:string, comment:null), FieldSchema(name:attisdropped, type:boolean, comment:null), FieldSchema(name:attcollation, type:int, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_attribute, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"attrelid","type":"integer","nullable":true,"metadata":{}},{"name":"attname","type":"string","nullable":true,"metadata":{}},{"name":"atttypid","type":"integer","nullable":true,"metadata":{}},{"name":"attnotnull","type":"boolean","nullable":true,"metadata":{}},{"name":"atthasdef","type":"boolean","nullable":true,"metadata":{}},{"name":"atttypmod","type":"integer","nullable":true,"metadata":{}},{"name":"attlen","type":"integer","nullable":true,"metadata":{}},{"name":"attnum","type":"integer","nullable":true,"metadata":{}},{"name":"attidentity","type":"string","nullable":true,"metadata":{}},{"name":"attisdropped","type":"boolean","nullable":true,"metadata":{}},{"name":"attcollation","type":"integer","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_attribute, dbName:pg_catalog, owner:root, createTime:1561830955, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:attrelid, type:int, comment:null), FieldSchema(name:attname, type:string, comment:null), FieldSchema(name:atttypid, type:int, comment:null), FieldSchema(name:attnotnull, type:boolean, comment:null), FieldSchema(name:atthasdef, type:boolean, comment:null), FieldSchema(name:atttypmod, type:int, comment:null), FieldSchema(name:attlen, type:int, comment:null), FieldSchema(name:attnum, type:int, comment:null), FieldSchema(name:attidentity, type:string, comment:null), FieldSchema(name:attisdropped, type:boolean, comment:null), FieldSchema(name:attcollation, type:int, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_attribute, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"attrelid","type":"integer","nullable":true,"metadata":{}},{"name":"attname","type":"string","nullable":true,"metadata":{}},{"name":"atttypid","type":"integer","nullable":true,"metadata":{}},{"name":"attnotnull","type":"boolean","nullable":true,"metadata":{}},{"name":"atthasdef","type":"boolean","nullable":true,"metadata":{}},{"name":"atttypmod","type":"integer","nullable":true,"metadata":{}},{"name":"attlen","type":"integer","nullable":true,"metadata":{}},{"name":"attnum","type":"integer","nullable":true,"metadata":{}},{"name":"attidentity","type":"string","nullable":true,"metadata":{}},{"name":"attisdropped","type":"boolean","nullable":true,"metadata":{}},{"name":"attcollation","type":"integer","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:55:55 WARN HiveMetaStore: Location: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_attribute specified for non-external table:pg_attribute
19/06/29 17:55:55 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_attribute
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: default
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: default
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_tables: db=default pat=*
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:55 INFO HiveMetaStore: 1: drop_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=drop_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:55 INFO hivemetastoressimpl: deleting  file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc
19/06/29 17:55:55 INFO TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
19/06/29 17:55:55 INFO hivemetastoressimpl: Deleted the diretory file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:55 INFO HiveMetaStore: 1: create_table: Table(tableName:pg_proc, dbName:pg_catalog, owner:root, createTime:1561830955, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:proname, type:string, comment:null), FieldSchema(name:prorettype, type:int, comment:null), FieldSchema(name:proargtypes, type:array<int>, comment:null), FieldSchema(name:pronamespace, type:int, comment:null), FieldSchema(name:proisagg, type:boolean, comment:null), FieldSchema(name:proiswindow, type:boolean, comment:null), FieldSchema(name:proretset, type:boolean, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"proname","type":"string","nullable":true,"metadata":{}},{"name":"prorettype","type":"integer","nullable":true,"metadata":{}},{"name":"proargtypes","type":{"type":"array","elementType":"integer","containsNull":true},"nullable":true,"metadata":{}},{"name":"pronamespace","type":"integer","nullable":true,"metadata":{}},{"name":"proisagg","type":"boolean","nullable":true,"metadata":{}},{"name":"proiswindow","type":"boolean","nullable":true,"metadata":{}},{"name":"proretset","type":"boolean","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_proc, dbName:pg_catalog, owner:root, createTime:1561830955, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:proname, type:string, comment:null), FieldSchema(name:prorettype, type:int, comment:null), FieldSchema(name:proargtypes, type:array<int>, comment:null), FieldSchema(name:pronamespace, type:int, comment:null), FieldSchema(name:proisagg, type:boolean, comment:null), FieldSchema(name:proiswindow, type:boolean, comment:null), FieldSchema(name:proretset, type:boolean, comment:null)], location:file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"proname","type":"string","nullable":true,"metadata":{}},{"name":"prorettype","type":"integer","nullable":true,"metadata":{}},{"name":"proargtypes","type":{"type":"array","elementType":"integer","containsNull":true},"nullable":true,"metadata":{}},{"name":"pronamespace","type":"integer","nullable":true,"metadata":{}},{"name":"proisagg","type":"boolean","nullable":true,"metadata":{}},{"name":"proiswindow","type":"boolean","nullable":true,"metadata":{}},{"name":"proretset","type":"boolean","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:55:55 WARN HiveMetaStore: Location: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc specified for non-external table:pg_proc
19/06/29 17:55:55 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: default
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_database: default
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/06/29 17:55:55 INFO HiveMetaStore: 1: get_functions: db=default pat=*
19/06/29 17:55:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19/06/29 17:55:56 INFO PgMetadata: Registering a function `ANY` in a system catalog `pg_proc`
19/06/29 17:55:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:56 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-56_082_2268812714281578213-1
19/06/29 17:55:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:56 INFO CodeGenerator: Code generated in 20.410397 ms
19/06/29 17:55:56 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:55:56 INFO DAGScheduler: Got job 2 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:55:56 INFO DAGScheduler: Final stage: ResultStage 2 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:55:56 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:55:56 INFO DAGScheduler: Missing parents: List()
19/06/29 17:55:56 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:55:56 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 215.3 KB, free 366.1 MB)
19/06/29 17:55:56 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 76.0 KB, free 366.0 MB)
19/06/29 17:55:56 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 35758d068699:41799 (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:55:56 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
19/06/29 17:55:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:55:56 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
19/06/29 17:55:56 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 8108 bytes)
19/06/29 17:55:56 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
19/06/29 17:55:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:56 INFO FileOutputCommitter: Saved output of task 'attempt_20190629175556_0002_m_000000_2' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-56_082_2268812714281578213-1/-ext-10000/_temporary/0/task_20190629175556_0002_m_000000
19/06/29 17:55:56 INFO SparkHadoopMapRedUtil: attempt_20190629175556_0002_m_000000_2: Committed
19/06/29 17:55:56 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1979 bytes result sent to driver
19/06/29 17:55:56 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 42 ms on localhost (executor driver) (1/1)
19/06/29 17:55:56 INFO DAGScheduler: ResultStage 2 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.086 s
19/06/29 17:55:56 INFO DAGScheduler: Job 2 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.088211 s
19/06/29 17:55:56 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
19/06/29 17:55:56 INFO FileFormatWriter: Write Job 0606b7c0-4424-4854-85e3-19cc5d957a5b committed.
19/06/29 17:55:56 INFO FileFormatWriter: Finished processing stats for write job 0606b7c0-4424-4854-85e3-19cc5d957a5b.
19/06/29 17:55:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:56 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:55:56 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-56_082_2268812714281578213-1/-ext-10000/part-00000-15572e3b-80ea-4f78-b2c8-c8380224ba11-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-15572e3b-80ea-4f78-b2c8-c8380224ba11-c000, Status:true
19/06/29 17:55:56 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:56 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:56 INFO log: Updated size of table pg_proc to 37
19/06/29 17:55:56 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:56 INFO PgMetadata: Registering a function `array_in` in a system catalog `pg_proc`
19/06/29 17:55:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:56 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-56_432_8854487546052120041-1
19/06/29 17:55:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:56 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:55:56 INFO DAGScheduler: Got job 3 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:55:56 INFO DAGScheduler: Final stage: ResultStage 3 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:55:56 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:55:56 INFO DAGScheduler: Missing parents: List()
19/06/29 17:55:56 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[7] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:55:56 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 215.3 KB, free 365.8 MB)
19/06/29 17:55:56 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 76.0 KB, free 365.7 MB)
19/06/29 17:55:56 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 35758d068699:41799 (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:55:56 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
19/06/29 17:55:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:55:56 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
19/06/29 17:55:56 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 8108 bytes)
19/06/29 17:55:56 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
19/06/29 17:55:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:56 INFO FileOutputCommitter: Saved output of task 'attempt_20190629175556_0003_m_000000_3' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-56_432_8854487546052120041-1/-ext-10000/_temporary/0/task_20190629175556_0003_m_000000
19/06/29 17:55:56 INFO SparkHadoopMapRedUtil: attempt_20190629175556_0003_m_000000_3: Committed
19/06/29 17:55:56 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1979 bytes result sent to driver
19/06/29 17:55:56 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 37 ms on localhost (executor driver) (1/1)
19/06/29 17:55:56 INFO DAGScheduler: ResultStage 3 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.072 s
19/06/29 17:55:56 INFO DAGScheduler: Job 3 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.076596 s
19/06/29 17:55:56 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
19/06/29 17:55:56 INFO FileFormatWriter: Write Job 1f78f100-21ae-4209-8408-4c1fd9cb711c committed.
19/06/29 17:55:56 INFO FileFormatWriter: Finished processing stats for write job 1f78f100-21ae-4209-8408-4c1fd9cb711c.
19/06/29 17:55:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:56 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:55:56 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-56_432_8854487546052120041-1/-ext-10000/part-00000-8e8c51ad-9a2c-445b-a0b9-cd78a3e63ec6-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-8e8c51ad-9a2c-445b-a0b9-cd78a3e63ec6-c000, Status:true
19/06/29 17:55:56 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:56 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:56 INFO log: Updated size of table pg_proc to 78
19/06/29 17:55:56 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:56 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:56 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:56 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:56 INFO log: Updated size of table pg_proc to 78
19/06/29 17:55:56 INFO PgMetadata: Registering a function `array_upper` in a system catalog `pg_proc`
19/06/29 17:55:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:56 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-56_806_3706520945201747877-1
19/06/29 17:55:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:56 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:55:56 INFO DAGScheduler: Got job 4 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:55:56 INFO DAGScheduler: Final stage: ResultStage 4 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:55:56 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:55:56 INFO DAGScheduler: Missing parents: List()
19/06/29 17:55:56 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[9] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:55:56 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 215.3 KB, free 365.5 MB)
19/06/29 17:55:56 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 76.0 KB, free 365.4 MB)
19/06/29 17:55:56 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 35758d068699:41799 (size: 76.0 KB, free: 366.1 MB)
19/06/29 17:55:56 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
19/06/29 17:55:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:55:56 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
19/06/29 17:55:56 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:55:56 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
19/06/29 17:55:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:56 INFO FileOutputCommitter: Saved output of task 'attempt_20190629175556_0004_m_000000_4' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-56_806_3706520945201747877-1/-ext-10000/_temporary/0/task_20190629175556_0004_m_000000
19/06/29 17:55:56 INFO SparkHadoopMapRedUtil: attempt_20190629175556_0004_m_000000_4: Committed
19/06/29 17:55:56 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1979 bytes result sent to driver
19/06/29 17:55:56 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 43 ms on localhost (executor driver) (1/1)
19/06/29 17:55:56 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
19/06/29 17:55:56 INFO DAGScheduler: ResultStage 4 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.075 s
19/06/29 17:55:56 INFO DAGScheduler: Job 4 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.077057 s
19/06/29 17:55:56 INFO FileFormatWriter: Write Job 0c52c8e8-3ebd-42f9-9555-b9450a1c23ac committed.
19/06/29 17:55:56 INFO FileFormatWriter: Finished processing stats for write job 0c52c8e8-3ebd-42f9-9555-b9450a1c23ac.
19/06/29 17:55:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:56 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:56 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:55:56 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-56_806_3706520945201747877-1/-ext-10000/part-00000-61863719-3e0f-49e7-97c1-f6d550859ee6-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-61863719-3e0f-49e7-97c1-f6d550859ee6-c000, Status:true
19/06/29 17:55:56 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:56 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:56 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:56 INFO log: Updated size of table pg_proc to 123
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:57 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:57 INFO log: Updated size of table pg_proc to 123
19/06/29 17:55:57 INFO PgMetadata: Registering a function `current_schema` in a system catalog `pg_proc`
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-57_190_3026430228890689257-1
19/06/29 17:55:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:57 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:55:57 INFO DAGScheduler: Got job 5 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:55:57 INFO DAGScheduler: Final stage: ResultStage 5 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:55:57 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:55:57 INFO DAGScheduler: Missing parents: List()
19/06/29 17:55:57 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[11] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:55:57 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 215.3 KB, free 365.2 MB)
19/06/29 17:55:57 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.0 KB, free 365.2 MB)
19/06/29 17:55:57 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 35758d068699:41799 (size: 76.0 KB, free: 366.0 MB)
19/06/29 17:55:57 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
19/06/29 17:55:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:55:57 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
19/06/29 17:55:57 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:55:57 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
19/06/29 17:55:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:57 INFO FileOutputCommitter: Saved output of task 'attempt_20190629175557_0005_m_000000_5' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-57_190_3026430228890689257-1/-ext-10000/_temporary/0/task_20190629175557_0005_m_000000
19/06/29 17:55:57 INFO SparkHadoopMapRedUtil: attempt_20190629175557_0005_m_000000_5: Committed
19/06/29 17:55:57 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1979 bytes result sent to driver
19/06/29 17:55:57 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 34 ms on localhost (executor driver) (1/1)
19/06/29 17:55:57 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
19/06/29 17:55:57 INFO DAGScheduler: ResultStage 5 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.063 s
19/06/29 17:55:57 INFO DAGScheduler: Job 5 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.067609 s
19/06/29 17:55:57 INFO FileFormatWriter: Write Job f8662d7a-a3b9-49b4-ae0e-53e0d39c5d97 committed.
19/06/29 17:55:57 INFO FileFormatWriter: Finished processing stats for write job f8662d7a-a3b9-49b4-ae0e-53e0d39c5d97.
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:55:57 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-57_190_3026430228890689257-1/-ext-10000/part-00000-af341549-e3fd-4f60-913c-b6974cedebe8-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-af341549-e3fd-4f60-913c-b6974cedebe8-c000, Status:true
19/06/29 17:55:57 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:57 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:57 INFO log: Updated size of table pg_proc to 171
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:57 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:57 INFO log: Updated size of table pg_proc to 171
19/06/29 17:55:57 INFO PgMetadata: Registering a function `current_schemas` in a system catalog `pg_proc`
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-57_529_6352981594975380193-1
19/06/29 17:55:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:57 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:55:57 INFO DAGScheduler: Got job 6 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:55:57 INFO DAGScheduler: Final stage: ResultStage 6 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:55:57 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:55:57 INFO DAGScheduler: Missing parents: List()
19/06/29 17:55:57 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[13] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:55:57 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 215.3 KB, free 365.0 MB)
19/06/29 17:55:57 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 76.0 KB, free 364.9 MB)
19/06/29 17:55:57 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 35758d068699:41799 (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:55:57 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1161
19/06/29 17:55:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:55:57 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
19/06/29 17:55:57 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:55:57 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
19/06/29 17:55:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:57 INFO FileOutputCommitter: Saved output of task 'attempt_20190629175557_0006_m_000000_6' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-57_529_6352981594975380193-1/-ext-10000/_temporary/0/task_20190629175557_0006_m_000000
19/06/29 17:55:57 INFO SparkHadoopMapRedUtil: attempt_20190629175557_0006_m_000000_6: Committed
19/06/29 17:55:57 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1979 bytes result sent to driver
19/06/29 17:55:57 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 28 ms on localhost (executor driver) (1/1)
19/06/29 17:55:57 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
19/06/29 17:55:57 INFO DAGScheduler: ResultStage 6 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.069 s
19/06/29 17:55:57 INFO DAGScheduler: Job 6 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.071894 s
19/06/29 17:55:57 INFO FileFormatWriter: Write Job 9866cd07-a4bb-48c4-afa9-f979615f260d committed.
19/06/29 17:55:57 INFO FileFormatWriter: Finished processing stats for write job 9866cd07-a4bb-48c4-afa9-f979615f260d.
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:55:57 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-57_529_6352981594975380193-1/-ext-10000/part-00000-df56a897-4099-405b-891a-dc9ad09a2d12-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-df56a897-4099-405b-891a-dc9ad09a2d12-c000, Status:true
19/06/29 17:55:57 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:57 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:57 INFO log: Updated size of table pg_proc to 220
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:57 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:57 INFO log: Updated size of table pg_proc to 220
19/06/29 17:55:57 INFO PgMetadata: Registering a function `pg_catalog.array_to_string` in a system catalog `pg_proc`
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-57_857_7413851541619967773-1
19/06/29 17:55:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:57 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:55:57 INFO DAGScheduler: Got job 7 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:55:57 INFO DAGScheduler: Final stage: ResultStage 7 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:55:57 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:55:57 INFO DAGScheduler: Missing parents: List()
19/06/29 17:55:57 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[15] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:55:57 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 215.3 KB, free 364.7 MB)
19/06/29 17:55:57 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 76.0 KB, free 364.6 MB)
19/06/29 17:55:57 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 35758d068699:41799 (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:55:57 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161
19/06/29 17:55:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:55:57 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
19/06/29 17:55:57 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:55:57 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
19/06/29 17:55:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:57 INFO FileOutputCommitter: Saved output of task 'attempt_20190629175557_0007_m_000000_7' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-57_857_7413851541619967773-1/-ext-10000/_temporary/0/task_20190629175557_0007_m_000000
19/06/29 17:55:57 INFO SparkHadoopMapRedUtil: attempt_20190629175557_0007_m_000000_7: Committed
19/06/29 17:55:57 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1979 bytes result sent to driver
19/06/29 17:55:57 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 26 ms on localhost (executor driver) (1/1)
19/06/29 17:55:57 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
19/06/29 17:55:57 INFO DAGScheduler: ResultStage 7 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.056 s
19/06/29 17:55:57 INFO DAGScheduler: Job 7 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.058030 s
19/06/29 17:55:57 INFO FileFormatWriter: Write Job 6253e7cf-6603-4837-bdd8-00c13971554d committed.
19/06/29 17:55:57 INFO FileFormatWriter: Finished processing stats for write job 6253e7cf-6603-4837-bdd8-00c13971554d.
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:57 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:55:57 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-57_857_7413851541619967773-1/-ext-10000/part-00000-a2d90875-7afe-48d4-abfa-e55a893bda2d-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-a2d90875-7afe-48d4-abfa-e55a893bda2d-c000, Status:true
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 60
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 168
19/06/29 17:55:58 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 35758d068699:41799 in memory (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 65
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 173
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 99
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 112
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 237
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 190
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 222
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 113
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 174
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 198
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 95
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 167
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 91
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 100
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 202
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 225
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 104
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 131
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 203
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 105
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 201
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 163
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 133
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 144
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 70
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 155
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 221
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 141
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 157
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 159
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 197
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 132
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 67
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 128
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 164
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 172
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 154
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 88
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 97
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 82
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 192
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 186
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 188
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 226
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 230
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 109
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 146
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 61
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 166
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 206
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 217
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 76
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 142
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 127
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 191
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 140
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 161
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 107
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 227
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 75
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 156
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 183
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 204
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 160
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 151
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 66
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 181
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 139
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 143
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 184
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 149
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 158
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 68
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 165
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 84
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 175
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 118
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 189
19/06/29 17:55:58 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:58 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 35758d068699:41799 in memory (size: 76.0 KB, free: 366.0 MB)
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 200
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 114
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 218
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 179
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 124
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 94
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 153
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 137
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 110
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 220
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 96
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 178
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 223
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 93
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 64
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 205
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 231
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 92
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 177
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 185
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 216
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 122
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 169
19/06/29 17:55:58 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 35758d068699:41799 in memory (size: 76.0 KB, free: 366.1 MB)
19/06/29 17:55:58 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:58 INFO log: Updated size of table pg_proc to 268
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 135
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 106
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 145
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 134
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 81
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 136
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 69
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 117
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 72
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 138
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 116
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 74
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 194
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 199
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 78
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 215
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 87
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 79
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 115
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 170
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 130
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 219
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 236
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 187
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 196
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 62
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 238
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 234
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 207
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 232
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 150
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 152
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 35758d068699:41799 in memory (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 229
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 86
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 101
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 233
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 89
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 77
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 162
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 35758d068699:41799 in memory (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 228
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 180
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 73
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 171
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 126
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 147
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 108
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 85
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 193
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 129
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 224
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 120
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 148
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 119
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 35758d068699:41799 in memory (size: 76.0 KB, free: 366.3 MB)
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 103
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 208
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 71
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 123
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 239
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 63
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 83
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 90
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 195
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 121
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 98
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 111
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 176
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 182
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 235
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 102
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 80
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 209
19/06/29 17:55:58 INFO ContextCleaner: Cleaned accumulator 125
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:58 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:58 INFO log: Updated size of table pg_proc to 268
19/06/29 17:55:58 INFO PgMetadata: Registering a function `pg_catalog.format_type` in a system catalog `pg_proc`
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-58_267_4436618517682936088-1
19/06/29 17:55:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:58 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:55:58 INFO DAGScheduler: Got job 8 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:55:58 INFO DAGScheduler: Final stage: ResultStage 8 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:55:58 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:55:58 INFO DAGScheduler: Missing parents: List()
19/06/29 17:55:58 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[17] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:55:58 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 215.3 KB, free 366.1 MB)
19/06/29 17:55:58 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 76.0 KB, free 366.0 MB)
19/06/29 17:55:58 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 35758d068699:41799 (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:55:58 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1161
19/06/29 17:55:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:55:58 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
19/06/29 17:55:58 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:55:58 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
19/06/29 17:55:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:58 INFO FileOutputCommitter: Saved output of task 'attempt_20190629175558_0008_m_000000_8' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-58_267_4436618517682936088-1/-ext-10000/_temporary/0/task_20190629175558_0008_m_000000
19/06/29 17:55:58 INFO SparkHadoopMapRedUtil: attempt_20190629175558_0008_m_000000_8: Committed
19/06/29 17:55:58 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 1979 bytes result sent to driver
19/06/29 17:55:58 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 27 ms on localhost (executor driver) (1/1)
19/06/29 17:55:58 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
19/06/29 17:55:58 INFO DAGScheduler: ResultStage 8 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.066 s
19/06/29 17:55:58 INFO DAGScheduler: Job 8 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.070355 s
19/06/29 17:55:58 INFO FileFormatWriter: Write Job 363f8c2d-139d-4681-ba0d-20c740960480 committed.
19/06/29 17:55:58 INFO FileFormatWriter: Finished processing stats for write job 363f8c2d-139d-4681-ba0d-20c740960480.
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:55:58 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-58_267_4436618517682936088-1/-ext-10000/part-00000-27a82298-76d7-4099-9f4d-d67593438499-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-27a82298-76d7-4099-9f4d-d67593438499-c000, Status:true
19/06/29 17:55:58 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:58 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:58 INFO log: Updated size of table pg_proc to 313
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:58 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:58 INFO log: Updated size of table pg_proc to 313
19/06/29 17:55:58 INFO PgMetadata: Registering a function `pg_catalog.obj_description` in a system catalog `pg_proc`
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-58_605_2713524578069880687-1
19/06/29 17:55:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:58 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:55:58 INFO DAGScheduler: Got job 9 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:55:58 INFO DAGScheduler: Final stage: ResultStage 9 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:55:58 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:55:58 INFO DAGScheduler: Missing parents: List()
19/06/29 17:55:58 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[19] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:55:58 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 215.3 KB, free 365.8 MB)
19/06/29 17:55:58 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 76.0 KB, free 365.7 MB)
19/06/29 17:55:58 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 35758d068699:41799 (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:55:58 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1161
19/06/29 17:55:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:55:58 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
19/06/29 17:55:58 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:55:58 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
19/06/29 17:55:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:58 INFO FileOutputCommitter: Saved output of task 'attempt_20190629175558_0009_m_000000_9' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-58_605_2713524578069880687-1/-ext-10000/_temporary/0/task_20190629175558_0009_m_000000
19/06/29 17:55:58 INFO SparkHadoopMapRedUtil: attempt_20190629175558_0009_m_000000_9: Committed
19/06/29 17:55:58 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 1979 bytes result sent to driver
19/06/29 17:55:58 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 23 ms on localhost (executor driver) (1/1)
19/06/29 17:55:58 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
19/06/29 17:55:58 INFO DAGScheduler: ResultStage 9 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.057 s
19/06/29 17:55:58 INFO DAGScheduler: Job 9 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.059422 s
19/06/29 17:55:58 INFO FileFormatWriter: Write Job 649701cd-cdbf-4a24-94b1-44785469a80e committed.
19/06/29 17:55:58 INFO FileFormatWriter: Finished processing stats for write job 649701cd-cdbf-4a24-94b1-44785469a80e.
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:55:58 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-58_605_2713524578069880687-1/-ext-10000/part-00000-225a6d3d-c70c-4dec-a7a8-d8c423245019-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-225a6d3d-c70c-4dec-a7a8-d8c423245019-c000, Status:true
19/06/29 17:55:58 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:58 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:58 INFO log: Updated size of table pg_proc to 362
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:58 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:58 INFO log: Updated size of table pg_proc to 362
19/06/29 17:55:58 INFO PgMetadata: Registering a function `pg_catalog.oidvectortypes` in a system catalog `pg_proc`
19/06/29 17:55:58 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:58 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-58_931_944529842123890660-1
19/06/29 17:55:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:58 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:55:58 INFO DAGScheduler: Got job 10 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:55:58 INFO DAGScheduler: Final stage: ResultStage 10 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:55:58 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:55:58 INFO DAGScheduler: Missing parents: List()
19/06/29 17:55:58 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[21] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:55:58 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 215.3 KB, free 365.5 MB)
19/06/29 17:55:59 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 75.9 KB, free 365.4 MB)
19/06/29 17:55:59 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 35758d068699:41799 (size: 75.9 KB, free: 366.1 MB)
19/06/29 17:55:59 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1161
19/06/29 17:55:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:55:59 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
19/06/29 17:55:59 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:55:59 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
19/06/29 17:55:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:59 INFO FileOutputCommitter: Saved output of task 'attempt_20190629175558_0010_m_000000_10' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-58_931_944529842123890660-1/-ext-10000/_temporary/0/task_20190629175558_0010_m_000000
19/06/29 17:55:59 INFO SparkHadoopMapRedUtil: attempt_20190629175558_0010_m_000000_10: Committed
19/06/29 17:55:59 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 1979 bytes result sent to driver
19/06/29 17:55:59 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 22 ms on localhost (executor driver) (1/1)
19/06/29 17:55:59 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
19/06/29 17:55:59 INFO DAGScheduler: ResultStage 10 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.058 s
19/06/29 17:55:59 INFO DAGScheduler: Job 10 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.060778 s
19/06/29 17:55:59 INFO FileFormatWriter: Write Job 68891add-0921-4ca9-ac8e-55671ba55853 committed.
19/06/29 17:55:59 INFO FileFormatWriter: Finished processing stats for write job 68891add-0921-4ca9-ac8e-55671ba55853.
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:55:59 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-58_931_944529842123890660-1/-ext-10000/part-00000-d30cb119-4065-409a-af7a-596cef14bf67-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-d30cb119-4065-409a-af7a-596cef14bf67-c000, Status:true
19/06/29 17:55:59 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:59 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:59 INFO log: Updated size of table pg_proc to 410
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:59 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:59 INFO log: Updated size of table pg_proc to 410
19/06/29 17:55:59 INFO PgMetadata: Registering a function `pg_catalog.pg_encoding_to_char` in a system catalog `pg_proc`
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-59_195_983394340117956800-1
19/06/29 17:55:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:59 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:55:59 INFO DAGScheduler: Got job 11 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:55:59 INFO DAGScheduler: Final stage: ResultStage 11 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:55:59 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:55:59 INFO DAGScheduler: Missing parents: List()
19/06/29 17:55:59 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[23] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:55:59 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 215.3 KB, free 365.2 MB)
19/06/29 17:55:59 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 75.9 KB, free 365.2 MB)
19/06/29 17:55:59 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 35758d068699:41799 (size: 75.9 KB, free: 366.0 MB)
19/06/29 17:55:59 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1161
19/06/29 17:55:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:55:59 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
19/06/29 17:55:59 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 8124 bytes)
19/06/29 17:55:59 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
19/06/29 17:55:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:59 INFO FileOutputCommitter: Saved output of task 'attempt_20190629175559_0011_m_000000_11' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-59_195_983394340117956800-1/-ext-10000/_temporary/0/task_20190629175559_0011_m_000000
19/06/29 17:55:59 INFO SparkHadoopMapRedUtil: attempt_20190629175559_0011_m_000000_11: Committed
19/06/29 17:55:59 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 1979 bytes result sent to driver
19/06/29 17:55:59 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 29 ms on localhost (executor driver) (1/1)
19/06/29 17:55:59 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
19/06/29 17:55:59 INFO DAGScheduler: ResultStage 11 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.066 s
19/06/29 17:55:59 INFO DAGScheduler: Job 11 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.067765 s
19/06/29 17:55:59 INFO FileFormatWriter: Write Job c5f472f8-fe66-417d-86dc-bca4bab956da committed.
19/06/29 17:55:59 INFO FileFormatWriter: Finished processing stats for write job c5f472f8-fe66-417d-86dc-bca4bab956da.
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:55:59 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-59_195_983394340117956800-1/-ext-10000/part-00000-24822b8e-d3fe-4943-abfe-64a538efd349-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-24822b8e-d3fe-4943-abfe-64a538efd349-c000, Status:true
19/06/29 17:55:59 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:59 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:59 INFO log: Updated size of table pg_proc to 463
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:59 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:59 INFO log: Updated size of table pg_proc to 463
19/06/29 17:55:59 INFO PgMetadata: Registering a function `pg_catalog.pg_function_is_visible` in a system catalog `pg_proc`
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-59_489_8861077846021988311-1
19/06/29 17:55:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:59 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:55:59 INFO DAGScheduler: Got job 12 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:55:59 INFO DAGScheduler: Final stage: ResultStage 12 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:55:59 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:55:59 INFO DAGScheduler: Missing parents: List()
19/06/29 17:55:59 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[25] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:55:59 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 215.3 KB, free 365.0 MB)
19/06/29 17:55:59 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 76.0 KB, free 364.9 MB)
19/06/29 17:55:59 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 35758d068699:41799 (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:55:59 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1161
19/06/29 17:55:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:55:59 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
19/06/29 17:55:59 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 8124 bytes)
19/06/29 17:55:59 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
19/06/29 17:55:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:59 INFO FileOutputCommitter: Saved output of task 'attempt_20190629175559_0012_m_000000_12' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-59_489_8861077846021988311-1/-ext-10000/_temporary/0/task_20190629175559_0012_m_000000
19/06/29 17:55:59 INFO SparkHadoopMapRedUtil: attempt_20190629175559_0012_m_000000_12: Committed
19/06/29 17:55:59 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 1979 bytes result sent to driver
19/06/29 17:55:59 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 24 ms on localhost (executor driver) (1/1)
19/06/29 17:55:59 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
19/06/29 17:55:59 INFO DAGScheduler: ResultStage 12 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.058 s
19/06/29 17:55:59 INFO DAGScheduler: Job 12 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.061411 s
19/06/29 17:55:59 INFO FileFormatWriter: Write Job 735eb80d-1ce4-4463-9182-8535357d5356 committed.
19/06/29 17:55:59 INFO FileFormatWriter: Finished processing stats for write job 735eb80d-1ce4-4463-9182-8535357d5356.
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:55:59 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-59_489_8861077846021988311-1/-ext-10000/part-00000-58ca0e1f-2ed7-4101-9cd2-23b9c646c067-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-58ca0e1f-2ed7-4101-9cd2-23b9c646c067-c000, Status:true
19/06/29 17:55:59 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:59 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:59 INFO log: Updated size of table pg_proc to 519
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:59 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:59 INFO log: Updated size of table pg_proc to 519
19/06/29 17:55:59 INFO PgMetadata: Registering a function `pg_catalog.pg_get_expr` in a system catalog `pg_proc`
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-59_770_7080557643473079999-1
19/06/29 17:55:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:59 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:55:59 INFO DAGScheduler: Got job 13 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:55:59 INFO DAGScheduler: Final stage: ResultStage 13 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:55:59 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:55:59 INFO DAGScheduler: Missing parents: List()
19/06/29 17:55:59 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[27] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:55:59 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 215.3 KB, free 364.7 MB)
19/06/29 17:55:59 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 76.0 KB, free 364.6 MB)
19/06/29 17:55:59 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 35758d068699:41799 (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:55:59 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1161
19/06/29 17:55:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:55:59 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
19/06/29 17:55:59 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:55:59 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
19/06/29 17:55:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:55:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:55:59 INFO FileOutputCommitter: Saved output of task 'attempt_20190629175559_0013_m_000000_13' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-59_770_7080557643473079999-1/-ext-10000/_temporary/0/task_20190629175559_0013_m_000000
19/06/29 17:55:59 INFO SparkHadoopMapRedUtil: attempt_20190629175559_0013_m_000000_13: Committed
19/06/29 17:55:59 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 1979 bytes result sent to driver
19/06/29 17:55:59 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 36 ms on localhost (executor driver) (1/1)
19/06/29 17:55:59 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
19/06/29 17:55:59 INFO DAGScheduler: ResultStage 13 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.071 s
19/06/29 17:55:59 INFO DAGScheduler: Job 13 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.073127 s
19/06/29 17:55:59 INFO FileFormatWriter: Write Job 22bbd62f-d767-4e2a-8bdb-2f790a3a6d4c committed.
19/06/29 17:55:59 INFO FileFormatWriter: Finished processing stats for write job 22bbd62f-d767-4e2a-8bdb-2f790a3a6d4c.
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:55:59 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-55-59_770_7080557643473079999-1/-ext-10000/part-00000-1797bf9f-d08c-4695-a965-189f839635c5-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-1797bf9f-d08c-4695-a965-189f839635c5-c000, Status:true
19/06/29 17:55:59 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:55:59 INFO log: Updating table stats fast for pg_proc
19/06/29 17:55:59 INFO log: Updated size of table pg_proc to 564
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:55:59 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:55:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:00 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:56:00 INFO log: Updating table stats fast for pg_proc
19/06/29 17:56:00 INFO log: Updated size of table pg_proc to 564
19/06/29 17:56:00 INFO PgMetadata: Registering a function `pg_catalog.pg_get_function_arguments` in a system catalog `pg_proc`
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:00 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-56-00_119_859822403986755207-1
19/06/29 17:56:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:56:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:56:00 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:56:00 INFO DAGScheduler: Got job 14 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:56:00 INFO DAGScheduler: Final stage: ResultStage 14 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:56:00 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:56:00 INFO DAGScheduler: Missing parents: List()
19/06/29 17:56:00 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[29] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:56:00 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 215.3 KB, free 364.4 MB)
19/06/29 17:56:00 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 76.0 KB, free 364.3 MB)
19/06/29 17:56:00 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 35758d068699:41799 (size: 76.0 KB, free: 365.8 MB)
19/06/29 17:56:00 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1161
19/06/29 17:56:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:56:00 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
19/06/29 17:56:00 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 8132 bytes)
19/06/29 17:56:00 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
19/06/29 17:56:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:56:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:56:00 INFO FileOutputCommitter: Saved output of task 'attempt_20190629175600_0014_m_000000_14' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-56-00_119_859822403986755207-1/-ext-10000/_temporary/0/task_20190629175600_0014_m_000000
19/06/29 17:56:00 INFO SparkHadoopMapRedUtil: attempt_20190629175600_0014_m_000000_14: Committed
19/06/29 17:56:00 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 1979 bytes result sent to driver
19/06/29 17:56:00 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 34 ms on localhost (executor driver) (1/1)
19/06/29 17:56:00 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
19/06/29 17:56:00 INFO DAGScheduler: ResultStage 14 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.072 s
19/06/29 17:56:00 INFO DAGScheduler: Job 14 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.076145 s
19/06/29 17:56:00 INFO FileFormatWriter: Write Job 04197c36-911b-4a1c-9d72-7a02bb20a244 committed.
19/06/29 17:56:00 INFO FileFormatWriter: Finished processing stats for write job 04197c36-911b-4a1c-9d72-7a02bb20a244.
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:00 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:56:00 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-56-00_119_859822403986755207-1/-ext-10000/part-00000-bdb70de9-7d62-470e-8308-89595621dd83-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-bdb70de9-7d62-470e-8308-89595621dd83-c000, Status:true
19/06/29 17:56:00 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:56:00 INFO log: Updating table stats fast for pg_proc
19/06/29 17:56:00 INFO log: Updated size of table pg_proc to 623
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:00 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:56:00 INFO log: Updating table stats fast for pg_proc
19/06/29 17:56:00 INFO log: Updated size of table pg_proc to 623
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 384
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 382
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 310
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 284
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 348
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 389
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 291
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 251
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 360
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 448
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 270
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 311
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 394
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 417
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 334
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 339
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 268
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 447
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 283
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 333
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 393
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 344
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 292
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 322
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 412
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 278
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 294
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 386
19/06/29 17:56:00 INFO PgMetadata: Registering a function `pg_catalog.pg_get_function_result` in a system catalog `pg_proc`
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:00 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 35758d068699:41799 in memory (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 263
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 428
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 306
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 413
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 241
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 282
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 320
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 298
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 397
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 250
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 439
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 299
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 383
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 323
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 375
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 330
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 253
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 340
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 425
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 410
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 315
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 379
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 401
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 327
19/06/29 17:56:00 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 35758d068699:41799 in memory (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:56:00 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-56-00_546_3331234930234704309-1
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 399
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 436
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 285
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 369
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 357
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 257
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 387
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 318
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 254
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 362
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 351
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 302
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 286
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 403
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 325
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 427
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 335
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 349
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 244
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 307
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 358
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 308
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 274
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 255
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 352
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 418
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 345
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 392
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 300
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 326
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 395
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 415
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 275
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 355
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 416
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 363
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 440
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 265
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 337
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 321
19/06/29 17:56:00 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 35758d068699:41799 in memory (size: 75.9 KB, free: 366.0 MB)
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 443
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 359
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 411
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 381
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 430
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 279
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 317
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 331
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 408
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 256
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 378
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 380
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 431
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 261
19/06/29 17:56:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:56:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:56:00 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 35758d068699:41799 in memory (size: 76.0 KB, free: 366.1 MB)
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 267
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 245
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 324
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 376
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 301
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 328
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 343
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 361
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 437
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 304
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 353
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 246
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 242
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 336
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 288
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 390
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 405
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 388
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 372
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 305
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 433
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 264
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 266
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 426
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 271
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 316
19/06/29 17:56:00 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 35758d068699:41799 in memory (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 303
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 373
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 441
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 314
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 409
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 347
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 243
19/06/29 17:56:00 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 35758d068699:41799 in memory (size: 75.9 KB, free: 366.2 MB)
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 341
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 402
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 252
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 444
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 273
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 368
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 281
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 280
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 446
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 449
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 364
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 371
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 276
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 262
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 429
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 297
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 277
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 258
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 309
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 442
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 269
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 313
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 374
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 407
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 398
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 248
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 400
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 296
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 247
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 260
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 377
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 445
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 396
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 259
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 365
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 434
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 346
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 367
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 385
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 406
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 289
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 290
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 391
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 293
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 249
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 356
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 404
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 329
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 370
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 319
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 342
19/06/29 17:56:00 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:56:00 INFO DAGScheduler: Got job 15 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:56:00 INFO DAGScheduler: Final stage: ResultStage 15 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:56:00 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:56:00 INFO DAGScheduler: Missing parents: List()
19/06/29 17:56:00 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[31] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:56:00 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 35758d068699:41799 in memory (size: 76.0 KB, free: 366.3 MB)
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 366
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 287
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 354
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 435
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 295
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 419
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 272
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 332
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 338
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 240
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 350
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 414
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 438
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 432
19/06/29 17:56:00 INFO ContextCleaner: Cleaned accumulator 312
19/06/29 17:56:00 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 215.3 KB, free 366.1 MB)
19/06/29 17:56:00 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 76.0 KB, free 366.0 MB)
19/06/29 17:56:00 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 35758d068699:41799 (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:56:00 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1161
19/06/29 17:56:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:56:00 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
19/06/29 17:56:00 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 8124 bytes)
19/06/29 17:56:00 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
19/06/29 17:56:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:56:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:56:00 INFO FileOutputCommitter: Saved output of task 'attempt_20190629175600_0015_m_000000_15' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-56-00_546_3331234930234704309-1/-ext-10000/_temporary/0/task_20190629175600_0015_m_000000
19/06/29 17:56:00 INFO SparkHadoopMapRedUtil: attempt_20190629175600_0015_m_000000_15: Committed
19/06/29 17:56:00 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 1979 bytes result sent to driver
19/06/29 17:56:00 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 32 ms on localhost (executor driver) (1/1)
19/06/29 17:56:00 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
19/06/29 17:56:00 INFO DAGScheduler: ResultStage 15 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.069 s
19/06/29 17:56:00 INFO DAGScheduler: Job 15 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.071364 s
19/06/29 17:56:00 INFO FileFormatWriter: Write Job 1571321f-0da2-4196-a7b8-9919fc716c13 committed.
19/06/29 17:56:00 INFO FileFormatWriter: Finished processing stats for write job 1571321f-0da2-4196-a7b8-9919fc716c13.
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:00 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:56:00 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-56-00_546_3331234930234704309-1/-ext-10000/part-00000-9b3532a7-cb34-435e-b88b-4807a03cf66e-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-9b3532a7-cb34-435e-b88b-4807a03cf66e-c000, Status:true
19/06/29 17:56:00 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:56:00 INFO log: Updating table stats fast for pg_proc
19/06/29 17:56:00 INFO log: Updated size of table pg_proc to 679
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:00 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:56:00 INFO log: Updating table stats fast for pg_proc
19/06/29 17:56:00 INFO log: Updated size of table pg_proc to 679
19/06/29 17:56:00 INFO PgMetadata: Registering a function `pg_catalog.pg_get_userbyid` in a system catalog `pg_proc`
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:00 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-56-00_880_1044673246717784832-1
19/06/29 17:56:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:56:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:56:00 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:56:00 INFO DAGScheduler: Got job 16 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:56:00 INFO DAGScheduler: Final stage: ResultStage 16 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:56:00 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:56:00 INFO DAGScheduler: Missing parents: List()
19/06/29 17:56:00 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[33] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:56:00 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 215.3 KB, free 365.8 MB)
19/06/29 17:56:00 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 76.0 KB, free 365.7 MB)
19/06/29 17:56:00 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 35758d068699:41799 (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:56:00 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1161
19/06/29 17:56:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[33] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:56:00 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
19/06/29 17:56:00 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:56:00 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
19/06/29 17:56:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:56:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:56:00 INFO FileOutputCommitter: Saved output of task 'attempt_20190629175600_0016_m_000000_16' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-56-00_880_1044673246717784832-1/-ext-10000/_temporary/0/task_20190629175600_0016_m_000000
19/06/29 17:56:00 INFO SparkHadoopMapRedUtil: attempt_20190629175600_0016_m_000000_16: Committed
19/06/29 17:56:00 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 1979 bytes result sent to driver
19/06/29 17:56:00 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 28 ms on localhost (executor driver) (1/1)
19/06/29 17:56:00 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
19/06/29 17:56:00 INFO DAGScheduler: ResultStage 16 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.057 s
19/06/29 17:56:00 INFO DAGScheduler: Job 16 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.058831 s
19/06/29 17:56:00 INFO FileFormatWriter: Write Job b1e32032-8c9c-4771-907a-28ac0e8a0800 committed.
19/06/29 17:56:00 INFO FileFormatWriter: Finished processing stats for write job b1e32032-8c9c-4771-907a-28ac0e8a0800.
19/06/29 17:56:00 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:00 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:56:01 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-56-00_880_1044673246717784832-1/-ext-10000/part-00000-1ead4664-d548-46f6-badf-b19a03adfb70-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-1ead4664-d548-46f6-badf-b19a03adfb70-c000, Status:true
19/06/29 17:56:01 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:56:01 INFO log: Updating table stats fast for pg_proc
19/06/29 17:56:01 INFO log: Updated size of table pg_proc to 728
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:56:01 INFO log: Updating table stats fast for pg_proc
19/06/29 17:56:01 INFO log: Updated size of table pg_proc to 728
19/06/29 17:56:01 INFO PgMetadata: Registering a function `pg_catalog.pg_table_is_visible` in a system catalog `pg_proc`
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-56-01_185_5639787733939943760-1
19/06/29 17:56:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:56:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:56:01 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:56:01 INFO DAGScheduler: Got job 17 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:56:01 INFO DAGScheduler: Final stage: ResultStage 17 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:56:01 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:56:01 INFO DAGScheduler: Missing parents: List()
19/06/29 17:56:01 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[35] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:56:01 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 215.3 KB, free 365.5 MB)
19/06/29 17:56:01 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 76.0 KB, free 365.4 MB)
19/06/29 17:56:01 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 35758d068699:41799 (size: 76.0 KB, free: 366.1 MB)
19/06/29 17:56:01 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1161
19/06/29 17:56:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[35] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:56:01 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks
19/06/29 17:56:01 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 8124 bytes)
19/06/29 17:56:01 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
19/06/29 17:56:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:56:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:56:01 INFO FileOutputCommitter: Saved output of task 'attempt_20190629175601_0017_m_000000_17' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-56-01_185_5639787733939943760-1/-ext-10000/_temporary/0/task_20190629175601_0017_m_000000
19/06/29 17:56:01 INFO SparkHadoopMapRedUtil: attempt_20190629175601_0017_m_000000_17: Committed
19/06/29 17:56:01 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 1979 bytes result sent to driver
19/06/29 17:56:01 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 23 ms on localhost (executor driver) (1/1)
19/06/29 17:56:01 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
19/06/29 17:56:01 INFO DAGScheduler: ResultStage 17 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.054 s
19/06/29 17:56:01 INFO DAGScheduler: Job 17 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.056966 s
19/06/29 17:56:01 INFO FileFormatWriter: Write Job 857e3790-8aae-411e-8f78-cd933d4e5e4a committed.
19/06/29 17:56:01 INFO FileFormatWriter: Finished processing stats for write job 857e3790-8aae-411e-8f78-cd933d4e5e4a.
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:56:01 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-56-01_185_5639787733939943760-1/-ext-10000/part-00000-b6b40254-9a51-4e6c-a4bb-8b22e7930c1b-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-b6b40254-9a51-4e6c-a4bb-8b22e7930c1b-c000, Status:true
19/06/29 17:56:01 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:56:01 INFO log: Updating table stats fast for pg_proc
19/06/29 17:56:01 INFO log: Updated size of table pg_proc to 781
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:56:01 INFO log: Updating table stats fast for pg_proc
19/06/29 17:56:01 INFO log: Updated size of table pg_proc to 781
19/06/29 17:56:01 INFO PgMetadata: Registering a function `pg_catalog.regtype` in a system catalog `pg_proc`
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-56-01_502_5828986655182718931-1
19/06/29 17:56:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:56:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:56:01 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:56:01 INFO DAGScheduler: Got job 18 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:56:01 INFO DAGScheduler: Final stage: ResultStage 18 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:56:01 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:56:01 INFO DAGScheduler: Missing parents: List()
19/06/29 17:56:01 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[37] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:56:01 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 215.3 KB, free 365.2 MB)
19/06/29 17:56:01 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 76.0 KB, free 365.2 MB)
19/06/29 17:56:01 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 35758d068699:41799 (size: 76.0 KB, free: 366.0 MB)
19/06/29 17:56:01 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1161
19/06/29 17:56:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[37] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:56:01 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
19/06/29 17:56:01 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18, localhost, executor driver, partition 0, PROCESS_LOCAL, 8108 bytes)
19/06/29 17:56:01 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
19/06/29 17:56:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:56:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:56:01 INFO FileOutputCommitter: Saved output of task 'attempt_20190629175601_0018_m_000000_18' to file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-56-01_502_5828986655182718931-1/-ext-10000/_temporary/0/task_20190629175601_0018_m_000000
19/06/29 17:56:01 INFO SparkHadoopMapRedUtil: attempt_20190629175601_0018_m_000000_18: Committed
19/06/29 17:56:01 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 1979 bytes result sent to driver
19/06/29 17:56:01 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 30 ms on localhost (executor driver) (1/1)
19/06/29 17:56:01 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
19/06/29 17:56:01 INFO DAGScheduler: ResultStage 18 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.061 s
19/06/29 17:56:01 INFO DAGScheduler: Job 18 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.067495 s
19/06/29 17:56:01 INFO FileFormatWriter: Write Job bdbecab6-ae26-4b11-ab5f-00716acdf1cd committed.
19/06/29 17:56:01 INFO FileFormatWriter: Finished processing stats for write job bdbecab6-ae26-4b11-ab5f-00716acdf1cd.
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:56:01 INFO Hive: Renaming src: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-56-01_502_5828986655182718931-1/-ext-10000/part-00000-737207ae-f10c-43c8-8514-55b9cec4c4d2-c000, dest: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-737207ae-f10c-43c8-8514-55b9cec4c4d2-c000, Status:true
19/06/29 17:56:01 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:56:01 INFO log: Updating table stats fast for pg_proc
19/06/29 17:56:01 INFO log: Updated size of table pg_proc to 822
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:56:01 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:56:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:56:01 INFO log: Updating table stats fast for pg_proc
19/06/29 17:56:01 INFO log: Updated size of table pg_proc to 822
19/06/29 17:56:01 INFO PgV3MessageHandler: Open a session (sessionId=0, channelId=898788581 userName=fdb hostAddr=35758d068699)
19/06/29 17:56:01 INFO PgWireProtocol: Query: statements=set DateStyle to 'ISO'
19/06/29 17:56:02 INFO OperationImpl: Running query with b8220f82-69ad-4ded-86df-ba3a43a3afc8:
Query:
set DateStyle to 'ISO'
Analyzed Plan:
SetCommand (DateStyle to 'ISO',None)

       
19/06/29 17:56:02 INFO PgWireProtocol: Query: statements=set extra_float_digits to 2
19/06/29 17:56:02 INFO OperationImpl: Running query with 9b4f0770-a3e7-4324-82fd-fa742962e2ac:
Query:
set extra_float_digits to 2
Analyzed Plan:
SetCommand (extra_float_digits to 2,None)

       
19/06/29 17:56:02 INFO PgWireProtocol: Query: statements=select oid, typbasetype from pg_type where typname = 'lo'
19/06/29 17:56:02 INFO HiveMetaStore: 1: get_table : db=default tbl=pg_type
19/06/29 17:56:02 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=pg_type	
19/06/29 17:56:02 ERROR PgV3MessageHandler: Exception detected in 'Query': org.apache.spark.sql.AnalysisException: Table or view not found: pg_type; line 1 pos 29
	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:733)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:685)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:715)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:708)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:708)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:654)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)
	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
	at scala.collection.immutable.List.foldLeft(List.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)
	at org.apache.spark.sql.fdb.service.OperationImpl$$anonfun$org$apache$spark$sql$fdb$service$OperationImpl$$analyzedPlan$1.apply(ExecutorImpl.scala:59)
	at org.apache.spark.sql.fdb.service.OperationImpl$$anonfun$org$apache$spark$sql$fdb$service$OperationImpl$$analyzedPlan$1.apply(ExecutorImpl.scala:59)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.fdb.service.OperationImpl.org$apache$spark$sql$fdb$service$OperationImpl$$analyzedPlan$lzycompute(ExecutorImpl.scala:58)
	at org.apache.spark.sql.fdb.service.OperationImpl.org$apache$spark$sql$fdb$service$OperationImpl$$analyzedPlan(ExecutorImpl.scala:58)
	at org.apache.spark.sql.fdb.service.OperationImpl.outputSchema$lzycompute(ExecutorImpl.scala:90)
	at org.apache.spark.sql.fdb.service.OperationImpl.outputSchema(ExecutorImpl.scala:89)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgWireProtocol$$anonfun$21$$anonfun$apply$24.apply(protocol.scala:615)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgWireProtocol$$anonfun$21$$anonfun$apply$24.apply(protocol.scala:592)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgV3MessageHandler$$anonfun$handleV3Messages$1.apply(protocol.scala:1284)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgV3MessageHandler$$anonfun$handleV3Messages$1.apply(protocol.scala:1275)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgV3MessageHandler.handleV3Messages(protocol.scala:1275)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgV3MessageHandler.channelRead0(protocol.scala:1084)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgV3MessageHandler.channelRead0(protocol.scala:1071)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'pg_type' not found in database 'default';
	at org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)
	at org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.hive.client.HiveClient$class.getTable(HiveClient.scala:81)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:83)
	at org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:118)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:700)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:700)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:699)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:706)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:730)
	... 79 more

19/06/29 17:56:02 INFO PgWireProtocol: Query: statements=select NULL, nspname, NULL from pg_catalog.pg_namespace n where true and nspname not in ('pg_catalog', 'information_schema', 'pg_toast', 'pg_temp_1') order by nspname
19/06/29 17:56:02 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:56:02 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:56:02 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_namespace
19/06/29 17:56:02 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_namespace	
19/06/29 17:56:02 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_namespace
19/06/29 17:56:02 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_namespace	
19/06/29 17:56:02 INFO OperationImpl: Running query with 1027cc96-908d-48c7-9fb5-a9f3ef14ea49:
Query:
select NULL, nspname, NULL from pg_catalog.pg_namespace n where true and nspname not in ('pg_catalog', 'information_schema', 'pg_toast', 'pg_temp_1') order by nspname
Analyzed Plan:
Sort [nspname#843 ASC NULLS FIRST], true
+- Project [null AS NULL#844, nspname#843, null AS NULL#845]
   +- Filter (true && NOT nspname#843 IN (pg_catalog,information_schema,pg_toast,pg_temp_1))
      +- SubqueryAlias `n`
         +- SubqueryAlias `pg_catalog`.`pg_namespace`
            +- HiveTableRelation `pg_catalog`.`pg_namespace`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [oid#842, nspname#843]

       
19/06/29 17:56:02 INFO CodeGenerator: Code generated in 30.242846 ms
19/06/29 17:56:02 INFO CodeGenerator: Code generated in 35.621809 ms
19/06/29 17:56:02 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 281.3 KB, free 364.9 MB)
19/06/29 17:56:02 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 23.6 KB, free 364.9 MB)
19/06/29 17:56:02 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 35758d068699:41799 (size: 23.6 KB, free: 366.0 MB)
19/06/29 17:56:02 INFO SparkContext: Created broadcast 19 from 
19/06/29 17:56:02 INFO CodeGenerator: Code generated in 19.521967 ms
19/06/29 17:56:02 INFO FileInputFormat: Total input paths to process : 1
19/06/29 17:56:02 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:56:02 INFO DAGScheduler: Got job 19 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:56:02 INFO DAGScheduler: Final stage: ResultStage 19 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:56:02 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:56:02 INFO DAGScheduler: Missing parents: List()
19/06/29 17:56:02 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[45] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:56:02 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 12.7 KB, free 364.9 MB)
19/06/29 17:56:02 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 6.1 KB, free 364.8 MB)
19/06/29 17:56:02 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 35758d068699:41799 (size: 6.1 KB, free: 366.0 MB)
19/06/29 17:56:02 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1161
19/06/29 17:56:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[45] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:56:02 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 556
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 505
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 555
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 454
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 531
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 537
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 569
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 553
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 459
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 465
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 496
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 457
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 452
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 559
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 551
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 540
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 525
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 462
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 494
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 533
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 480
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 491
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 523
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 520
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 456
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 493
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 543
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 483
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 522
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 490
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 472
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 517
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 486
19/06/29 17:56:02 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 35758d068699:41799 in memory (size: 76.0 KB, free: 366.0 MB)
19/06/29 17:56:02 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19, localhost, executor driver, partition 0, PROCESS_LOCAL, 7979 bytes)
19/06/29 17:56:02 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 500
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 451
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 549
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 509
19/06/29 17:56:02 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 35758d068699:41799 in memory (size: 76.0 KB, free: 366.1 MB)
19/06/29 17:56:02 INFO HadoopRDD: Input split: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_namespace/part-00000-f5a923f9-fba6-47cf-beaf-480ccf41f0bb-c000:0+11
19/06/29 17:56:02 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 35758d068699:41799 in memory (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 458
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 504
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 516
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 476
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 502
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 526
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 478
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 510
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 497
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 492
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 468
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 561
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 545
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 514
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 455
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 503
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 461
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 528
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 541
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 488
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 536
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 499
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 560
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 535
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 460
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 521
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 484
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 558
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 565
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 469
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 501
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 518
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 481
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 515
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 527
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 547
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 513
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 532
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 539
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 544
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 512
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 474
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 450
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 570
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 566
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 495
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 485
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 463
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 467
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 473
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 542
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 464
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 508
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 507
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 489
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 567
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 524
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 498
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 546
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 564
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 550
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 477
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 519
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 568
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 471
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 563
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 466
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 487
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 470
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 548
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 506
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 475
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 453
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 554
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 511
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 552
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 562
19/06/29 17:56:02 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 35758d068699:41799 in memory (size: 76.0 KB, free: 366.3 MB)
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 482
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 479
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 530
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 534
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 538
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 557
19/06/29 17:56:02 INFO ContextCleaner: Cleaned accumulator 529
19/06/29 17:56:02 INFO CodeGenerator: Code generated in 10.121982 ms
19/06/29 17:56:03 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 1654 bytes result sent to driver
19/06/29 17:56:03 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 122 ms on localhost (executor driver) (1/1)
19/06/29 17:56:03 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
19/06/29 17:56:03 INFO DAGScheduler: ResultStage 19 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.170 s
19/06/29 17:56:03 INFO DAGScheduler: Job 19 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.174100 s
19/06/29 17:56:03 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:56:03 INFO DAGScheduler: Registering RDD 46 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:56:03 INFO DAGScheduler: Got job 20 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:56:03 INFO DAGScheduler: Final stage: ResultStage 21 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:56:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
19/06/29 17:56:03 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 20)
19/06/29 17:56:03 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[46] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:56:03 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 14.7 KB, free 366.0 MB)
19/06/29 17:56:03 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 7.2 KB, free 366.0 MB)
19/06/29 17:56:03 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 35758d068699:41799 (size: 7.2 KB, free: 366.3 MB)
19/06/29 17:56:03 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1161
19/06/29 17:56:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[46] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:56:03 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks
19/06/29 17:56:03 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 7968 bytes)
19/06/29 17:56:03 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
19/06/29 17:56:03 INFO HadoopRDD: Input split: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_namespace/part-00000-f5a923f9-fba6-47cf-beaf-480ccf41f0bb-c000:0+11
19/06/29 17:56:03 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 1535 bytes result sent to driver
19/06/29 17:56:03 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 74 ms on localhost (executor driver) (1/1)
19/06/29 17:56:03 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
19/06/29 17:56:03 INFO DAGScheduler: ShuffleMapStage 20 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.105 s
19/06/29 17:56:03 INFO DAGScheduler: looking for newly runnable stages
19/06/29 17:56:03 INFO DAGScheduler: running: Set()
19/06/29 17:56:03 INFO DAGScheduler: waiting: Set(ResultStage 21)
19/06/29 17:56:03 INFO DAGScheduler: failed: Set()
19/06/29 17:56:03 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[49] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:56:03 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 15.1 KB, free 365.9 MB)
19/06/29 17:56:03 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 7.9 KB, free 365.9 MB)
19/06/29 17:56:03 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 35758d068699:41799 (size: 7.9 KB, free: 366.3 MB)
19/06/29 17:56:03 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1161
19/06/29 17:56:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[49] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:56:03 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks
19/06/29 17:56:03 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21, localhost, executor driver, partition 0, ANY, 7767 bytes)
19/06/29 17:56:03 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)
19/06/29 17:56:03 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
19/06/29 17:56:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
19/06/29 17:56:03 INFO CodeGenerator: Code generated in 15.732881 ms
19/06/29 17:56:03 INFO Executor: Finished task 0.0 in stage 21.0 (TID 21). 2165 bytes result sent to driver
19/06/29 17:56:03 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 108 ms on localhost (executor driver) (1/1)
19/06/29 17:56:03 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
19/06/29 17:56:03 INFO DAGScheduler: ResultStage 21 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.122 s
19/06/29 17:56:03 INFO DAGScheduler: Job 20 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.253224 s
19/06/29 17:56:03 INFO PgWireProtocol: Query: statements=select relname, nspname, relkind from pg_catalog.pg_class c, pg_catalog.pg_namespace n where relkind in ('r', 'v') and nspname like 'spark' and relname like '%' and nspname not in ('pg_catalog', 'information_schema', 'pg_toast', 'pg_temp_1') and n.oid = relnamespace order by nspname, relname
19/06/29 17:56:03 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:56:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:56:03 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:56:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:56:03 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:56:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:56:03 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:56:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:56:03 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_namespace
19/06/29 17:56:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_namespace	
19/06/29 17:56:03 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_namespace
19/06/29 17:56:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_namespace	
19/06/29 17:56:03 INFO OperationImpl: Running query with 36555a47-e814-45d0-949b-4f58a4b08100:
Query:
select relname, nspname, relkind from pg_catalog.pg_class c, pg_catalog.pg_namespace n where relkind in ('r', 'v') and nspname like 'spark' and relname like '%' and nspname not in ('pg_catalog', 'information_schema', 'pg_toast', 'pg_temp_1') and n.oid = relnamespace order by nspname, relname
Analyzed Plan:
Sort [nspname#871 ASC NULLS FIRST, relname#851 ASC NULLS FIRST], true
+- Project [relname#851, nspname#871, relkind#854]
   +- Filter (((relkind#854 IN (r,v) && nspname#871 LIKE spark) && relname#851 LIKE %) && (NOT nspname#871 IN (pg_catalog,information_schema,pg_toast,pg_temp_1) && (oid#870 = relnamespace#855)))
      +- Join Inner
         :- SubqueryAlias `c`
         :  +- SubqueryAlias `pg_catalog`.`pg_class`
         :     +- HiveTableRelation `pg_catalog`.`pg_class`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [oid#849, reltablespace#850, relname#851, reloftype#852, relpersistence#853, relkind#854, relnamespace#855, relowner#856, relacl#857, relchecks#858, reltoastrelid#859, relhasindex#860, relhasrules#861, relhastriggers#862, relrowsecurity#863, relforcerowsecurity#864, relreplident#865, reltriggers#866, relhasoids#867, relispartition#868, relpartbound#869]
         +- SubqueryAlias `n`
            +- SubqueryAlias `pg_catalog`.`pg_namespace`
               +- HiveTableRelation `pg_catalog`.`pg_namespace`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [oid#870, nspname#871]

       
19/06/29 17:56:03 INFO CodeGenerator: Code generated in 30.16389 ms
19/06/29 17:56:03 INFO CodeGenerator: Code generated in 34.075976 ms
19/06/29 17:56:03 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 281.3 KB, free 365.7 MB)
19/06/29 17:56:03 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 23.6 KB, free 365.6 MB)
19/06/29 17:56:03 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 35758d068699:41799 (size: 23.6 KB, free: 366.2 MB)
19/06/29 17:56:03 INFO SparkContext: Created broadcast 23 from 
19/06/29 17:56:03 INFO FileInputFormat: Total input paths to process : 1
19/06/29 17:56:03 INFO SparkContext: Starting job: run at ThreadPoolExecutor.java:1142
19/06/29 17:56:03 INFO DAGScheduler: Got job 21 (run at ThreadPoolExecutor.java:1142) with 1 output partitions
19/06/29 17:56:03 INFO DAGScheduler: Final stage: ResultStage 22 (run at ThreadPoolExecutor.java:1142)
19/06/29 17:56:03 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:56:03 INFO DAGScheduler: Missing parents: List()
19/06/29 17:56:03 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[55] at run at ThreadPoolExecutor.java:1142), which has no missing parents
19/06/29 17:56:03 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 12.4 KB, free 365.6 MB)
19/06/29 17:56:03 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 6.0 KB, free 365.6 MB)
19/06/29 17:56:03 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 35758d068699:41799 (size: 6.0 KB, free: 366.2 MB)
19/06/29 17:56:03 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1161
19/06/29 17:56:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[55] at run at ThreadPoolExecutor.java:1142) (first 15 tasks are for partitions Vector(0))
19/06/29 17:56:03 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks
19/06/29 17:56:03 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22, localhost, executor driver, partition 0, PROCESS_LOCAL, 7979 bytes)
19/06/29 17:56:03 INFO Executor: Running task 0.0 in stage 22.0 (TID 22)
19/06/29 17:56:03 INFO HadoopRDD: Input split: file:/opt/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_namespace/part-00000-f5a923f9-fba6-47cf-beaf-480ccf41f0bb-c000:0+11
19/06/29 17:56:03 INFO CodeGenerator: Code generated in 34.318199 ms
19/06/29 17:56:03 INFO Executor: Finished task 0.0 in stage 22.0 (TID 22). 1452 bytes result sent to driver
19/06/29 17:56:03 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 69 ms on localhost (executor driver) (1/1)
19/06/29 17:56:03 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
19/06/29 17:56:03 INFO DAGScheduler: ResultStage 22 (run at ThreadPoolExecutor.java:1142) finished in 0.125 s
19/06/29 17:56:03 INFO DAGScheduler: Job 21 finished: run at ThreadPoolExecutor.java:1142, took 0.128091 s
19/06/29 17:56:03 INFO CodeGenerator: Code generated in 13.611737 ms
19/06/29 17:56:03 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 1024.0 KB, free 364.6 MB)
19/06/29 17:56:03 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 212.0 B, free 364.6 MB)
19/06/29 17:56:03 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 35758d068699:41799 (size: 212.0 B, free: 366.2 MB)
19/06/29 17:56:03 INFO SparkContext: Created broadcast 25 from run at ThreadPoolExecutor.java:1142
19/06/29 17:56:03 INFO CodeGenerator: Code generated in 24.229603 ms
19/06/29 17:56:03 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 282.1 KB, free 364.3 MB)
19/06/29 17:56:03 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 23.9 KB, free 364.3 MB)
19/06/29 17:56:03 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 35758d068699:41799 (size: 23.9 KB, free: 366.2 MB)
19/06/29 17:56:03 INFO SparkContext: Created broadcast 26 from 
19/06/29 17:56:03 INFO CodeGenerator: Code generated in 13.092995 ms
19/06/29 17:56:03 INFO FileInputFormat: Total input paths to process : 0
19/06/29 17:56:03 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:56:03 INFO DAGScheduler: Job 22 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.000066 s
19/06/29 17:56:04 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:56:04 INFO DAGScheduler: Registering RDD 64 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:56:04 INFO DAGScheduler: Got job 23 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:56:04 INFO DAGScheduler: Final stage: ResultStage 24 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:56:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)
19/06/29 17:56:04 INFO DAGScheduler: Missing parents: List()
19/06/29 17:56:04 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[67] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:56:04 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 21.3 KB, free 364.3 MB)
19/06/29 17:56:04 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 10.4 KB, free 364.3 MB)
19/06/29 17:56:04 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 35758d068699:41799 (size: 10.4 KB, free: 366.2 MB)
19/06/29 17:56:04 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1161
19/06/29 17:56:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[67] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:56:04 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks
19/06/29 17:56:04 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 23, localhost, executor driver, partition 0, PROCESS_LOCAL, 7767 bytes)
19/06/29 17:56:04 INFO Executor: Running task 0.0 in stage 24.0 (TID 23)
19/06/29 17:56:04 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
19/06/29 17:56:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
19/06/29 17:56:04 INFO Executor: Finished task 0.0 in stage 24.0 (TID 23). 2776 bytes result sent to driver
19/06/29 17:56:04 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 23) in 11 ms on localhost (executor driver) (1/1)
19/06/29 17:56:04 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
19/06/29 17:56:04 INFO DAGScheduler: ResultStage 24 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.024 s
19/06/29 17:56:04 INFO DAGScheduler: Job 23 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.027427 s
19/06/29 17:56:04 INFO PgWireProtocol: Query: statements=SET search_path TO spark,public
19/06/29 17:56:04 INFO OperationImpl: Running query with 4db35ea4-9995-4ee9-895a-7b063ef71c4b:
Query:
SET search_path TO spark,public
Analyzed Plan:
SetCommand (search_path TO spark,public,None)

       
19/06/29 17:56:04 INFO PgWireProtocol: Query: statements=SET statement_timeout TO 120000
19/06/29 17:56:04 INFO OperationImpl: Running query with 95b7a138-9cae-41ac-9c0e-7bb4a66e7c71:
Query:
SET statement_timeout TO 120000
Analyzed Plan:
SetCommand (statement_timeout TO 120000,None)

       
19/06/29 17:56:18 INFO PgV3MessageHandler: Close the session (sessionId=0, channelId=898788581)
19/06/29 17:57:45 ERROR SQLServer: RECEIVED SIGNAL TERM
19/06/29 17:57:45 INFO SparkContext: Invoking stop() from shutdown hook
19/06/29 17:57:45 INFO SparkContext: SparkContext already stopped.
19/06/29 17:57:45 INFO SparkUI: Stopped Spark web UI at http://35758d068699:4040
19/06/29 17:57:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/06/29 17:57:45 INFO MemoryStore: MemoryStore cleared
19/06/29 17:57:45 INFO BlockManager: BlockManager stopped
19/06/29 17:57:45 INFO BlockManagerMaster: BlockManagerMaster stopped
19/06/29 17:57:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/06/29 17:57:45 INFO SparkContext: Successfully stopped SparkContext
19/06/29 17:57:45 INFO ShutdownHookManager: Shutdown hook called
19/06/29 17:57:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-2f29e61f-233b-422e-a591-5e9a3f62c2b0
19/06/29 17:57:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-efac556f-7225-4ed3-a960-fdeb6840ce9d
