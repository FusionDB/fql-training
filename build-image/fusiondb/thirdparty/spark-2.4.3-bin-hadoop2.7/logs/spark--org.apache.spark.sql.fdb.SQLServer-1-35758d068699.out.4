Spark Command: /opt/jdk1.8.0_112/bin/java -cp ../thirdparty/spark-2.4.3-bin-hadoop2.7/conf/:/opt/fusiondb/sbin/../thirdparty/spark-2.4.3-bin-hadoop2.7/jars/* -Xmx1g org.apache.spark.deploy.SparkSubmit --conf spark.sql.server.port=54322 --conf spark.sql.server.psql.enabled=true --conf spark.sql.server.binaryTransferMode=false --properties-file /opt/fusiondb/sbin/../conf/spark-defaults.conf --class org.apache.spark.sql.fdb.SQLServer --name FusionDB SQL Server /opt/fusiondb/sbin/../assembly/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar
========================================
19/06/29 17:39:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
19/06/29 17:39:54 INFO SQLServer: Started daemon with process name: 142@35758d068699
19/06/29 17:39:54 INFO SignalUtils: Registered signal handler for TERM
19/06/29 17:39:54 INFO SignalUtils: Registered signal handler for HUP
19/06/29 17:39:54 INFO SignalUtils: Registered signal handler for INT
19/06/29 17:39:54 INFO SQLServer: Spark properties passed to the SQL server:
  key=spark.sql.crossJoin.enabled value=true
  key=spark.sql.server.port value=54322
  key=spark.app.name value=FusionDB SQL Server
  key=spark.sql.server.binaryTransferMode value=false
  key=spark.master value=local[*]
  key=spark.submit.deployMode value=client
  key=spark.sql.server.psql.enabled value=true
  key=spark.jars value=file:/opt/fusiondb/sbin/../assembly/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar
       
19/06/29 17:39:54 INFO RecurringTimer: Started timer for Idle Session Cleaner at time 1561830000000
19/06/29 17:39:54 INFO SparkContext: Running Spark version 2.4.3
19/06/29 17:39:54 INFO SparkContext: Submitted application: FusionDB SQL Server
19/06/29 17:39:54 INFO SecurityManager: Changing view acls to: root
19/06/29 17:39:54 INFO SecurityManager: Changing modify acls to: root
19/06/29 17:39:54 INFO SecurityManager: Changing view acls groups to: 
19/06/29 17:39:54 INFO SecurityManager: Changing modify acls groups to: 
19/06/29 17:39:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
19/06/29 17:39:55 INFO Utils: Successfully started service 'sparkDriver' on port 38821.
19/06/29 17:39:55 INFO SparkEnv: Registering MapOutputTracker
19/06/29 17:39:55 INFO SparkEnv: Registering BlockManagerMaster
19/06/29 17:39:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/06/29 17:39:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/06/29 17:39:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5617de61-6cb4-4510-8140-8b2bb9a3baea
19/06/29 17:39:55 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
19/06/29 17:39:55 INFO SparkEnv: Registering OutputCommitCoordinator
19/06/29 17:39:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/06/29 17:39:55 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://35758d068699:4040
19/06/29 17:39:55 INFO SparkContext: Added JAR file:/opt/fusiondb/sbin/../assembly/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar at spark://35758d068699:38821/jars/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar with timestamp 1561829995461
19/06/29 17:39:55 INFO Executor: Starting executor ID driver on host localhost
19/06/29 17:39:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46235.
19/06/29 17:39:55 INFO NettyBlockTransferService: Server created on 35758d068699:46235
19/06/29 17:39:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/06/29 17:39:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 35758d068699, 46235, None)
19/06/29 17:39:55 INFO BlockManagerMasterEndpoint: Registering block manager 35758d068699:46235 with 366.3 MB RAM, BlockManagerId(driver, 35758d068699, 46235, None)
19/06/29 17:39:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 35758d068699, 46235, None)
19/06/29 17:39:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 35758d068699, 46235, None)
19/06/29 17:39:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/fusiondb/sbin/spark-warehouse/').
19/06/29 17:39:56 INFO SharedState: Warehouse path is 'file:/opt/fusiondb/sbin/spark-warehouse/'.
19/06/29 17:39:56 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
19/06/29 17:39:56 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19/06/29 17:39:57 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19/06/29 17:39:57 INFO ObjectStore: ObjectStore, initialize called
19/06/29 17:39:58 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
19/06/29 17:39:58 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
19/06/29 17:40:00 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19/06/29 17:40:02 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:40:02 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:40:02 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:40:02 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:40:02 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19/06/29 17:40:02 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/06/29 17:40:02 INFO ObjectStore: Initialized ObjectStore
19/06/29 17:40:03 INFO HiveMetaStore: Added admin role in metastore
19/06/29 17:40:03 INFO HiveMetaStore: Added public role in metastore
19/06/29 17:40:03 INFO HiveMetaStore: No user is added in admin role, since config is empty
19/06/29 17:40:03 INFO HiveMetaStore: 0: get_all_databases
19/06/29 17:40:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
19/06/29 17:40:03 INFO HiveMetaStore: 0: get_functions: db=default pat=*
19/06/29 17:40:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19/06/29 17:40:03 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:40:03 INFO HiveMetaStore: 0: get_functions: db=pg_catalog pat=*
19/06/29 17:40:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=pg_catalog pat=*	
19/06/29 17:40:03 INFO SessionState: Created HDFS directory: /tmp/hive/root
19/06/29 17:40:03 INFO SessionState: Created local directory: /tmp/root
19/06/29 17:40:03 INFO SessionState: Created local directory: /tmp/46e8451f-af5a-498a-a22b-b42fdbddd190_resources
19/06/29 17:40:03 INFO SessionState: Created HDFS directory: /tmp/hive/root/46e8451f-af5a-498a-a22b-b42fdbddd190
19/06/29 17:40:03 INFO SessionState: Created local directory: /tmp/root/46e8451f-af5a-498a-a22b-b42fdbddd190
19/06/29 17:40:03 INFO SessionState: Created HDFS directory: /tmp/hive/root/46e8451f-af5a-498a-a22b-b42fdbddd190/_tmp_space.db
19/06/29 17:40:03 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/opt/fusiondb/sbin/spark-warehouse/
19/06/29 17:40:03 INFO HiveMetaStore: 0: get_database: default
19/06/29 17:40:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/06/29 17:40:03 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:40:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:03 INFO LoggingHandler: [id: 0x09154c6c] REGISTERED
19/06/29 17:40:03 INFO LoggingHandler: [id: 0x09154c6c] BIND: 0.0.0.0/0.0.0.0:54322
19/06/29 17:40:03 INFO LoggingHandler: [id: 0x09154c6c, L:/0.0.0.0:54322] ACTIVE
19/06/29 17:40:03 INFO PgProtocolService: Start running the SQL server (port=54322, workerThreads=4)
19/06/29 17:40:15 INFO LoggingHandler: [id: 0x09154c6c, L:/0.0.0.0:54322] READ: [id: 0xcdb51524, L:/172.17.0.2:54322 - R:/172.17.0.1:58480]
19/06/29 17:40:15 INFO LoggingHandler: [id: 0x09154c6c, L:/0.0.0.0:54322] READ COMPLETE
19/06/29 17:40:18 INFO HiveMetaStore: 1: get_database: global_temp
19/06/29 17:40:18 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
19/06/29 17:40:18 INFO HiveMetaStore: 1: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19/06/29 17:40:18 INFO ObjectStore: ObjectStore, initialize called
19/06/29 17:40:18 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19/06/29 17:40:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/06/29 17:40:18 INFO ObjectStore: Initialized ObjectStore
19/06/29 17:40:18 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
19/06/29 17:40:18 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:18 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:18 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:18 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:18 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:18 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:18 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:18 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:19 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:19 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:19 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:19 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:19 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:19 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:19 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:19 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:19 INFO HiveMetaStore: 1: drop_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=drop_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:40:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:40:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:40:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:40:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:40:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:40:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:40:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:40:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:40:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:40:20 INFO hivemetastoressimpl: deleting  file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database
19/06/29 17:40:20 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:20 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:20 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:20 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:20 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:20 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:20 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:20 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:20 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:20 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:20 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:20 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:20 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:20 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:20 INFO HiveMetaStore: 1: create_table: Table(tableName:pg_database, dbName:pg_catalog, owner:root, createTime:1561830020, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:datname, type:string, comment:null), FieldSchema(name:datdba, type:int, comment:null), FieldSchema(name:encoding, type:int, comment:null), FieldSchema(name:datcollate, type:string, comment:null), FieldSchema(name:datctype, type:string, comment:null), FieldSchema(name:datacl, type:array<string>, comment:null)], location:file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"datname","type":"string","nullable":true,"metadata":{}},{"name":"datdba","type":"integer","nullable":true,"metadata":{}},{"name":"encoding","type":"integer","nullable":true,"metadata":{}},{"name":"datcollate","type":"string","nullable":true,"metadata":{}},{"name":"datctype","type":"string","nullable":true,"metadata":{}},{"name":"datacl","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:40:20 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_database, dbName:pg_catalog, owner:root, createTime:1561830020, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:datname, type:string, comment:null), FieldSchema(name:datdba, type:int, comment:null), FieldSchema(name:encoding, type:int, comment:null), FieldSchema(name:datcollate, type:string, comment:null), FieldSchema(name:datctype, type:string, comment:null), FieldSchema(name:datacl, type:array<string>, comment:null)], location:file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"datname","type":"string","nullable":true,"metadata":{}},{"name":"datdba","type":"integer","nullable":true,"metadata":{}},{"name":"encoding","type":"integer","nullable":true,"metadata":{}},{"name":"datcollate","type":"string","nullable":true,"metadata":{}},{"name":"datctype","type":"string","nullable":true,"metadata":{}},{"name":"datacl","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:40:20 WARN HiveMetaStore: Location: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database specified for non-external table:pg_database
19/06/29 17:40:20 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database
19/06/29 17:40:20 INFO HiveMetaStore: 1: get_databases: *
19/06/29 17:40:20 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_databases: *	
19/06/29 17:40:20 INFO PgMetadata: Registering a database `default` in a system catalog `pg_database`
19/06/29 17:40:20 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:20 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:20 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/.hive-staging_hive_2019-06-29_17-40-20_943_8474107826118309793-1
19/06/29 17:40:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:21 INFO CodeGenerator: Code generated in 343.377013 ms
19/06/29 17:40:22 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:40:22 INFO DAGScheduler: Got job 0 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:40:22 INFO DAGScheduler: Final stage: ResultStage 0 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:40:22 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:40:22 INFO DAGScheduler: Missing parents: List()
19/06/29 17:40:22 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:40:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 215.0 KB, free 366.1 MB)
19/06/29 17:40:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 75.7 KB, free 366.0 MB)
19/06/29 17:40:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 35758d068699:46235 (size: 75.7 KB, free: 366.2 MB)
19/06/29 17:40:22 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
19/06/29 17:40:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:40:22 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
19/06/29 17:40:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8092 bytes)
19/06/29 17:40:22 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
19/06/29 17:40:22 INFO Executor: Fetching spark://35758d068699:38821/jars/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar with timestamp 1561829995461
19/06/29 17:40:22 INFO TransportClientFactory: Successfully created connection to 35758d068699/172.17.0.2:38821 after 66 ms (0 ms spent in bootstraps)
19/06/29 17:40:22 INFO Utils: Fetching spark://35758d068699:38821/jars/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar to /tmp/spark-5b7bcea9-d126-4cc1-b2bc-2530e939a279/userFiles-3fc7a133-7166-4768-b662-e9bc4e078c64/fetchFileTemp4661838883476082373.tmp
19/06/29 17:40:23 INFO Executor: Adding file:/tmp/spark-5b7bcea9-d126-4cc1-b2bc-2530e939a279/userFiles-3fc7a133-7166-4768-b662-e9bc4e078c64/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar to class loader
19/06/29 17:40:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:23 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174022_0000_m_000000_0' to file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/.hive-staging_hive_2019-06-29_17-40-20_943_8474107826118309793-1/-ext-10000/_temporary/0/task_20190629174022_0000_m_000000
19/06/29 17:40:23 INFO SparkHadoopMapRedUtil: attempt_20190629174022_0000_m_000000_0: Committed
19/06/29 17:40:23 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2022 bytes result sent to driver
19/06/29 17:40:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 920 ms on localhost (executor driver) (1/1)
19/06/29 17:40:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
19/06/29 17:40:23 INFO DAGScheduler: ResultStage 0 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 1.190 s
19/06/29 17:40:23 INFO DAGScheduler: Job 0 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 1.265091 s
19/06/29 17:40:23 INFO FileFormatWriter: Write Job a0077c78-e6a0-487f-b5dd-a1238ccff132 committed.
19/06/29 17:40:23 INFO FileFormatWriter: Finished processing stats for write job a0077c78-e6a0-487f-b5dd-a1238ccff132.
19/06/29 17:40:23 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:23 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:23 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:23 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:23 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:40:23 INFO Hive: Renaming src: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/.hive-staging_hive_2019-06-29_17-40-20_943_8474107826118309793-1/-ext-10000/part-00000-75f66cb2-9ada-4df1-a0be-530274295ec4-c000, dest: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/part-00000-75f66cb2-9ada-4df1-a0be-530274295ec4-c000, Status:true
19/06/29 17:40:23 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_database newtbl=pg_database
19/06/29 17:40:23 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_database newtbl=pg_database	
19/06/29 17:40:23 INFO log: Updating table stats fast for pg_database
19/06/29 17:40:23 INFO log: Updated size of table pg_database to 17
19/06/29 17:40:23 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:23 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:23 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:23 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:23 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:23 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:23 INFO PgMetadata: Registering a database `pg_catalog` in a system catalog `pg_database`
19/06/29 17:40:23 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:23 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:23 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/.hive-staging_hive_2019-06-29_17-40-23_713_9149872438725016353-1
19/06/29 17:40:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:23 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:40:23 INFO DAGScheduler: Got job 1 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:40:23 INFO DAGScheduler: Final stage: ResultStage 1 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:40:23 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:40:23 INFO DAGScheduler: Missing parents: List()
19/06/29 17:40:23 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:40:23 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 215.0 KB, free 365.8 MB)
19/06/29 17:40:23 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 75.7 KB, free 365.7 MB)
19/06/29 17:40:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 35758d068699:46235 (size: 75.7 KB, free: 366.2 MB)
19/06/29 17:40:23 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
19/06/29 17:40:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:40:23 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
19/06/29 17:40:23 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 8100 bytes)
19/06/29 17:40:23 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
19/06/29 17:40:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:23 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174023_0001_m_000000_1' to file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/.hive-staging_hive_2019-06-29_17-40-23_713_9149872438725016353-1/-ext-10000/_temporary/0/task_20190629174023_0001_m_000000
19/06/29 17:40:23 INFO SparkHadoopMapRedUtil: attempt_20190629174023_0001_m_000000_1: Committed
19/06/29 17:40:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1979 bytes result sent to driver
19/06/29 17:40:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 49 ms on localhost (executor driver) (1/1)
19/06/29 17:40:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
19/06/29 17:40:23 INFO DAGScheduler: ResultStage 1 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.084 s
19/06/29 17:40:23 INFO DAGScheduler: Job 1 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.088363 s
19/06/29 17:40:23 INFO FileFormatWriter: Write Job 60c9d563-f06f-4015-80ae-0ef1a1eafbab committed.
19/06/29 17:40:23 INFO FileFormatWriter: Finished processing stats for write job 60c9d563-f06f-4015-80ae-0ef1a1eafbab.
19/06/29 17:40:23 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:23 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:23 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:23 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:23 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:40:23 INFO Hive: Renaming src: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/.hive-staging_hive_2019-06-29_17-40-23_713_9149872438725016353-1/-ext-10000/part-00000-70d7d367-af61-4d48-ac65-4c13b6165110-c000, dest: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_database/part-00000-70d7d367-af61-4d48-ac65-4c13b6165110-c000, Status:true
19/06/29 17:40:23 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_database newtbl=pg_database
19/06/29 17:40:23 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_database newtbl=pg_database	
19/06/29 17:40:23 INFO log: Updating table stats fast for pg_database
19/06/29 17:40:23 INFO log: Updated size of table pg_database to 37
19/06/29 17:40:23 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:23 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:23 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:23 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_database
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_database	
19/06/29 17:40:24 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_database newtbl=pg_database
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_database newtbl=pg_database	
19/06/29 17:40:24 INFO log: Updating table stats fast for pg_database
19/06/29 17:40:24 INFO log: Updated size of table pg_database to 37
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:40:24 INFO HiveMetaStore: 1: drop_table : db=pg_catalog tbl=pg_class
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=drop_table : db=pg_catalog tbl=pg_class	
19/06/29 17:40:24 INFO hivemetastoressimpl: deleting  file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_class
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_class
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_class	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: create_table: Table(tableName:pg_class, dbName:pg_catalog, owner:root, createTime:1561830024, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:reltablespace, type:int, comment:null), FieldSchema(name:relname, type:string, comment:null), FieldSchema(name:reloftype, type:int, comment:null), FieldSchema(name:relpersistence, type:string, comment:null), FieldSchema(name:relkind, type:string, comment:null), FieldSchema(name:relnamespace, type:int, comment:null), FieldSchema(name:relowner, type:int, comment:null), FieldSchema(name:relacl, type:array<string>, comment:null), FieldSchema(name:relchecks, type:smallint, comment:null), FieldSchema(name:reltoastrelid, type:int, comment:null), FieldSchema(name:relhasindex, type:boolean, comment:null), FieldSchema(name:relhasrules, type:boolean, comment:null), FieldSchema(name:relhastriggers, type:boolean, comment:null), FieldSchema(name:relrowsecurity, type:boolean, comment:null), FieldSchema(name:relforcerowsecurity, type:boolean, comment:null), FieldSchema(name:relreplident, type:string, comment:null), FieldSchema(name:reltriggers, type:smallint, comment:null), FieldSchema(name:relhasoids, type:boolean, comment:null), FieldSchema(name:relispartition, type:boolean, comment:null), FieldSchema(name:relpartbound, type:string, comment:null)], location:file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_class, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"reltablespace","type":"integer","nullable":true,"metadata":{}},{"name":"relname","type":"string","nullable":true,"metadata":{}},{"name":"reloftype","type":"integer","nullable":true,"metadata":{}},{"name":"relpersistence","type":"string","nullable":true,"metadata":{}},{"name":"relkind","type":"string","nullable":true,"metadata":{}},{"name":"relnamespace","type":"integer","nullable":true,"metadata":{}},{"name":"relowner","type":"integer","nullable":true,"metadata":{}},{"name":"relacl","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}},{"name":"relchecks","type":"short","nullable":true,"metadata":{}},{"name":"reltoastrelid","type":"integer","nullable":true,"metadata":{}},{"name":"relhasindex","type":"boolean","nullable":true,"metadata":{}},{"name":"relhasrules","type":"boolean","nullable":true,"metadata":{}},{"name":"relhastriggers","type":"boolean","nullable":true,"metadata":{}},{"name":"relrowsecurity","type":"boolean","nullable":true,"metadata":{}},{"name":"relforcerowsecurity","type":"boolean","nullable":true,"metadata":{}},{"name":"relreplident","type":"string","nullable":true,"metadata":{}},{"name":"reltriggers","type":"short","nullable":true,"metadata":{}},{"name":"relhasoids","type":"boolean","nullable":true,"metadata":{}},{"name":"relispartition","type":"boolean","nullable":true,"metadata":{}},{"name":"relpartbound","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_class, dbName:pg_catalog, owner:root, createTime:1561830024, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:reltablespace, type:int, comment:null), FieldSchema(name:relname, type:string, comment:null), FieldSchema(name:reloftype, type:int, comment:null), FieldSchema(name:relpersistence, type:string, comment:null), FieldSchema(name:relkind, type:string, comment:null), FieldSchema(name:relnamespace, type:int, comment:null), FieldSchema(name:relowner, type:int, comment:null), FieldSchema(name:relacl, type:array<string>, comment:null), FieldSchema(name:relchecks, type:smallint, comment:null), FieldSchema(name:reltoastrelid, type:int, comment:null), FieldSchema(name:relhasindex, type:boolean, comment:null), FieldSchema(name:relhasrules, type:boolean, comment:null), FieldSchema(name:relhastriggers, type:boolean, comment:null), FieldSchema(name:relrowsecurity, type:boolean, comment:null), FieldSchema(name:relforcerowsecurity, type:boolean, comment:null), FieldSchema(name:relreplident, type:string, comment:null), FieldSchema(name:reltriggers, type:smallint, comment:null), FieldSchema(name:relhasoids, type:boolean, comment:null), FieldSchema(name:relispartition, type:boolean, comment:null), FieldSchema(name:relpartbound, type:string, comment:null)], location:file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_class, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"reltablespace","type":"integer","nullable":true,"metadata":{}},{"name":"relname","type":"string","nullable":true,"metadata":{}},{"name":"reloftype","type":"integer","nullable":true,"metadata":{}},{"name":"relpersistence","type":"string","nullable":true,"metadata":{}},{"name":"relkind","type":"string","nullable":true,"metadata":{}},{"name":"relnamespace","type":"integer","nullable":true,"metadata":{}},{"name":"relowner","type":"integer","nullable":true,"metadata":{}},{"name":"relacl","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}},{"name":"relchecks","type":"short","nullable":true,"metadata":{}},{"name":"reltoastrelid","type":"integer","nullable":true,"metadata":{}},{"name":"relhasindex","type":"boolean","nullable":true,"metadata":{}},{"name":"relhasrules","type":"boolean","nullable":true,"metadata":{}},{"name":"relhastriggers","type":"boolean","nullable":true,"metadata":{}},{"name":"relrowsecurity","type":"boolean","nullable":true,"metadata":{}},{"name":"relforcerowsecurity","type":"boolean","nullable":true,"metadata":{}},{"name":"relreplident","type":"string","nullable":true,"metadata":{}},{"name":"reltriggers","type":"short","nullable":true,"metadata":{}},{"name":"relhasoids","type":"boolean","nullable":true,"metadata":{}},{"name":"relispartition","type":"boolean","nullable":true,"metadata":{}},{"name":"relpartbound","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:40:24 WARN HiveMetaStore: Location: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_class specified for non-external table:pg_class
19/06/29 17:40:24 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_class
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 16
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 51
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 24
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 39
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 58
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 10
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 37
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 36
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 38
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 54
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 45
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 8
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 5
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 20
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 57
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 31
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 50
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 26
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 21
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 13
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 59
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 11
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 27
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 41
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 9
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 25
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:40:24 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 35758d068699:46235 in memory (size: 75.7 KB, free: 366.2 MB)
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 35
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 43
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 53
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 12
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 18
19/06/29 17:40:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 35758d068699:46235 in memory (size: 75.7 KB, free: 366.3 MB)
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 6
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 22
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 46
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 33
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 42
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 47
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 34
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 15
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 28
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 55
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 17
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 48
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 44
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 56
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 23
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 29
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 14
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 30
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 52
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 7
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 40
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 19
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 49
19/06/29 17:40:24 INFO ContextCleaner: Cleaned accumulator 32
19/06/29 17:40:24 INFO HiveMetaStore: 1: drop_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=drop_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:40:24 INFO hivemetastoressimpl: deleting  file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_attribute
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_attribute
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_attribute	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: create_table: Table(tableName:pg_attribute, dbName:pg_catalog, owner:root, createTime:1561830024, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:attrelid, type:int, comment:null), FieldSchema(name:attname, type:string, comment:null), FieldSchema(name:atttypid, type:int, comment:null), FieldSchema(name:attnotnull, type:boolean, comment:null), FieldSchema(name:atthasdef, type:boolean, comment:null), FieldSchema(name:atttypmod, type:int, comment:null), FieldSchema(name:attlen, type:int, comment:null), FieldSchema(name:attnum, type:int, comment:null), FieldSchema(name:attidentity, type:string, comment:null), FieldSchema(name:attisdropped, type:boolean, comment:null), FieldSchema(name:attcollation, type:int, comment:null)], location:file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_attribute, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"attrelid","type":"integer","nullable":true,"metadata":{}},{"name":"attname","type":"string","nullable":true,"metadata":{}},{"name":"atttypid","type":"integer","nullable":true,"metadata":{}},{"name":"attnotnull","type":"boolean","nullable":true,"metadata":{}},{"name":"atthasdef","type":"boolean","nullable":true,"metadata":{}},{"name":"atttypmod","type":"integer","nullable":true,"metadata":{}},{"name":"attlen","type":"integer","nullable":true,"metadata":{}},{"name":"attnum","type":"integer","nullable":true,"metadata":{}},{"name":"attidentity","type":"string","nullable":true,"metadata":{}},{"name":"attisdropped","type":"boolean","nullable":true,"metadata":{}},{"name":"attcollation","type":"integer","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_attribute, dbName:pg_catalog, owner:root, createTime:1561830024, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:attrelid, type:int, comment:null), FieldSchema(name:attname, type:string, comment:null), FieldSchema(name:atttypid, type:int, comment:null), FieldSchema(name:attnotnull, type:boolean, comment:null), FieldSchema(name:atthasdef, type:boolean, comment:null), FieldSchema(name:atttypmod, type:int, comment:null), FieldSchema(name:attlen, type:int, comment:null), FieldSchema(name:attnum, type:int, comment:null), FieldSchema(name:attidentity, type:string, comment:null), FieldSchema(name:attisdropped, type:boolean, comment:null), FieldSchema(name:attcollation, type:int, comment:null)], location:file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_attribute, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"attrelid","type":"integer","nullable":true,"metadata":{}},{"name":"attname","type":"string","nullable":true,"metadata":{}},{"name":"atttypid","type":"integer","nullable":true,"metadata":{}},{"name":"attnotnull","type":"boolean","nullable":true,"metadata":{}},{"name":"atthasdef","type":"boolean","nullable":true,"metadata":{}},{"name":"atttypmod","type":"integer","nullable":true,"metadata":{}},{"name":"attlen","type":"integer","nullable":true,"metadata":{}},{"name":"attnum","type":"integer","nullable":true,"metadata":{}},{"name":"attidentity","type":"string","nullable":true,"metadata":{}},{"name":"attisdropped","type":"boolean","nullable":true,"metadata":{}},{"name":"attcollation","type":"integer","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:40:24 WARN HiveMetaStore: Location: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_attribute specified for non-external table:pg_attribute
19/06/29 17:40:24 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_attribute
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: default
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: default
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_tables: db=default pat=*
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:24 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:24 INFO HiveMetaStore: 1: drop_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=drop_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:24 INFO hivemetastoressimpl: deleting  file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:25 INFO HiveMetaStore: 1: create_table: Table(tableName:pg_proc, dbName:pg_catalog, owner:root, createTime:1561830025, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:proname, type:string, comment:null), FieldSchema(name:prorettype, type:int, comment:null), FieldSchema(name:proargtypes, type:array<int>, comment:null), FieldSchema(name:pronamespace, type:int, comment:null), FieldSchema(name:proisagg, type:boolean, comment:null), FieldSchema(name:proiswindow, type:boolean, comment:null), FieldSchema(name:proretset, type:boolean, comment:null)], location:file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"proname","type":"string","nullable":true,"metadata":{}},{"name":"prorettype","type":"integer","nullable":true,"metadata":{}},{"name":"proargtypes","type":{"type":"array","elementType":"integer","containsNull":true},"nullable":true,"metadata":{}},{"name":"pronamespace","type":"integer","nullable":true,"metadata":{}},{"name":"proisagg","type":"boolean","nullable":true,"metadata":{}},{"name":"proiswindow","type":"boolean","nullable":true,"metadata":{}},{"name":"proretset","type":"boolean","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:pg_proc, dbName:pg_catalog, owner:root, createTime:1561830025, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:proname, type:string, comment:null), FieldSchema(name:prorettype, type:int, comment:null), FieldSchema(name:proargtypes, type:array<int>, comment:null), FieldSchema(name:pronamespace, type:int, comment:null), FieldSchema(name:proisagg, type:boolean, comment:null), FieldSchema(name:proiswindow, type:boolean, comment:null), FieldSchema(name:proretset, type:boolean, comment:null)], location:file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"oid","type":"integer","nullable":true,"metadata":{}},{"name":"proname","type":"string","nullable":true,"metadata":{}},{"name":"prorettype","type":"integer","nullable":true,"metadata":{}},{"name":"proargtypes","type":{"type":"array","elementType":"integer","containsNull":true},"nullable":true,"metadata":{}},{"name":"pronamespace","type":"integer","nullable":true,"metadata":{}},{"name":"proisagg","type":"boolean","nullable":true,"metadata":{}},{"name":"proiswindow","type":"boolean","nullable":true,"metadata":{}},{"name":"proretset","type":"boolean","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.create.version=2.4.3}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
19/06/29 17:40:25 WARN HiveMetaStore: Location: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc specified for non-external table:pg_proc
19/06/29 17:40:25 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_database: default
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_database: default
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_functions: db=default pat=*
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19/06/29 17:40:25 INFO PgMetadata: Registering a function `ANY` in a system catalog `pg_proc`
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:25 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-25_198_7099833730571127313-1
19/06/29 17:40:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:25 INFO CodeGenerator: Code generated in 21.38555 ms
19/06/29 17:40:25 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:40:25 INFO DAGScheduler: Got job 2 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:40:25 INFO DAGScheduler: Final stage: ResultStage 2 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:40:25 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:40:25 INFO DAGScheduler: Missing parents: List()
19/06/29 17:40:25 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:40:25 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 215.4 KB, free 366.1 MB)
19/06/29 17:40:25 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 76.0 KB, free 366.0 MB)
19/06/29 17:40:25 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 35758d068699:46235 (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:40:25 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
19/06/29 17:40:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:40:25 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
19/06/29 17:40:25 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 8108 bytes)
19/06/29 17:40:25 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
19/06/29 17:40:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:25 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174025_0002_m_000000_2' to file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-25_198_7099833730571127313-1/-ext-10000/_temporary/0/task_20190629174025_0002_m_000000
19/06/29 17:40:25 INFO SparkHadoopMapRedUtil: attempt_20190629174025_0002_m_000000_2: Committed
19/06/29 17:40:25 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1979 bytes result sent to driver
19/06/29 17:40:25 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 36 ms on localhost (executor driver) (1/1)
19/06/29 17:40:25 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
19/06/29 17:40:25 INFO DAGScheduler: ResultStage 2 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.072 s
19/06/29 17:40:25 INFO DAGScheduler: Job 2 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.076361 s
19/06/29 17:40:25 INFO FileFormatWriter: Write Job 595d1900-cc59-4e99-8b7a-6a1ff92a5815 committed.
19/06/29 17:40:25 INFO FileFormatWriter: Finished processing stats for write job 595d1900-cc59-4e99-8b7a-6a1ff92a5815.
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:25 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:40:25 INFO Hive: Renaming src: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-25_198_7099833730571127313-1/-ext-10000/part-00000-31e0f8dc-8b66-404d-bf58-6c70fc8597ca-c000, dest: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-31e0f8dc-8b66-404d-bf58-6c70fc8597ca-c000, Status:true
19/06/29 17:40:25 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:25 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:25 INFO log: Updated size of table pg_proc to 37
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:25 INFO PgMetadata: Registering a function `array_in` in a system catalog `pg_proc`
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:25 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-25_496_5948257072298946182-1
19/06/29 17:40:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:25 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:40:25 INFO DAGScheduler: Got job 3 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:40:25 INFO DAGScheduler: Final stage: ResultStage 3 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:40:25 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:40:25 INFO DAGScheduler: Missing parents: List()
19/06/29 17:40:25 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[7] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:40:25 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 215.4 KB, free 365.8 MB)
19/06/29 17:40:25 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 76.0 KB, free 365.7 MB)
19/06/29 17:40:25 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 35758d068699:46235 (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:40:25 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
19/06/29 17:40:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:40:25 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
19/06/29 17:40:25 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 8108 bytes)
19/06/29 17:40:25 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
19/06/29 17:40:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:25 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174025_0003_m_000000_3' to file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-25_496_5948257072298946182-1/-ext-10000/_temporary/0/task_20190629174025_0003_m_000000
19/06/29 17:40:25 INFO SparkHadoopMapRedUtil: attempt_20190629174025_0003_m_000000_3: Committed
19/06/29 17:40:25 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1979 bytes result sent to driver
19/06/29 17:40:25 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 47 ms on localhost (executor driver) (1/1)
19/06/29 17:40:25 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
19/06/29 17:40:25 INFO DAGScheduler: ResultStage 3 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.078 s
19/06/29 17:40:25 INFO DAGScheduler: Job 3 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.081348 s
19/06/29 17:40:25 INFO FileFormatWriter: Write Job ae98f40a-3c42-4e02-b506-d3c6e99b90ee committed.
19/06/29 17:40:25 INFO FileFormatWriter: Finished processing stats for write job ae98f40a-3c42-4e02-b506-d3c6e99b90ee.
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:25 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:40:25 INFO Hive: Renaming src: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-25_496_5948257072298946182-1/-ext-10000/part-00000-ec02c047-393b-4ab4-a4c5-911287c44e72-c000, dest: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-ec02c047-393b-4ab4-a4c5-911287c44e72-c000, Status:true
19/06/29 17:40:25 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:25 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:25 INFO log: Updated size of table pg_proc to 78
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:25 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:25 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:25 INFO log: Updated size of table pg_proc to 78
19/06/29 17:40:25 INFO PgMetadata: Registering a function `array_upper` in a system catalog `pg_proc`
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:25 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-25_819_959578227600288326-1
19/06/29 17:40:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:25 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:40:25 INFO DAGScheduler: Got job 4 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:40:25 INFO DAGScheduler: Final stage: ResultStage 4 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:40:25 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:40:25 INFO DAGScheduler: Missing parents: List()
19/06/29 17:40:25 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[9] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:40:25 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 215.4 KB, free 365.5 MB)
19/06/29 17:40:25 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 76.0 KB, free 365.4 MB)
19/06/29 17:40:25 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 35758d068699:46235 (size: 76.0 KB, free: 366.1 MB)
19/06/29 17:40:25 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
19/06/29 17:40:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:40:25 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
19/06/29 17:40:25 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:40:25 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
19/06/29 17:40:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:25 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174025_0004_m_000000_4' to file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-25_819_959578227600288326-1/-ext-10000/_temporary/0/task_20190629174025_0004_m_000000
19/06/29 17:40:25 INFO SparkHadoopMapRedUtil: attempt_20190629174025_0004_m_000000_4: Committed
19/06/29 17:40:25 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1979 bytes result sent to driver
19/06/29 17:40:25 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 28 ms on localhost (executor driver) (1/1)
19/06/29 17:40:25 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
19/06/29 17:40:25 INFO DAGScheduler: ResultStage 4 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.058 s
19/06/29 17:40:25 INFO DAGScheduler: Job 4 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.059804 s
19/06/29 17:40:25 INFO FileFormatWriter: Write Job 4c51af1c-92d3-4ed1-9641-7ae989e81a37 committed.
19/06/29 17:40:25 INFO FileFormatWriter: Finished processing stats for write job 4c51af1c-92d3-4ed1-9641-7ae989e81a37.
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:25 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:40:25 INFO Hive: Renaming src: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-25_819_959578227600288326-1/-ext-10000/part-00000-e8546548-6cd3-4596-a2d4-25c84274ea23-c000, dest: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-e8546548-6cd3-4596-a2d4-25c84274ea23-c000, Status:true
19/06/29 17:40:25 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:25 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:25 INFO log: Updated size of table pg_proc to 123
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:25 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:26 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:26 INFO log: Updated size of table pg_proc to 123
19/06/29 17:40:26 INFO PgMetadata: Registering a function `current_schema` in a system catalog `pg_proc`
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-26_089_444802139996712873-1
19/06/29 17:40:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:26 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:40:26 INFO DAGScheduler: Got job 5 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:40:26 INFO DAGScheduler: Final stage: ResultStage 5 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:40:26 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:40:26 INFO DAGScheduler: Missing parents: List()
19/06/29 17:40:26 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[11] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:40:26 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 215.4 KB, free 365.2 MB)
19/06/29 17:40:26 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.0 KB, free 365.2 MB)
19/06/29 17:40:26 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 35758d068699:46235 (size: 76.0 KB, free: 366.0 MB)
19/06/29 17:40:26 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
19/06/29 17:40:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:40:26 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
19/06/29 17:40:26 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:40:26 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
19/06/29 17:40:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:26 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174026_0005_m_000000_5' to file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-26_089_444802139996712873-1/-ext-10000/_temporary/0/task_20190629174026_0005_m_000000
19/06/29 17:40:26 INFO SparkHadoopMapRedUtil: attempt_20190629174026_0005_m_000000_5: Committed
19/06/29 17:40:26 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2022 bytes result sent to driver
19/06/29 17:40:26 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 33 ms on localhost (executor driver) (1/1)
19/06/29 17:40:26 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
19/06/29 17:40:26 INFO DAGScheduler: ResultStage 5 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.063 s
19/06/29 17:40:26 INFO DAGScheduler: Job 5 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.065497 s
19/06/29 17:40:26 INFO FileFormatWriter: Write Job 91015126-1eb1-4f02-8d79-8e5d740887d9 committed.
19/06/29 17:40:26 INFO FileFormatWriter: Finished processing stats for write job 91015126-1eb1-4f02-8d79-8e5d740887d9.
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:40:26 INFO Hive: Renaming src: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-26_089_444802139996712873-1/-ext-10000/part-00000-dd5a2416-a777-48ee-bb4f-55d04888537c-c000, dest: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-dd5a2416-a777-48ee-bb4f-55d04888537c-c000, Status:true
19/06/29 17:40:26 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:26 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:26 INFO log: Updated size of table pg_proc to 171
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:26 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:26 INFO log: Updated size of table pg_proc to 171
19/06/29 17:40:26 INFO PgMetadata: Registering a function `current_schemas` in a system catalog `pg_proc`
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-26_400_8512176033097754743-1
19/06/29 17:40:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:26 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:40:26 INFO DAGScheduler: Got job 6 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:40:26 INFO DAGScheduler: Final stage: ResultStage 6 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:40:26 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:40:26 INFO DAGScheduler: Missing parents: List()
19/06/29 17:40:26 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[13] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:40:26 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 215.4 KB, free 365.0 MB)
19/06/29 17:40:26 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 76.0 KB, free 364.9 MB)
19/06/29 17:40:26 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 35758d068699:46235 (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:40:26 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1161
19/06/29 17:40:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:40:26 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
19/06/29 17:40:26 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:40:26 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
19/06/29 17:40:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:26 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174026_0006_m_000000_6' to file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-26_400_8512176033097754743-1/-ext-10000/_temporary/0/task_20190629174026_0006_m_000000
19/06/29 17:40:26 INFO SparkHadoopMapRedUtil: attempt_20190629174026_0006_m_000000_6: Committed
19/06/29 17:40:26 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1979 bytes result sent to driver
19/06/29 17:40:26 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 44 ms on localhost (executor driver) (1/1)
19/06/29 17:40:26 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
19/06/29 17:40:26 INFO DAGScheduler: ResultStage 6 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.071 s
19/06/29 17:40:26 INFO DAGScheduler: Job 6 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.073671 s
19/06/29 17:40:26 INFO FileFormatWriter: Write Job bfc4fbaa-8a8c-4039-b06e-b612e2b90ade committed.
19/06/29 17:40:26 INFO FileFormatWriter: Finished processing stats for write job bfc4fbaa-8a8c-4039-b06e-b612e2b90ade.
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:40:26 INFO Hive: Renaming src: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-26_400_8512176033097754743-1/-ext-10000/part-00000-d0f15fdb-25e6-4b33-868d-cd090c3deea1-c000, dest: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-d0f15fdb-25e6-4b33-868d-cd090c3deea1-c000, Status:true
19/06/29 17:40:26 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:26 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:26 INFO log: Updated size of table pg_proc to 220
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:26 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:26 INFO log: Updated size of table pg_proc to 220
19/06/29 17:40:26 INFO PgMetadata: Registering a function `pg_catalog.array_to_string` in a system catalog `pg_proc`
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-26_674_8527634921115104486-1
19/06/29 17:40:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:26 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:40:26 INFO DAGScheduler: Got job 7 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:40:26 INFO DAGScheduler: Final stage: ResultStage 7 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:40:26 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:40:26 INFO DAGScheduler: Missing parents: List()
19/06/29 17:40:26 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[15] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:40:26 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 215.4 KB, free 364.7 MB)
19/06/29 17:40:26 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 76.0 KB, free 364.6 MB)
19/06/29 17:40:26 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 35758d068699:46235 (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:40:26 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161
19/06/29 17:40:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:40:26 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
19/06/29 17:40:26 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:40:26 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
19/06/29 17:40:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:26 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174026_0007_m_000000_7' to file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-26_674_8527634921115104486-1/-ext-10000/_temporary/0/task_20190629174026_0007_m_000000
19/06/29 17:40:26 INFO SparkHadoopMapRedUtil: attempt_20190629174026_0007_m_000000_7: Committed
19/06/29 17:40:26 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1979 bytes result sent to driver
19/06/29 17:40:26 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 37 ms on localhost (executor driver) (1/1)
19/06/29 17:40:26 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
19/06/29 17:40:26 INFO DAGScheduler: ResultStage 7 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.065 s
19/06/29 17:40:26 INFO DAGScheduler: Job 7 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.067192 s
19/06/29 17:40:26 INFO FileFormatWriter: Write Job c16f3767-48dd-462e-8e72-6009e41ad7a0 committed.
19/06/29 17:40:26 INFO FileFormatWriter: Finished processing stats for write job c16f3767-48dd-462e-8e72-6009e41ad7a0.
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:40:26 INFO Hive: Renaming src: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-26_674_8527634921115104486-1/-ext-10000/part-00000-48c2cda9-c50f-4f6d-aa3e-19bdf62ffcd4-c000, dest: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-48c2cda9-c50f-4f6d-aa3e-19bdf62ffcd4-c000, Status:true
19/06/29 17:40:26 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:26 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:26 INFO log: Updated size of table pg_proc to 268
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:26 INFO ContextCleaner: Cleaned accumulator 102
19/06/29 17:40:26 INFO ContextCleaner: Cleaned accumulator 150
19/06/29 17:40:26 INFO ContextCleaner: Cleaned accumulator 136
19/06/29 17:40:26 INFO ContextCleaner: Cleaned accumulator 76
19/06/29 17:40:26 INFO ContextCleaner: Cleaned accumulator 232
19/06/29 17:40:26 INFO ContextCleaner: Cleaned accumulator 159
19/06/29 17:40:26 INFO ContextCleaner: Cleaned accumulator 154
19/06/29 17:40:26 INFO ContextCleaner: Cleaned accumulator 153
19/06/29 17:40:26 INFO ContextCleaner: Cleaned accumulator 200
19/06/29 17:40:26 INFO ContextCleaner: Cleaned accumulator 233
19/06/29 17:40:26 INFO ContextCleaner: Cleaned accumulator 181
19/06/29 17:40:26 INFO ContextCleaner: Cleaned accumulator 215
19/06/29 17:40:26 INFO ContextCleaner: Cleaned accumulator 68
19/06/29 17:40:26 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:26 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 35758d068699:46235 in memory (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:40:26 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:26 INFO log: Updated size of table pg_proc to 268
19/06/29 17:40:27 INFO PgMetadata: Registering a function `pg_catalog.format_type` in a system catalog `pg_proc`
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 103
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 115
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 100
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 144
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 207
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 221
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 186
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 112
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 131
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 141
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 170
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 97
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 203
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 95
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 79
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 130
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 239
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 121
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 124
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 174
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 218
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 230
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 101
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 155
19/06/29 17:40:27 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 35758d068699:46235 in memory (size: 76.0 KB, free: 366.0 MB)
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 216
19/06/29 17:40:27 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-27_029_3168572519337309144-1
19/06/29 17:40:27 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 35758d068699:46235 in memory (size: 76.0 KB, free: 366.1 MB)
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 146
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 219
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 71
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 185
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 237
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 127
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 182
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 89
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 83
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 86
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 190
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 123
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 93
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 81
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 143
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 177
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 145
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 62
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 179
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 147
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 191
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 162
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 175
19/06/29 17:40:27 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 35758d068699:46235 in memory (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:40:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 139
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 197
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 172
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 199
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 167
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 108
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 69
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 94
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 73
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 77
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 111
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 220
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 92
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 198
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 113
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 107
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 158
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 236
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 160
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 180
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 208
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 110
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 85
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 98
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 65
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 238
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 152
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 224
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 99
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 128
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 67
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 80
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 157
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 148
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 122
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 209
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 163
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 120
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 104
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 195
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 74
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 183
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 118
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 176
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 222
19/06/29 17:40:27 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 35758d068699:46235 in memory (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 189
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 129
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 105
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 217
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 66
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 91
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 88
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 168
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 72
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 126
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 206
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 228
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 75
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 166
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 231
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 132
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 138
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 140
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 225
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 78
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 229
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 165
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 137
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 109
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 226
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 87
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 235
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 161
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 194
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 63
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 192
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 82
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 125
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 227
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 151
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 193
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 96
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 184
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 106
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 156
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 164
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 64
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 114
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 169
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 196
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 116
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 202
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 173
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 201
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 171
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 205
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 223
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 117
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 188
19/06/29 17:40:27 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 35758d068699:46235 in memory (size: 76.0 KB, free: 366.3 MB)
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 90
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 178
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 187
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 234
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 61
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 133
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 70
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 60
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 149
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 119
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 135
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 134
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 142
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 204
19/06/29 17:40:27 INFO ContextCleaner: Cleaned accumulator 84
19/06/29 17:40:27 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:40:27 INFO DAGScheduler: Got job 8 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:40:27 INFO DAGScheduler: Final stage: ResultStage 8 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:40:27 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:40:27 INFO DAGScheduler: Missing parents: List()
19/06/29 17:40:27 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[17] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:40:27 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 215.4 KB, free 366.1 MB)
19/06/29 17:40:27 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 76.0 KB, free 366.0 MB)
19/06/29 17:40:27 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 35758d068699:46235 (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:40:27 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1161
19/06/29 17:40:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:40:27 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
19/06/29 17:40:27 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:40:27 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
19/06/29 17:40:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:27 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174027_0008_m_000000_8' to file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-27_029_3168572519337309144-1/-ext-10000/_temporary/0/task_20190629174027_0008_m_000000
19/06/29 17:40:27 INFO SparkHadoopMapRedUtil: attempt_20190629174027_0008_m_000000_8: Committed
19/06/29 17:40:27 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 1979 bytes result sent to driver
19/06/29 17:40:27 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 32 ms on localhost (executor driver) (1/1)
19/06/29 17:40:27 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
19/06/29 17:40:27 INFO DAGScheduler: ResultStage 8 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.058 s
19/06/29 17:40:27 INFO DAGScheduler: Job 8 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.060526 s
19/06/29 17:40:27 INFO FileFormatWriter: Write Job c53d1edb-3c38-4488-a660-10fe58ae4c8a committed.
19/06/29 17:40:27 INFO FileFormatWriter: Finished processing stats for write job c53d1edb-3c38-4488-a660-10fe58ae4c8a.
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:40:27 INFO Hive: Renaming src: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-27_029_3168572519337309144-1/-ext-10000/part-00000-2a0140b5-aa24-4e28-804f-7b8526461e4c-c000, dest: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-2a0140b5-aa24-4e28-804f-7b8526461e4c-c000, Status:true
19/06/29 17:40:27 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:27 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:27 INFO log: Updated size of table pg_proc to 313
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:27 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:27 INFO log: Updated size of table pg_proc to 313
19/06/29 17:40:27 INFO PgMetadata: Registering a function `pg_catalog.obj_description` in a system catalog `pg_proc`
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-27_356_635251901578451737-1
19/06/29 17:40:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:27 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:40:27 INFO DAGScheduler: Got job 9 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:40:27 INFO DAGScheduler: Final stage: ResultStage 9 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:40:27 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:40:27 INFO DAGScheduler: Missing parents: List()
19/06/29 17:40:27 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[19] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:40:27 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 215.4 KB, free 365.8 MB)
19/06/29 17:40:27 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 76.0 KB, free 365.7 MB)
19/06/29 17:40:27 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 35758d068699:46235 (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:40:27 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1161
19/06/29 17:40:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:40:27 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
19/06/29 17:40:27 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:40:27 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
19/06/29 17:40:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:27 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174027_0009_m_000000_9' to file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-27_356_635251901578451737-1/-ext-10000/_temporary/0/task_20190629174027_0009_m_000000
19/06/29 17:40:27 INFO SparkHadoopMapRedUtil: attempt_20190629174027_0009_m_000000_9: Committed
19/06/29 17:40:27 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 1979 bytes result sent to driver
19/06/29 17:40:27 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 32 ms on localhost (executor driver) (1/1)
19/06/29 17:40:27 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
19/06/29 17:40:27 INFO DAGScheduler: ResultStage 9 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.059 s
19/06/29 17:40:27 INFO DAGScheduler: Job 9 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.061587 s
19/06/29 17:40:27 INFO FileFormatWriter: Write Job 266d3ea2-3183-4b75-904b-9df6ccafb796 committed.
19/06/29 17:40:27 INFO FileFormatWriter: Finished processing stats for write job 266d3ea2-3183-4b75-904b-9df6ccafb796.
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:40:27 INFO Hive: Renaming src: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-27_356_635251901578451737-1/-ext-10000/part-00000-150a7f27-59e0-4573-9e3d-3132a5e5155e-c000, dest: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-150a7f27-59e0-4573-9e3d-3132a5e5155e-c000, Status:true
19/06/29 17:40:27 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:27 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:27 INFO log: Updated size of table pg_proc to 362
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:27 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:27 INFO log: Updated size of table pg_proc to 362
19/06/29 17:40:27 INFO PgMetadata: Registering a function `pg_catalog.oidvectortypes` in a system catalog `pg_proc`
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-27_644_378967832262623979-1
19/06/29 17:40:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:27 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:40:27 INFO DAGScheduler: Got job 10 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:40:27 INFO DAGScheduler: Final stage: ResultStage 10 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:40:27 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:40:27 INFO DAGScheduler: Missing parents: List()
19/06/29 17:40:27 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[21] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:40:27 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 215.4 KB, free 365.5 MB)
19/06/29 17:40:27 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 76.0 KB, free 365.4 MB)
19/06/29 17:40:27 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 35758d068699:46235 (size: 76.0 KB, free: 366.1 MB)
19/06/29 17:40:27 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1161
19/06/29 17:40:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:40:27 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
19/06/29 17:40:27 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:40:27 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
19/06/29 17:40:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:27 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174027_0010_m_000000_10' to file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-27_644_378967832262623979-1/-ext-10000/_temporary/0/task_20190629174027_0010_m_000000
19/06/29 17:40:27 INFO SparkHadoopMapRedUtil: attempt_20190629174027_0010_m_000000_10: Committed
19/06/29 17:40:27 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 1979 bytes result sent to driver
19/06/29 17:40:27 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 26 ms on localhost (executor driver) (1/1)
19/06/29 17:40:27 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
19/06/29 17:40:27 INFO DAGScheduler: ResultStage 10 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.061 s
19/06/29 17:40:27 INFO DAGScheduler: Job 10 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.063373 s
19/06/29 17:40:27 INFO FileFormatWriter: Write Job 3376eb0d-9cec-4269-8940-62efc959008c committed.
19/06/29 17:40:27 INFO FileFormatWriter: Finished processing stats for write job 3376eb0d-9cec-4269-8940-62efc959008c.
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:40:27 INFO Hive: Renaming src: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-27_644_378967832262623979-1/-ext-10000/part-00000-c8b80e60-2251-48fb-a645-f47c52999b34-c000, dest: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-c8b80e60-2251-48fb-a645-f47c52999b34-c000, Status:true
19/06/29 17:40:27 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:27 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:27 INFO log: Updated size of table pg_proc to 410
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:27 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:27 INFO log: Updated size of table pg_proc to 410
19/06/29 17:40:27 INFO PgMetadata: Registering a function `pg_catalog.pg_encoding_to_char` in a system catalog `pg_proc`
19/06/29 17:40:27 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:27 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-27_938_5329319073634871250-1
19/06/29 17:40:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:27 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:40:27 INFO DAGScheduler: Got job 11 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:40:27 INFO DAGScheduler: Final stage: ResultStage 11 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:40:27 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:40:27 INFO DAGScheduler: Missing parents: List()
19/06/29 17:40:27 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[23] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:40:28 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 215.4 KB, free 365.2 MB)
19/06/29 17:40:28 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 76.0 KB, free 365.2 MB)
19/06/29 17:40:28 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 35758d068699:46235 (size: 76.0 KB, free: 366.0 MB)
19/06/29 17:40:28 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1161
19/06/29 17:40:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:40:28 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
19/06/29 17:40:28 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 8124 bytes)
19/06/29 17:40:28 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
19/06/29 17:40:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:28 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174027_0011_m_000000_11' to file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-27_938_5329319073634871250-1/-ext-10000/_temporary/0/task_20190629174027_0011_m_000000
19/06/29 17:40:28 INFO SparkHadoopMapRedUtil: attempt_20190629174027_0011_m_000000_11: Committed
19/06/29 17:40:28 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 1979 bytes result sent to driver
19/06/29 17:40:28 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 24 ms on localhost (executor driver) (1/1)
19/06/29 17:40:28 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
19/06/29 17:40:28 INFO DAGScheduler: ResultStage 11 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.054 s
19/06/29 17:40:28 INFO DAGScheduler: Job 11 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.056865 s
19/06/29 17:40:28 INFO FileFormatWriter: Write Job d338a26e-1bb2-47e1-ab2f-08ad9f4fd5c9 committed.
19/06/29 17:40:28 INFO FileFormatWriter: Finished processing stats for write job d338a26e-1bb2-47e1-ab2f-08ad9f4fd5c9.
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:40:28 INFO Hive: Renaming src: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-27_938_5329319073634871250-1/-ext-10000/part-00000-b34c915c-518a-44ca-a7b0-c4a3faab447a-c000, dest: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-b34c915c-518a-44ca-a7b0-c4a3faab447a-c000, Status:true
19/06/29 17:40:28 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:28 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:28 INFO log: Updated size of table pg_proc to 463
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:28 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:28 INFO log: Updated size of table pg_proc to 463
19/06/29 17:40:28 INFO PgMetadata: Registering a function `pg_catalog.pg_function_is_visible` in a system catalog `pg_proc`
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-28_216_9074068913533235382-1
19/06/29 17:40:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:28 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:40:28 INFO DAGScheduler: Got job 12 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:40:28 INFO DAGScheduler: Final stage: ResultStage 12 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:40:28 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:40:28 INFO DAGScheduler: Missing parents: List()
19/06/29 17:40:28 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[25] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:40:28 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 215.4 KB, free 365.0 MB)
19/06/29 17:40:28 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 76.0 KB, free 364.9 MB)
19/06/29 17:40:28 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 35758d068699:46235 (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:40:28 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1161
19/06/29 17:40:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:40:28 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
19/06/29 17:40:28 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 8124 bytes)
19/06/29 17:40:28 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
19/06/29 17:40:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:28 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174028_0012_m_000000_12' to file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-28_216_9074068913533235382-1/-ext-10000/_temporary/0/task_20190629174028_0012_m_000000
19/06/29 17:40:28 INFO SparkHadoopMapRedUtil: attempt_20190629174028_0012_m_000000_12: Committed
19/06/29 17:40:28 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 1979 bytes result sent to driver
19/06/29 17:40:28 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 27 ms on localhost (executor driver) (1/1)
19/06/29 17:40:28 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
19/06/29 17:40:28 INFO DAGScheduler: ResultStage 12 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.056 s
19/06/29 17:40:28 INFO DAGScheduler: Job 12 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.059460 s
19/06/29 17:40:28 INFO FileFormatWriter: Write Job 73253834-4b85-4a91-8fd7-358b2be92898 committed.
19/06/29 17:40:28 INFO FileFormatWriter: Finished processing stats for write job 73253834-4b85-4a91-8fd7-358b2be92898.
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:40:28 INFO Hive: Renaming src: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-28_216_9074068913533235382-1/-ext-10000/part-00000-79023076-0934-4b60-b1cd-d4488cd6ca6e-c000, dest: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-79023076-0934-4b60-b1cd-d4488cd6ca6e-c000, Status:true
19/06/29 17:40:28 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:28 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:28 INFO log: Updated size of table pg_proc to 519
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:28 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:28 INFO log: Updated size of table pg_proc to 519
19/06/29 17:40:28 INFO PgMetadata: Registering a function `pg_catalog.pg_get_expr` in a system catalog `pg_proc`
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-28_484_7720516217238245458-1
19/06/29 17:40:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:28 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:40:28 INFO DAGScheduler: Got job 13 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:40:28 INFO DAGScheduler: Final stage: ResultStage 13 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:40:28 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:40:28 INFO DAGScheduler: Missing parents: List()
19/06/29 17:40:28 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[27] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:40:28 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 215.4 KB, free 364.7 MB)
19/06/29 17:40:28 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 76.0 KB, free 364.6 MB)
19/06/29 17:40:28 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 35758d068699:46235 (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:40:28 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1161
19/06/29 17:40:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:40:28 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
19/06/29 17:40:28 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:40:28 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
19/06/29 17:40:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:28 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174028_0013_m_000000_13' to file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-28_484_7720516217238245458-1/-ext-10000/_temporary/0/task_20190629174028_0013_m_000000
19/06/29 17:40:28 INFO SparkHadoopMapRedUtil: attempt_20190629174028_0013_m_000000_13: Committed
19/06/29 17:40:28 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 1979 bytes result sent to driver
19/06/29 17:40:28 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 23 ms on localhost (executor driver) (1/1)
19/06/29 17:40:28 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
19/06/29 17:40:28 INFO DAGScheduler: ResultStage 13 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.052 s
19/06/29 17:40:28 INFO DAGScheduler: Job 13 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.053469 s
19/06/29 17:40:28 INFO FileFormatWriter: Write Job f6fae582-53fe-41da-a719-b94d0abf27a1 committed.
19/06/29 17:40:28 INFO FileFormatWriter: Finished processing stats for write job f6fae582-53fe-41da-a719-b94d0abf27a1.
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:40:28 INFO Hive: Renaming src: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-28_484_7720516217238245458-1/-ext-10000/part-00000-07ae0e1b-01e3-4dec-b19a-8918c2bfdc21-c000, dest: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-07ae0e1b-01e3-4dec-b19a-8918c2bfdc21-c000, Status:true
19/06/29 17:40:28 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:28 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:28 INFO log: Updated size of table pg_proc to 564
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:28 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:28 INFO log: Updated size of table pg_proc to 564
19/06/29 17:40:28 INFO PgMetadata: Registering a function `pg_catalog.pg_get_function_arguments` in a system catalog `pg_proc`
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-28_793_6980925052116445287-1
19/06/29 17:40:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:28 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:40:28 INFO DAGScheduler: Got job 14 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:40:28 INFO DAGScheduler: Final stage: ResultStage 14 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:40:28 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:40:28 INFO DAGScheduler: Missing parents: List()
19/06/29 17:40:28 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[29] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:40:28 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 215.4 KB, free 364.4 MB)
19/06/29 17:40:28 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 76.0 KB, free 364.3 MB)
19/06/29 17:40:28 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 35758d068699:46235 (size: 76.0 KB, free: 365.8 MB)
19/06/29 17:40:28 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1161
19/06/29 17:40:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:40:28 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
19/06/29 17:40:28 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 8132 bytes)
19/06/29 17:40:28 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
19/06/29 17:40:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:28 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174028_0014_m_000000_14' to file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-28_793_6980925052116445287-1/-ext-10000/_temporary/0/task_20190629174028_0014_m_000000
19/06/29 17:40:28 INFO SparkHadoopMapRedUtil: attempt_20190629174028_0014_m_000000_14: Committed
19/06/29 17:40:28 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 1979 bytes result sent to driver
19/06/29 17:40:28 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 27 ms on localhost (executor driver) (1/1)
19/06/29 17:40:28 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
19/06/29 17:40:28 INFO DAGScheduler: ResultStage 14 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.054 s
19/06/29 17:40:28 INFO DAGScheduler: Job 14 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.057043 s
19/06/29 17:40:28 INFO FileFormatWriter: Write Job b722b984-c024-48b5-895a-bae9680f839a committed.
19/06/29 17:40:28 INFO FileFormatWriter: Finished processing stats for write job b722b984-c024-48b5-895a-bae9680f839a.
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:28 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:40:28 INFO Hive: Renaming src: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-28_793_6980925052116445287-1/-ext-10000/part-00000-9875e86a-fa46-45ca-90ee-20a110f53ac9-c000, dest: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-9875e86a-fa46-45ca-90ee-20a110f53ac9-c000, Status:true
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 372
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 267
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 405
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 295
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 349
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 419
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 383
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 410
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 342
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 402
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 444
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 327
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 434
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 441
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 323
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 271
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 302
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 436
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 311
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 360
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 362
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 347
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 332
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 264
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 396
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 245
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 213
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 345
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 259
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 326
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 341
19/06/29 17:40:28 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 35758d068699:46235 in memory (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 433
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 324
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 363
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 241
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 284
19/06/29 17:40:28 INFO ContextCleaner: Cleaned accumulator 212
19/06/29 17:40:28 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 35758d068699:46235 in memory (size: 76.0 KB, free: 365.9 MB)
19/06/29 17:40:28 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:28 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:29 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:29 INFO log: Updated size of table pg_proc to 623
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 430
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 346
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 393
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 298
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 282
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 373
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 281
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 378
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 285
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 376
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 381
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 269
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 266
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 330
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 313
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 386
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 401
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 309
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 367
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 255
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 294
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 366
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 296
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 336
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 301
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 435
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 322
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 249
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 344
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 263
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 418
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 325
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 446
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 448
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 289
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 283
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 351
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 355
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 417
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 429
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 380
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 352
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 318
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 280
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 375
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 377
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 258
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 400
19/06/29 17:40:29 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 35758d068699:46235 in memory (size: 76.0 KB, free: 366.0 MB)
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 394
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 256
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 303
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 409
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 254
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 368
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 445
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 214
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 443
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 374
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 308
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 447
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 305
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 328
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 379
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 321
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 406
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 274
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 387
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 384
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 358
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 257
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 438
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 338
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 304
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 412
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 278
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 244
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 260
19/06/29 17:40:29 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 35758d068699:46235 in memory (size: 76.0 KB, free: 366.1 MB)
19/06/29 17:40:29 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 35758d068699:46235 in memory (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 357
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 293
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 273
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 290
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 391
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 277
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 316
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 369
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 370
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 265
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 343
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 310
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 320
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 390
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 404
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 425
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 398
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 246
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 287
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 413
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 312
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 268
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 415
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 307
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 329
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 331
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 359
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 382
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 356
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 399
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 411
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 427
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 440
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 333
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 389
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 365
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 397
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 392
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 437
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 262
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 388
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 339
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 242
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 299
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 317
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 250
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 337
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 348
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 428
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 252
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 292
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 371
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 272
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 314
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 431
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 291
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 35758d068699:46235 in memory (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 275
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 334
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 408
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 253
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 247
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 340
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 210
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 297
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 288
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 261
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 385
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 286
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 414
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 395
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 300
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 364
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 248
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 407
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 319
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 403
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 335
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 361
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 279
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 354
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 353
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 449
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 426
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 306
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 432
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 35758d068699:46235 in memory (size: 76.0 KB, free: 366.3 MB)
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 439
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 442
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 276
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 270
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 350
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 240
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 251
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 211
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 416
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 315
19/06/29 17:40:29 INFO ContextCleaner: Cleaned accumulator 243
19/06/29 17:40:29 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:29 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:29 INFO log: Updated size of table pg_proc to 623
19/06/29 17:40:29 INFO PgMetadata: Registering a function `pg_catalog.pg_get_function_result` in a system catalog `pg_proc`
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-29_140_5793979313083643579-1
19/06/29 17:40:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:29 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:40:29 INFO DAGScheduler: Got job 15 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:40:29 INFO DAGScheduler: Final stage: ResultStage 15 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:40:29 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:40:29 INFO DAGScheduler: Missing parents: List()
19/06/29 17:40:29 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[31] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:40:29 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 215.4 KB, free 366.1 MB)
19/06/29 17:40:29 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 76.0 KB, free 366.0 MB)
19/06/29 17:40:29 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 35758d068699:46235 (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:40:29 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1161
19/06/29 17:40:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:40:29 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
19/06/29 17:40:29 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 8124 bytes)
19/06/29 17:40:29 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
19/06/29 17:40:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:29 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174029_0015_m_000000_15' to file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-29_140_5793979313083643579-1/-ext-10000/_temporary/0/task_20190629174029_0015_m_000000
19/06/29 17:40:29 INFO SparkHadoopMapRedUtil: attempt_20190629174029_0015_m_000000_15: Committed
19/06/29 17:40:29 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 1979 bytes result sent to driver
19/06/29 17:40:29 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 33 ms on localhost (executor driver) (1/1)
19/06/29 17:40:29 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
19/06/29 17:40:29 INFO DAGScheduler: ResultStage 15 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.062 s
19/06/29 17:40:29 INFO DAGScheduler: Job 15 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.064236 s
19/06/29 17:40:29 INFO FileFormatWriter: Write Job dfe89b88-9532-48a0-9105-0ea3758acc8a committed.
19/06/29 17:40:29 INFO FileFormatWriter: Finished processing stats for write job dfe89b88-9532-48a0-9105-0ea3758acc8a.
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:40:29 INFO Hive: Renaming src: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-29_140_5793979313083643579-1/-ext-10000/part-00000-32d5899f-49a9-4688-ba36-713cadb9cee1-c000, dest: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-32d5899f-49a9-4688-ba36-713cadb9cee1-c000, Status:true
19/06/29 17:40:29 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:29 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:29 INFO log: Updated size of table pg_proc to 679
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:29 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:29 INFO log: Updated size of table pg_proc to 679
19/06/29 17:40:29 INFO PgMetadata: Registering a function `pg_catalog.pg_get_userbyid` in a system catalog `pg_proc`
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-29_413_9037998012843068287-1
19/06/29 17:40:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:29 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:40:29 INFO DAGScheduler: Got job 16 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:40:29 INFO DAGScheduler: Final stage: ResultStage 16 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:40:29 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:40:29 INFO DAGScheduler: Missing parents: List()
19/06/29 17:40:29 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[33] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:40:29 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 215.4 KB, free 365.8 MB)
19/06/29 17:40:29 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 76.0 KB, free 365.7 MB)
19/06/29 17:40:29 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 35758d068699:46235 (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:40:29 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1161
19/06/29 17:40:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[33] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:40:29 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
19/06/29 17:40:29 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 8116 bytes)
19/06/29 17:40:29 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
19/06/29 17:40:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:29 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174029_0016_m_000000_16' to file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-29_413_9037998012843068287-1/-ext-10000/_temporary/0/task_20190629174029_0016_m_000000
19/06/29 17:40:29 INFO SparkHadoopMapRedUtil: attempt_20190629174029_0016_m_000000_16: Committed
19/06/29 17:40:29 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 1979 bytes result sent to driver
19/06/29 17:40:29 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 34 ms on localhost (executor driver) (1/1)
19/06/29 17:40:29 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
19/06/29 17:40:29 INFO DAGScheduler: ResultStage 16 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.075 s
19/06/29 17:40:29 INFO DAGScheduler: Job 16 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.077548 s
19/06/29 17:40:29 INFO FileFormatWriter: Write Job 4eba9a30-0737-4628-82d1-f36fa3d3a828 committed.
19/06/29 17:40:29 INFO FileFormatWriter: Finished processing stats for write job 4eba9a30-0737-4628-82d1-f36fa3d3a828.
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:40:29 INFO Hive: Renaming src: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-29_413_9037998012843068287-1/-ext-10000/part-00000-8419ec69-cde9-45ec-92f2-a5e040873886-c000, dest: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-8419ec69-cde9-45ec-92f2-a5e040873886-c000, Status:true
19/06/29 17:40:29 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:29 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:29 INFO log: Updated size of table pg_proc to 728
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:29 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:29 INFO log: Updated size of table pg_proc to 728
19/06/29 17:40:29 INFO PgMetadata: Registering a function `pg_catalog.pg_table_is_visible` in a system catalog `pg_proc`
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-29_724_2134482004184217079-1
19/06/29 17:40:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:29 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:40:29 INFO DAGScheduler: Got job 17 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:40:29 INFO DAGScheduler: Final stage: ResultStage 17 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:40:29 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:40:29 INFO DAGScheduler: Missing parents: List()
19/06/29 17:40:29 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[35] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:40:29 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 215.4 KB, free 365.5 MB)
19/06/29 17:40:29 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 76.0 KB, free 365.4 MB)
19/06/29 17:40:29 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 35758d068699:46235 (size: 76.0 KB, free: 366.1 MB)
19/06/29 17:40:29 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1161
19/06/29 17:40:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[35] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:40:29 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks
19/06/29 17:40:29 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 8124 bytes)
19/06/29 17:40:29 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
19/06/29 17:40:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:29 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174029_0017_m_000000_17' to file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-29_724_2134482004184217079-1/-ext-10000/_temporary/0/task_20190629174029_0017_m_000000
19/06/29 17:40:29 INFO SparkHadoopMapRedUtil: attempt_20190629174029_0017_m_000000_17: Committed
19/06/29 17:40:29 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 1979 bytes result sent to driver
19/06/29 17:40:29 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 27 ms on localhost (executor driver) (1/1)
19/06/29 17:40:29 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
19/06/29 17:40:29 INFO DAGScheduler: ResultStage 17 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.052 s
19/06/29 17:40:29 INFO DAGScheduler: Job 17 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.052750 s
19/06/29 17:40:29 INFO FileFormatWriter: Write Job 9d6fdb3c-29f8-4064-81fc-483b655a5b5b committed.
19/06/29 17:40:29 INFO FileFormatWriter: Finished processing stats for write job 9d6fdb3c-29f8-4064-81fc-483b655a5b5b.
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:40:29 INFO Hive: Renaming src: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-29_724_2134482004184217079-1/-ext-10000/part-00000-6dc96f9e-e5fd-4cd4-8c06-609fefe7783c-c000, dest: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-6dc96f9e-e5fd-4cd4-8c06-609fefe7783c-c000, Status:true
19/06/29 17:40:29 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:29 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:29 INFO log: Updated size of table pg_proc to 781
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:29 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:29 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:29 INFO log: Updated size of table pg_proc to 781
19/06/29 17:40:29 INFO PgMetadata: Registering a function `pg_catalog.regtype` in a system catalog `pg_proc`
19/06/29 17:40:29 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:30 INFO FileUtils: Creating directory if it doesn't exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-30_002_6356339441909579963-1
19/06/29 17:40:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:30 INFO SparkContext: Starting job: channelRead0 at SimpleChannelInboundHandler.java:105
19/06/29 17:40:30 INFO DAGScheduler: Got job 18 (channelRead0 at SimpleChannelInboundHandler.java:105) with 1 output partitions
19/06/29 17:40:30 INFO DAGScheduler: Final stage: ResultStage 18 (channelRead0 at SimpleChannelInboundHandler.java:105)
19/06/29 17:40:30 INFO DAGScheduler: Parents of final stage: List()
19/06/29 17:40:30 INFO DAGScheduler: Missing parents: List()
19/06/29 17:40:30 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[37] at channelRead0 at SimpleChannelInboundHandler.java:105), which has no missing parents
19/06/29 17:40:30 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 215.4 KB, free 365.2 MB)
19/06/29 17:40:30 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 76.0 KB, free 365.2 MB)
19/06/29 17:40:30 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 35758d068699:46235 (size: 76.0 KB, free: 366.0 MB)
19/06/29 17:40:30 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1161
19/06/29 17:40:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[37] at channelRead0 at SimpleChannelInboundHandler.java:105) (first 15 tasks are for partitions Vector(0))
19/06/29 17:40:30 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
19/06/29 17:40:30 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18, localhost, executor driver, partition 0, PROCESS_LOCAL, 8108 bytes)
19/06/29 17:40:30 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
19/06/29 17:40:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
19/06/29 17:40:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/06/29 17:40:30 INFO FileOutputCommitter: Saved output of task 'attempt_20190629174030_0018_m_000000_18' to file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-30_002_6356339441909579963-1/-ext-10000/_temporary/0/task_20190629174030_0018_m_000000
19/06/29 17:40:30 INFO SparkHadoopMapRedUtil: attempt_20190629174030_0018_m_000000_18: Committed
19/06/29 17:40:30 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 1979 bytes result sent to driver
19/06/29 17:40:30 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 29 ms on localhost (executor driver) (1/1)
19/06/29 17:40:30 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
19/06/29 17:40:30 INFO DAGScheduler: ResultStage 18 (channelRead0 at SimpleChannelInboundHandler.java:105) finished in 0.059 s
19/06/29 17:40:30 INFO DAGScheduler: Job 18 finished: channelRead0 at SimpleChannelInboundHandler.java:105, took 0.062290 s
19/06/29 17:40:30 INFO FileFormatWriter: Write Job 17594a68-df04-4534-b2e1-d8b6c705e8a5 committed.
19/06/29 17:40:30 INFO FileFormatWriter: Finished processing stats for write job 17594a68-df04-4534-b2e1-d8b6c705e8a5.
19/06/29 17:40:30 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:30 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:30 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:30 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:30 INFO SessionState: Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
19/06/29 17:40:30 INFO Hive: Renaming src: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/.hive-staging_hive_2019-06-29_17-40-30_002_6356339441909579963-1/-ext-10000/part-00000-61e6e5d8-55af-4e5a-811a-eccde57903fb-c000, dest: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_proc/part-00000-61e6e5d8-55af-4e5a-811a-eccde57903fb-c000, Status:true
19/06/29 17:40:30 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:30 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:30 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:30 INFO log: Updated size of table pg_proc to 822
19/06/29 17:40:30 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:30 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:30 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:30 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:30 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:30 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:30 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:30 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:30 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:30 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:30 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:30 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:30 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_proc
19/06/29 17:40:30 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_proc	
19/06/29 17:40:30 INFO HiveMetaStore: 1: alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc
19/06/29 17:40:30 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=pg_catalog tbl=pg_proc newtbl=pg_proc	
19/06/29 17:40:30 INFO log: Updating table stats fast for pg_proc
19/06/29 17:40:30 INFO log: Updated size of table pg_proc to 822
19/06/29 17:40:30 INFO PgV3MessageHandler: Open a session (sessionId=0, channelId=-721470680 userName=fdb hostAddr=35758d068699)
19/06/29 17:40:30 INFO PgWireProtocol: Query: statements=set DateStyle to 'ISO'
19/06/29 17:40:30 INFO OperationImpl: Running query with 39cc03fa-f157-43d9-9350-43961a5e4032:
Query:
set DateStyle to 'ISO'
Analyzed Plan:
SetCommand (DateStyle to 'ISO',None)

       
19/06/29 17:40:30 INFO PgWireProtocol: Query: statements=set extra_float_digits to 2
19/06/29 17:40:30 INFO OperationImpl: Running query with bca5074a-335a-406e-9cb6-9e590818c174:
Query:
set extra_float_digits to 2
Analyzed Plan:
SetCommand (extra_float_digits to 2,None)

       
19/06/29 17:40:30 INFO PgWireProtocol: Query: statements=select oid, typbasetype from pg_type where typname = 'lo'
19/06/29 17:40:30 INFO HiveMetaStore: 1: get_table : db=default tbl=pg_type
19/06/29 17:40:30 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=pg_type	
19/06/29 17:40:30 ERROR PgV3MessageHandler: Exception detected in 'Query': org.apache.spark.sql.AnalysisException: Table or view not found: pg_type; line 1 pos 29
	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:733)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:685)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:715)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:708)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:708)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:654)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)
	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
	at scala.collection.immutable.List.foldLeft(List.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)
	at org.apache.spark.sql.fdb.service.OperationImpl$$anonfun$org$apache$spark$sql$fdb$service$OperationImpl$$analyzedPlan$1.apply(ExecutorImpl.scala:59)
	at org.apache.spark.sql.fdb.service.OperationImpl$$anonfun$org$apache$spark$sql$fdb$service$OperationImpl$$analyzedPlan$1.apply(ExecutorImpl.scala:59)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.fdb.service.OperationImpl.org$apache$spark$sql$fdb$service$OperationImpl$$analyzedPlan$lzycompute(ExecutorImpl.scala:58)
	at org.apache.spark.sql.fdb.service.OperationImpl.org$apache$spark$sql$fdb$service$OperationImpl$$analyzedPlan(ExecutorImpl.scala:58)
	at org.apache.spark.sql.fdb.service.OperationImpl.outputSchema$lzycompute(ExecutorImpl.scala:90)
	at org.apache.spark.sql.fdb.service.OperationImpl.outputSchema(ExecutorImpl.scala:89)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgWireProtocol$$anonfun$21$$anonfun$apply$24.apply(protocol.scala:615)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgWireProtocol$$anonfun$21$$anonfun$apply$24.apply(protocol.scala:592)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgV3MessageHandler$$anonfun$handleV3Messages$1.apply(protocol.scala:1284)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgV3MessageHandler$$anonfun$handleV3Messages$1.apply(protocol.scala:1275)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgV3MessageHandler.handleV3Messages(protocol.scala:1275)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgV3MessageHandler.channelRead0(protocol.scala:1084)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgV3MessageHandler.channelRead0(protocol.scala:1071)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'pg_type' not found in database 'default';
	at org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)
	at org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.hive.client.HiveClient$class.getTable(HiveClient.scala:81)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:83)
	at org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:118)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:700)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:700)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:699)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:706)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:730)
	... 79 more

19/06/29 17:40:30 INFO PgWireProtocol: Query: statements=select NULL, nspname, NULL from pg_catalog.pg_namespace n where true and nspname not in ('pg_catalog', 'information_schema', 'pg_toast', 'pg_temp_1') order by nspname
19/06/29 17:40:30 INFO HiveMetaStore: 1: get_database: pg_catalog
19/06/29 17:40:30 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:40:30 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_namespace
19/06/29 17:40:30 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_namespace	
19/06/29 17:40:30 INFO HiveMetaStore: 1: get_table : db=pg_catalog tbl=pg_namespace
19/06/29 17:40:30 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=pg_catalog tbl=pg_namespace	
19/06/29 17:40:30 INFO OperationImpl: Running query with b98e4298-16e7-4887-b038-f190ee264e26:
Query:
select NULL, nspname, NULL from pg_catalog.pg_namespace n where true and nspname not in ('pg_catalog', 'information_schema', 'pg_toast', 'pg_temp_1') order by nspname
Analyzed Plan:
Sort [nspname#843 ASC NULLS FIRST], true
+- Project [null AS NULL#844, nspname#843, null AS NULL#845]
   +- Filter (true && NOT nspname#843 IN (pg_catalog,information_schema,pg_toast,pg_temp_1))
      +- SubqueryAlias `n`
         +- SubqueryAlias `pg_catalog`.`pg_namespace`
            +- HiveTableRelation `pg_catalog`.`pg_namespace`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [oid#842, nspname#843]

       
19/06/29 17:40:30 INFO CodeGenerator: Code generated in 27.1103 ms
19/06/29 17:40:30 INFO CodeGenerator: Code generated in 19.153451 ms
19/06/29 17:40:31 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 281.3 KB, free 364.9 MB)
19/06/29 17:40:31 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 23.6 KB, free 364.9 MB)
19/06/29 17:40:31 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 35758d068699:46235 (size: 23.6 KB, free: 366.0 MB)
19/06/29 17:40:31 INFO SparkContext: Created broadcast 19 from 
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 555
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 552
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 460
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 462
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 532
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 484
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 540
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 508
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 535
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 494
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 502
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 505
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 422
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 452
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 533
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 522
19/06/29 17:40:31 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 35758d068699:46235 in memory (size: 76.0 KB, free: 366.1 MB)
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 492
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 521
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 548
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 482
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 423
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 545
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 518
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 517
19/06/29 17:40:31 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 35758d068699:46235 in memory (size: 76.0 KB, free: 366.1 MB)
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 473
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 541
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 479
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 489
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 459
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 558
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 499
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 504
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 486
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 480
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 537
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 539
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 531
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 554
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 565
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 476
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 496
19/06/29 17:40:31 INFO CodeGenerator: Code generated in 33.873551 ms
19/06/29 17:40:31 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 35758d068699:46235 in memory (size: 76.0 KB, free: 366.2 MB)
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 526
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 536
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 524
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 530
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 513
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 493
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 497
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 469
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 507
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 546
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 561
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 551
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 456
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 520
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 543
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 471
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 453
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 564
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 491
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 527
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 468
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 490
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 511
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 450
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 463
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 516
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 481
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 485
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 544
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 568
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 525
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 488
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 475
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 498
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 461
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 454
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 534
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 566
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 501
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 542
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 560
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 523
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 515
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 503
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 567
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 569
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 549
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 495
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 465
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 472
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 458
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 477
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 483
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 529
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 500
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 547
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 464
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 424
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 457
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 514
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 528
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 467
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 506
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 455
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 563
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 421
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 550
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 559
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 562
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 570
19/06/29 17:40:31 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 35758d068699:46235 in memory (size: 76.0 KB, free: 366.3 MB)
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 420
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 538
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 509
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 478
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 519
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 487
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 556
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 470
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 557
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 510
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 512
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 474
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 553
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 466
19/06/29 17:40:31 INFO ContextCleaner: Cleaned accumulator 451
19/06/29 17:40:31 ERROR PgV3MessageHandler: Exception detected in 'Query': org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
Exchange rangepartitioning(nspname#843 ASC NULLS FIRST, 200)
+- *(1) Project [null AS NULL#844, nspname#843, null AS NULL#845]
   +- *(1) Filter NOT nspname#843 IN (pg_catalog,information_schema,pg_toast,pg_temp_1)
      +- Scan hive pg_catalog.pg_namespace [nspname#843], HiveTableRelation `pg_catalog`.`pg_namespace`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [oid#842, nspname#843]

	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)
	at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)
	at org.apache.spark.sql.fdb.service.OperationImpl.run(ExecutorImpl.scala:206)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgWireProtocol$$anonfun$21$$anonfun$apply$24.apply(protocol.scala:620)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgWireProtocol$$anonfun$21$$anonfun$apply$24.apply(protocol.scala:592)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgV3MessageHandler$$anonfun$handleV3Messages$1.apply(protocol.scala:1284)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgV3MessageHandler$$anonfun$handleV3Messages$1.apply(protocol.scala:1275)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgV3MessageHandler.handleV3Messages(protocol.scala:1275)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgV3MessageHandler.channelRead0(protocol.scala:1084)
	at org.apache.spark.sql.fdb.service.postgresql.protocol.v3.PgV3MessageHandler.channelRead0(protocol.scala:1071)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/data/gitlab/fusiondb/sbin/spark-warehouse/pg_catalog.db/pg_namespace
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:170)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:224)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	... 48 more

19/06/29 17:40:31 INFO PgWireProtocol: Query: statements=SET statement_timeout TO 120000
19/06/29 17:40:31 INFO OperationImpl: Running query with e8853b34-c4d9-43ca-a621-11cdd24a562b:
Query:
SET statement_timeout TO 120000
Analyzed Plan:
SetCommand (statement_timeout TO 120000,None)

       
19/06/29 17:41:14 ERROR SQLServer: RECEIVED SIGNAL TERM
19/06/29 17:41:14 WARN SessionManager: SessionManager stopped though, 1 opened sessions still existed
19/06/29 17:41:14 INFO SparkContext: Invoking stop() from shutdown hook
19/06/29 17:41:14 INFO SparkContext: SparkContext already stopped.
19/06/29 17:41:14 INFO SparkUI: Stopped Spark web UI at http://35758d068699:4040
19/06/29 17:41:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/06/29 17:41:14 INFO MemoryStore: MemoryStore cleared
19/06/29 17:41:14 INFO BlockManager: BlockManager stopped
19/06/29 17:41:14 INFO BlockManagerMaster: BlockManagerMaster stopped
19/06/29 17:41:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/06/29 17:41:14 INFO SparkContext: Successfully stopped SparkContext
19/06/29 17:41:14 INFO ShutdownHookManager: Shutdown hook called
19/06/29 17:41:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-5b7bcea9-d126-4cc1-b2bc-2530e939a279
19/06/29 17:41:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-414b0b46-0e94-4968-bd59-b5115a3fa577
