Spark Command: /opt/jdk1.8.0_112/bin/java -cp ../thirdparty/spark-2.4.3-bin-hadoop2.7/conf/:/opt/fusiondb/sbin/../thirdparty/spark-2.4.3-bin-hadoop2.7/jars/* -Xmx1g org.apache.spark.deploy.SparkSubmit --conf spark.sql.server.port=54322 --conf spark.sql.server.psql.enabled=true --conf spark.sql.server.binaryTransferMode=false --properties-file /opt/fusiondb/sbin/../conf/spark-defaults.conf --class org.apache.spark.sql.fdb.SQLServer --name FusionDB SQL Server /opt/fusiondb/sbin/../assembly/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar
========================================
19/06/29 17:58:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
19/06/29 17:58:33 INFO SQLServer: Started daemon with process name: 3668@35758d068699
19/06/29 17:58:33 INFO SignalUtils: Registered signal handler for TERM
19/06/29 17:58:33 INFO SignalUtils: Registered signal handler for HUP
19/06/29 17:58:33 INFO SignalUtils: Registered signal handler for INT
19/06/29 17:58:33 INFO SQLServer: Spark properties passed to the SQL server:
  key=spark.sql.crossJoin.enabled value=true
  key=spark.sql.server.port value=54322
  key=spark.app.name value=FusionDB SQL Server
  key=spark.sql.server.binaryTransferMode value=false
  key=spark.master value=local[*]
  key=spark.submit.deployMode value=client
  key=spark.sql.server.psql.enabled value=true
  key=spark.jars value=file:/opt/fusiondb/sbin/../assembly/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar
       
19/06/29 17:58:33 INFO RecurringTimer: Started timer for Idle Session Cleaner at time 1561831200000
19/06/29 17:58:33 INFO SparkContext: Running Spark version 2.4.3
19/06/29 17:58:33 INFO SparkContext: Submitted application: FusionDB SQL Server
19/06/29 17:58:33 INFO SecurityManager: Changing view acls to: root
19/06/29 17:58:33 INFO SecurityManager: Changing modify acls to: root
19/06/29 17:58:33 INFO SecurityManager: Changing view acls groups to: 
19/06/29 17:58:33 INFO SecurityManager: Changing modify acls groups to: 
19/06/29 17:58:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
19/06/29 17:58:33 INFO Utils: Successfully started service 'sparkDriver' on port 45563.
19/06/29 17:58:33 INFO SparkEnv: Registering MapOutputTracker
19/06/29 17:58:33 INFO SparkEnv: Registering BlockManagerMaster
19/06/29 17:58:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/06/29 17:58:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/06/29 17:58:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-32e1ff27-17c8-4d41-989f-04ad2246a717
19/06/29 17:58:33 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
19/06/29 17:58:33 INFO SparkEnv: Registering OutputCommitCoordinator
19/06/29 17:58:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/06/29 17:58:34 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://35758d068699:4040
19/06/29 17:58:34 INFO SparkContext: Added JAR file:/opt/fusiondb/sbin/../assembly/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar at spark://35758d068699:45563/jars/fusiondb-fql_2.11_2.4.3_0.1.0-SNAPSHOT-with-dependencies.jar with timestamp 1561831114098
19/06/29 17:58:34 INFO Executor: Starting executor ID driver on host localhost
19/06/29 17:58:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40697.
19/06/29 17:58:34 INFO NettyBlockTransferService: Server created on 35758d068699:40697
19/06/29 17:58:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/06/29 17:58:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 35758d068699, 40697, None)
19/06/29 17:58:34 INFO BlockManagerMasterEndpoint: Registering block manager 35758d068699:40697 with 366.3 MB RAM, BlockManagerId(driver, 35758d068699, 40697, None)
19/06/29 17:58:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 35758d068699, 40697, None)
19/06/29 17:58:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 35758d068699, 40697, None)
19/06/29 17:58:34 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/fusiondb/sbin/spark-warehouse/').
19/06/29 17:58:34 INFO SharedState: Warehouse path is 'file:/opt/fusiondb/sbin/spark-warehouse/'.
19/06/29 17:58:35 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
19/06/29 17:58:35 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19/06/29 17:58:36 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19/06/29 17:58:36 INFO ObjectStore: ObjectStore, initialize called
19/06/29 17:58:36 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
19/06/29 17:58:36 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
19/06/29 17:58:38 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19/06/29 17:58:39 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:58:39 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:58:39 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:58:39 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:58:39 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19/06/29 17:58:39 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/06/29 17:58:39 INFO ObjectStore: Initialized ObjectStore
19/06/29 17:58:39 INFO HiveMetaStore: Added admin role in metastore
19/06/29 17:58:39 INFO HiveMetaStore: Added public role in metastore
19/06/29 17:58:39 INFO HiveMetaStore: No user is added in admin role, since config is empty
19/06/29 17:58:39 INFO HiveMetaStore: 0: get_all_databases
19/06/29 17:58:39 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
19/06/29 17:58:40 INFO HiveMetaStore: 0: get_functions: db=default pat=*
19/06/29 17:58:40 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19/06/29 17:58:40 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
19/06/29 17:58:40 INFO HiveMetaStore: 0: get_functions: db=pg_catalog pat=*
19/06/29 17:58:40 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=pg_catalog pat=*	
19/06/29 17:58:40 INFO SessionState: Created local directory: /tmp/50fa79e9-d399-4d6a-9861-fac2ae0870a7_resources
19/06/29 17:58:40 INFO SessionState: Created HDFS directory: /tmp/hive/root/50fa79e9-d399-4d6a-9861-fac2ae0870a7
19/06/29 17:58:40 INFO SessionState: Created local directory: /tmp/root/50fa79e9-d399-4d6a-9861-fac2ae0870a7
19/06/29 17:58:40 INFO SessionState: Created HDFS directory: /tmp/hive/root/50fa79e9-d399-4d6a-9861-fac2ae0870a7/_tmp_space.db
19/06/29 17:58:40 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/opt/fusiondb/sbin/spark-warehouse/
19/06/29 17:58:40 INFO HiveMetaStore: 0: get_database: default
19/06/29 17:58:40 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/06/29 17:58:40 INFO HiveMetaStore: 0: get_database: pg_catalog
19/06/29 17:58:40 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: pg_catalog	
19/06/29 17:58:40 INFO LoggingHandler: [id: 0x58c39bf1] REGISTERED
19/06/29 17:58:40 INFO LoggingHandler: [id: 0x58c39bf1] BIND: 0.0.0.0/0.0.0.0:54322
19/06/29 17:58:40 INFO LoggingHandler: [id: 0x58c39bf1, L:/0.0.0.0:54322] ACTIVE
19/06/29 17:58:40 INFO PgProtocolService: Start running the SQL server (port=54322, workerThreads=4)
19/06/29 17:58:59 ERROR SQLServer: RECEIVED SIGNAL TERM
19/06/29 17:58:59 INFO SparkContext: Invoking stop() from shutdown hook
19/06/29 17:58:59 INFO SparkUI: Stopped Spark web UI at http://35758d068699:4040
19/06/29 17:58:59 INFO SparkContext: SparkContext already stopped.
19/06/29 17:58:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/06/29 17:58:59 INFO MemoryStore: MemoryStore cleared
19/06/29 17:58:59 INFO BlockManager: BlockManager stopped
19/06/29 17:58:59 INFO BlockManagerMaster: BlockManagerMaster stopped
19/06/29 17:58:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/06/29 17:58:59 INFO SparkContext: Successfully stopped SparkContext
19/06/29 17:58:59 INFO ShutdownHookManager: Shutdown hook called
19/06/29 17:58:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-5020a08d-03b4-4465-b865-80415bf18191
19/06/29 17:58:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-9eb0c383-92d2-4e4f-be50-d22a9deffa8f
